{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55K0twjjjFLo"
      },
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "In The Name of God\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Sharif University of Technology - Department of Electrical Engineering\n",
        "</font>\n",
        "<br>\n",
        "<font color=\"#008080\" size=6>\n",
        "Deep Generative Models\n",
        "</font>\n",
        "<hr/>\n",
        "<font color=\"#800080\" size=5>\n",
        "Assignment 1 : Deep Autoregressive Models\n",
        "<br>\n",
        "</font>\n",
        "<font size=5>\n",
        "Instructor: Dr. S. Amini\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Fall 2025\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "</font>\n",
        "<hr>\n",
        "<font color='red'  size=4>\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Feel free to ask your questions in Telegram : @imoonamm\n",
        "</font>\n",
        "<br>\n",
        "<hr>\n",
        "</div></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You should only change the blank sections, marked with TODO**\n",
        "\n",
        "Pay attention to docstrings, as they may drastically help with your implementation.\n",
        "\n",
        "You are advised to read all related papers and material, to help you better understand the design of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1: WaveNet : A generative model for raw audio\n",
        "\n",
        "WaveNet is a general purpose technology that has allowed us to unlock a range of new applications, from improving video calls on even the weakest connections to helping people regain their original voice after losing the ability to speak.\n",
        "\n",
        "![Local GIF](unnamed.gif)\n",
        "\n",
        "WaveNet models raw audio waveforms autoregressively:\n",
        "$\n",
        "p(x_{1:T}) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$\n",
        "\\\n",
        "Instead of predicting spectrograms or using a vocoder, WaveNet predicts the next audio sample directly (often after µ-law companding and 8-bit/256-way quantization). This yields very natural sounding speech and music because the network learns the waveform structure end-to-end.\n",
        "\n",
        "## 2. Core\n",
        "\n",
        "### 2.1 Causal convolutions\n",
        "A causal 1D convolution ensures predictions at time \\(t\\) don't depend on future samples:\n",
        "- Achieved by padding only on the left (past).\n",
        "- In practice, use `padding = dilation * (kernel_size - 1)` and then trim the rightmost elements (or use `Chomp1d` cropping).\n",
        "\n",
        "### 2.2 Dilated convolutions\n",
        "Dilations `[1, 2, 4, 8, ...]` in stacked layers let the receptive field grow exponentially with depth while keeping the number of parameters manageable. A stack of several cycles of dilations covers long temporal context (hundreds to thousands of samples).\n",
        "\n",
        "### 2.3 Gated activation units\n",
        "Each residual block uses a gated unit:\n",
        "$\n",
        "\\text{z} = \\tanh(W_f * x) \\odot \\sigma(W_g * x)\n",
        "$\n",
        "where `*` is dilated causal conv, `W_f` and `W_g` are convolution filters. The output splits to (1) residual connection and (2) skip connection that is added to final output.\n",
        "\n",
        "### 2.4 Residual & skip connections\n",
        "- Residual: `x_{l+1} = x_l + \\text{residual\\_out}` to ease training.\n",
        "- Skip: every block outputs a skip tensor; all skip outputs are summed, then passed through post-processing (ReLU, Conv, softmax) to produce the final distribution over quantized samples.\n",
        "\n",
        "### 2.5 Output quantization / softmax\n",
        "Audio samples are often µ-law quantized to 256 values; WaveNet predicts a categorical distribution (`softmax(256)`) over those values for each time step. Continuous outputs are also possible (mixture of logistics / Gaussians) but discrete softmax is standard in the original paper.\n",
        "\n",
        "### 2.6 Conditioning\n",
        "- **Global conditioning**: a per-utterance vector (e.g., speaker id embedding) is added to layer activations.\n",
        "- **Local conditioning**: low-rate features (e.g., mel spectrogram) are upsampled (transposed conv / nearest) and added at each time step.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training & sampling\n",
        "\n",
        "### Training\n",
        "- Teacher forcing: at training time the network receives true previous samples and learns to predict the next sample with cross-entropy loss (if quantized).\n",
        "- Batch size and optimization: use Adam, gradient clipping recommended for stability.\n",
        "- Preprocess: µ-law companding + 256-level quantization is common.\n",
        "\n",
        "### Sampling (inference)\n",
        "- Autoregressive generation: sample one sample at a time and feed it back.\n",
        "- Slow by default — many acceleration techniques exist (distillation → Parallel Wavenet, caching convolutions, WaveRNN, etc.).\n",
        "\n",
        "\n",
        "\n",
        "**Reference:** [WaveNet: A Generative Model for Raw Audio* (van den Oord et al., DeepMind, 2016)](https://arxiv.org/abs/1609.03499)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchaudio\n",
        "# !pip install git+git://github.com/pytorch/audio\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import ModuleList\n",
        "from IPython.display import Audio\n",
        "from torch.autograd import Variable\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### at first you must convert input into One Hot vector\n",
        "\n",
        "5 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneHot(nn.Module):\n",
        "    def __init__(self, MU):\n",
        "        super(OneHot,self).__init__()\n",
        "        self.MU = MU\n",
        "        self.ones = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        if (self.ones is None) or (self.ones.device != x.device) or (self.ones.size(0) != self.MU):\n",
        "            self.ones = torch.eye(self.MU, device=x.device)\n",
        "        flat = x.view(-1)\n",
        "        oh = self.ones.index_select(0, flat)           # (batch*time, MU)\n",
        "        return oh.view(*x.size(), self.MU)             # (batch, time, MU)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"({})\".format(self.MU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before diving into the WaveNet model itself, it’s important to understand what kind of **data** we are modeling — **raw audio waveforms**.  \n",
        "Neural networks can only process numbers, so we need to represent sound in a numerical form that captures its essential structure and variation.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Sampling Rate and Bit Depth\n",
        "\n",
        "### Sound as a Signal\n",
        "Sound in the real world is a **continuous analog waveform** — a pressure signal that varies smoothly over time.  \n",
        "To process it digitally, we **sample** it: we take discrete measurements of the amplitude at equally spaced time intervals.\n",
        "\n",
        "This process converts a continuous signal into a **time series of numbers**:\n",
        "$\n",
        "x = [x_1, x_2, \\dots, x_T]\n",
        "$\n",
        "where each \\(x_t\\) represents the air pressure (or voltage) at time step \\(t\\).\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling Rate\n",
        "The **sampling rate** defines **how many times per second** we measure the amplitude of the signal.  \n",
        "Typical values:\n",
        "- CD quality audio: 44,100 Hz (samples per second)\n",
        "- Speech datasets: 16,000 Hz or 22,050 Hz\n",
        "\n",
        "A higher sampling rate means more detail, but also more data to process.  \n",
        "WaveNet typically models 16 kHz or 22 kHz audio for speech generation tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### Bit Depth\n",
        "Each recorded sample must be represented by a finite number of bits.  \n",
        "The **bit depth** determines the range and precision of these values:\n",
        "- 8-bit → 256 possible levels  \n",
        "- 16-bit → 65,536 levels  \n",
        "- 24-bit → 16,777,216 levels  \n",
        "\n",
        "The bit depth affects the **signal-to-noise ratio (SNR)** — higher bit depth means less quantization noise and a larger dynamic range:\n",
        "$\n",
        "\\text{SNR} \\approx 6.02 \\times \\text{bit depth} + 1.76\\ \\text{dB}\n",
        "$\n",
        "\n",
        "For example:\n",
        "- 8-bit ≈ 49.9 dB SNR  \n",
        "- 16-bit ≈ 98 dB SNR\n",
        "\n",
        "---\n",
        "\n",
        "## 2. From Audio to Time Series\n",
        "\n",
        "After digitization, the audio signal becomes a **sequence of numbers over time**, also known as a **time series**.  \n",
        "Each point depends on previous ones — this temporal dependency is what WaveNet models autoregressively:\n",
        "$\n",
        "p(x_{1:T}) = \\prod_{t=1}^{T} p(x_t \\mid x_{<t})\n",
        "$\n",
        "\n",
        "However, raw 16-bit audio contains **too many values** (65,536 possible amplitudes).  \n",
        "Predicting the exact next value among that huge range is extremely hard for a neural network.\n",
        "\n",
        "To simplify, we reduce this dynamic range to a smaller set — typically **256 levels** — using a technique called **µ-law companding**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. µ-law Quantization (Companding Transformation)\n",
        "\n",
        "### Motivation\n",
        "Human hearing follows the **Weber–Fechner law** — our perception of changes in loudness is **logarithmic**, not linear.  \n",
        "That means we are more sensitive to small amplitude changes in quiet sounds than in loud ones.\n",
        "\n",
        "µ-law companding exploits this property by **compressing the amplitude range logarithmically** —  \n",
        "it allocates more resolution to small signals (quiet sounds) and less to large ones.\n",
        "\n",
        "---\n",
        "\n",
        "### µ-law Formula\n",
        "\n",
        "$\n",
        "f(x) = \\text{sign}(x) \\cdot \\frac{\\ln(1 + \\mu |x|)}{\\ln(1 + \\mu)}, \\quad -1 \\le x \\le 1\n",
        "$\n",
        "\n",
        "where:\n",
        "- \\(x\\) = input waveform (normalized to [-1, 1])  \n",
        "- \\(\\mu\\) = companding constant (usually 255 for 8-bit quantization)\n",
        "\n",
        "After applying µ-law, we map the range \\([-1, 1]\\) to discrete integer values \\([0, 255]\\).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Wavenet\n",
        "\n",
        "20 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following figure describes the overall architecture of WaveNet. Note that Transformers were non-existent in 2016!\n",
        "\n",
        "![Wavenet Arch](WaveNet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class WavenetCorrected(nn.Module):\n",
        "    def __init__(self, dilation_depth, n_blocks, n_dil_channels, n_residual_channels, \n",
        "                 n_skip_channels, n_category, kernel_size):\n",
        "        super(WavenetCorrected, self).__init__()\n",
        "        self.dilation_depth = dilation_depth\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_dil_channels = n_dil_channels\n",
        "        self.n_residual_channels = n_residual_channels\n",
        "        self.n_skip_channels = n_skip_channels\n",
        "        self.n_category = n_category\n",
        "        self.kernel_size = kernel_size\n",
        "        self.One_hot = OneHot(n_category)\n",
        "\n",
        "        # Causal convolution\n",
        "        self.causal = nn.Conv1d(n_category, n_residual_channels, kernel_size)\n",
        "\n",
        "        # Dilated convolution stacks\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.res_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "        \n",
        "        for _ in range(n_blocks):\n",
        "            for i in range(dilation_depth):\n",
        "                dilation = 2 ** i\n",
        "                self.filter_convs.append(\n",
        "                    nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "                              kernel_size, dilation=dilation))\n",
        "                self.gate_convs.append(\n",
        "                    nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "                              kernel_size, dilation=dilation))\n",
        "                self.res_convs.append(\n",
        "                    nn.Conv1d(n_dil_channels, n_residual_channels, 1))\n",
        "                self.skip_convs.append(\n",
        "                    nn.Conv1d(n_dil_channels, n_skip_channels, 1))\n",
        "\n",
        "        # Post-processing\n",
        "        self.post = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(n_skip_channels, n_skip_channels, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(n_skip_channels, n_category, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Handle input format\n",
        "        if x.dim() == 2:\n",
        "            x_oh = self.One_hot(x)\n",
        "            x_oh = x_oh.transpose(1, 2)\n",
        "        else:\n",
        "            x_oh = x\n",
        "        \n",
        "        # Apply causal convolution with left padding\n",
        "        x = F.pad(x_oh, (self.kernel_size - 1, 0))\n",
        "        x = self.causal(x)\n",
        "\n",
        "        skip_connections = []\n",
        "        \n",
        "        for f, g, r, s in zip(self.filter_convs, self.gate_convs,\n",
        "                              self.res_convs, self.skip_convs):\n",
        "            # Apply causal padding for this dilation\n",
        "            dilation = f.dilation[0]\n",
        "            padding = (self.kernel_size - 1) * dilation\n",
        "            x_padded = F.pad(x, (padding, 0))\n",
        "            \n",
        "            # Gated activation\n",
        "            tanh_out = torch.tanh(f(x_padded))\n",
        "            sigm_out = torch.sigmoid(g(x_padded))\n",
        "            z = tanh_out * sigm_out\n",
        "            \n",
        "            # Skip and residual connections\n",
        "            skip = s(z)\n",
        "            skip_connections.append(skip)\n",
        "            res = r(z)\n",
        "            \n",
        "            # Residual connection\n",
        "            x = x + res\n",
        "\n",
        "        # Sum skip connections and generate output\n",
        "        x = sum(skip_connections)\n",
        "        x = self.post(x)\n",
        "        x = x.transpose(1, 2)  # (B, T, n_category)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def generate(self, seed, num_samples, device, temperature=1.0):\n",
        "        \"\"\"Optimized generation function\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            current_sequence = seed.clone().to(device)\n",
        "            generated = []\n",
        "            \n",
        "            # Calculate receptive field for this model\n",
        "            receptive_field = calculate_receptive_field(\n",
        "                self.dilation_depth, self.n_blocks, self.kernel_size\n",
        "            )\n",
        "            \n",
        "            for i in range(num_samples):\n",
        "                # Use only the last receptive_field samples\n",
        "                if len(current_sequence) >= receptive_field:\n",
        "                    context = current_sequence[-receptive_field:]\n",
        "                else:\n",
        "                    context = current_sequence\n",
        "                \n",
        "                # One-hot encode\n",
        "                x = F.one_hot(context, num_classes=self.n_category).float()\n",
        "                x = x.unsqueeze(0).transpose(1, 2)\n",
        "                \n",
        "                # Get prediction\n",
        "                logits = self.forward(x)[0, -1]  # Last timestep\n",
        "                logits = logits / temperature\n",
        "                \n",
        "                # Sample next token\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1)\n",
        "                \n",
        "                generated.append(next_token.item())\n",
        "                current_sequence = torch.cat([current_sequence, next_token.cpu()])\n",
        "                \n",
        "                if (i + 1) % 1000 == 0:\n",
        "                    print(f\"Generated {i+1}/{num_samples} samples\")\n",
        "            \n",
        "            return torch.tensor(generated)\n",
        "\n",
        "# class Wavenet(nn.Module):\n",
        "#   def __init__(self,dilation_depth, n_blocks, n_dil_channels, n_residual_channels,n_skip_channels, n_category, kernel_size):\n",
        "#     super(Wavenet,self).__init__()\n",
        "#     self.dilation_depth = dilation_depth\n",
        "#     self.n_blocks = n_blocks\n",
        "#     self.n_dil_channels = n_dil_channels\n",
        "#     self.n_residual_channels = n_residual_channels\n",
        "#     self.n_skip_channels = n_skip_channels\n",
        "#     self.n_category = n_category\n",
        "#     self.kernel_size = kernel_size\n",
        "#     self.One_hot = OneHot(n_category)\n",
        "\n",
        "#     ###Building the model###\n",
        "\n",
        "#     ##creating first channels##\n",
        "#     self.causal = nn.Conv1d(n_category, n_residual_channels,\n",
        "#                             kernel_size, padding=kernel_size - 1)\n",
        "\n",
        "#     ###Creating wavenet blocks stacks###\n",
        "#     self.filter_convs = nn.ModuleList()\n",
        "#     self.gate_convs = nn.ModuleList()\n",
        "#     self.res_convs = nn.ModuleList()\n",
        "#     self.skip_convs = nn.ModuleList()\n",
        "#     for _ in range(n_blocks):\n",
        "#         for i in range(dilation_depth):\n",
        "#             dilation = 2 ** i\n",
        "#             pad = (kernel_size - 1) * dilation  # left padding for causality\n",
        "#             self.filter_convs.append(\n",
        "#                 nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "#                           kernel_size, padding=pad, dilation=dilation))\n",
        "#             self.gate_convs.append(\n",
        "#                 nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "#                           kernel_size, padding=pad, dilation=dilation))\n",
        "#             self.res_convs.append(\n",
        "#                 nn.Conv1d(n_dil_channels, n_residual_channels, 1))\n",
        "#             self.skip_convs.append(\n",
        "#                 nn.Conv1d(n_dil_channels, n_skip_channels, 1))\n",
        "#     ##post convoluions\n",
        "#     self.post = nn.Sequential(\n",
        "#       nn.ReLU(),\n",
        "#       nn.Conv1d(n_skip_channels, n_skip_channels, 1),\n",
        "#       nn.ReLU(),\n",
        "#       nn.Conv1d(n_skip_channels, n_category, 1)\n",
        "#     )\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     # if x.dim() == 2:  # (B, T)\n",
        "#     #     x_oh = self.One_hot(x)              # (B, T, C)\n",
        "#     # else:  # already one-hot\n",
        "#     #     x_oh = x\n",
        "#     # x_oh = x_oh.transpose(1, 2)             # (B, C, T)\n",
        "#     # --- FIX STARTS HERE ---\n",
        "#     # Input x can be:\n",
        "#     # 1. LongTensor (B, T) of class indices -> needs one-hot encoding\n",
        "#     # 2. FloatTensor (B, C, T) from dataloader -> use as is\n",
        "#     if x.dim() == 2:\n",
        "#         x_oh = self.One_hot(x)      # -> (B, T, C)\n",
        "#         x_oh = x_oh.transpose(1, 2) # -> (B, C, T) for Conv1d\n",
        "#     else:\n",
        "#     # Assumes input is already one-hot and in (B, C, T) format\n",
        "#         x_oh = x\n",
        "#     # --- FIX ENDS HERE ---\n",
        "\n",
        "#     # Causal front-end (trim right to keep causality)\n",
        "#     x = self.causal(x_oh)\n",
        "#     if self.kernel_size > 1:\n",
        "#         x = x[:, :, :-(self.kernel_size - 1)]\n",
        "\n",
        "#     skip_connections = []\n",
        "#     for f, g, r, s in zip(self.filter_convs,\n",
        "#                           self.gate_convs,\n",
        "#                           self.res_convs,\n",
        "#                           self.skip_convs):\n",
        "#         tanh_out = torch.tanh(f(x))\n",
        "#         sigm_out = torch.sigmoid(g(x))\n",
        "\n",
        "#         # remove the extra right padding introduced by the dilated conv\n",
        "#         trim = (self.kernel_size - 1) * f.dilation[0]\n",
        "#         if trim > 0:\n",
        "#             tanh_out = tanh_out[:, :, :-trim]\n",
        "#             sigm_out = sigm_out[:, :, :-trim]\n",
        "\n",
        "#         z = tanh_out * sigm_out\n",
        "#         skip = s(z)\n",
        "#         skip_connections.append(skip)\n",
        "\n",
        "#         res = r(z)\n",
        "#         x = x[:, :, -res.size(2):]  # align with residual length\n",
        "#         x = x + res\n",
        "\n",
        "#     # Sum all skip connections and project to logits\n",
        "#     x = sum(skip_connections)\n",
        "#     x = self.post(x)                                # (B, n_category, T)\n",
        "#     x = x.transpose(1, 2)                           # (B, T, n_category)\n",
        "#     return x.squeeze(0) if x.size(0) == 1 else x\n",
        "#   ###Function to generate samples###\n",
        "#   def generate(self, input, num_samples=100):\n",
        "#     self.eval()\n",
        "#     if not torch.is_tensor(input):\n",
        "#         x = torch.tensor(input, dtype=torch.long)\n",
        "#     else:\n",
        "#         x = input.long()\n",
        "#     x = x.view(1, -1)  # (1, T)\n",
        "#     gen_list = x.squeeze(0).tolist()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for _ in range(num_samples):\n",
        "#             logits = self.forward(x)  # (T, n_category)\n",
        "#             probs = F.softmax(logits[-1], dim=-1)\n",
        "#             next_sample = torch.multinomial(probs, 1)  # sample from categorical\n",
        "#             gen_list.append(next_sample.item())\n",
        "#             x = torch.cat([x, next_sample.view(1, 1)], dim=1)\n",
        "\n",
        "#     return gen_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantization of 16 bit audio\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mulaw_quantize(x, qc):\n",
        "    mu = qc - 1\n",
        "    x_mu = torch.sign(x) * torch.log1p(mu * torch.abs(x)) / torch.log1p(mu)\n",
        "    return x_mu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inverse quantization\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inv_mulaw_quantize(x_mu, quantization_channels=256, device = device):\n",
        "    x_mu = x_mu.to(device)\n",
        "    mu = quantization_channels - 1\n",
        "    x = torch.sign(x_mu) * (torch.exp(torch.abs(x_mu) * torch.log1p(mu)) - 1.0) / mu\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "In this part you must load the audio_dataset.npz that exist in zip file of homework\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import math\n",
        "import threading\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import librosa as lr\n",
        "import bisect\n",
        "\n",
        "\n",
        "class WavenetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_file,\n",
        "                 item_length,\n",
        "                 target_length,\n",
        "                 file_location=None,\n",
        "                 classes=256,\n",
        "                 sampling_rate=16000,\n",
        "                 mono=True,\n",
        "                 normalize=False,\n",
        "                 dtype=np.uint8,\n",
        "                 train=True,\n",
        "                 test_stride=100):\n",
        "\n",
        "        #           |----receptive_field----|\n",
        "        #                                 |--output_length--|\n",
        "        # example:  | | | | | | | | | | | | | | | | | | | | |\n",
        "        # target:                           | | | | | | | | | |\n",
        "\n",
        "        self.dataset_file = dataset_file\n",
        "        self._item_length = item_length\n",
        "        self._test_stride = test_stride\n",
        "        self.target_length = target_length\n",
        "        self.classes = classes\n",
        "\n",
        "        if not os.path.isfile(dataset_file):\n",
        "            assert file_location is not None, \"no location for dataset files specified\"\n",
        "            self.mono = mono\n",
        "            self.normalize = normalize\n",
        "\n",
        "            self.sampling_rate = sampling_rate\n",
        "            self.dtype = dtype\n",
        "            self.create_dataset(file_location, dataset_file)\n",
        "        else:\n",
        "            # Unknown parameters of the stored dataset\n",
        "            # TODO Can these parameters be stored, too?\n",
        "            self.mono = None\n",
        "            self.normalize = None\n",
        "\n",
        "            self.sampling_rate = None\n",
        "            self.dtype = None\n",
        "\n",
        "        self.data = np.load(self.dataset_file, mmap_mode='r')\n",
        "        self.start_samples = [0]\n",
        "        self._length = 0\n",
        "        self.calculate_length()\n",
        "        self.train = train\n",
        "        print(\"one hot input\")\n",
        "        # assign every *test_stride*th item to the test set\n",
        "\n",
        "    def calculate_length(self):\n",
        "        start_samples = [0]\n",
        "        for i in range(len(self.data.keys())):\n",
        "            start_samples.append(start_samples[-1] + len(self.data[f'arr_{i}']))\n",
        "\n",
        "        total = start_samples[-1]\n",
        "        usable = max(0, total - self._item_length + 1)  # last valid start is total - item_length\n",
        "        self._length = usable // self.target_length      # stride = target_length if that’s your intent\n",
        "        self.start_samples = start_samples\n",
        "        # start_samples = [0]\n",
        "        # for i in range(len(self.data.keys())):\n",
        "        #     start_samples.append(start_samples[-1] + len(self.data['arr_' + str(i)]))\n",
        "        # available_length = start_samples[-1] - (self._item_length - (self.target_length - 1)) - 1\n",
        "        # usable = max(0, self.start_samples[-1] - self._item_length + 1)\n",
        "        # self._length = usable // self.target_length  # stride = target_length if you want that spacing\n",
        "        # # self._length = math.floor(available_length / self.target_length)\n",
        "        # self.start_samples = start_samples\n",
        "\n",
        "    def set_item_length(self, l):\n",
        "        self._item_length = l\n",
        "        self.calculate_length()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Clarify sample_index calculation for train/test with test_stride\n",
        "        if self._test_stride <= 0:\n",
        "            sample_number = idx\n",
        "        elif self.train:\n",
        "            if self._test_stride == 1:\n",
        "                raise IndexError(\"test_stride=1 moves every item to the test split.\")\n",
        "            block = idx // (self._test_stride - 1)\n",
        "            offset = idx % (self._test_stride - 1)\n",
        "            sample_number = block * self._test_stride + offset + 1\n",
        "        else:\n",
        "            sample_number = idx * self._test_stride\n",
        "        sample_index = sample_number * self.target_length\n",
        "\n",
        "        # TODO: Check file_index bounds; currently prints error, consider raising IndexError\n",
        "        file_index = bisect.bisect_right(self.start_samples, sample_index) - 1\n",
        "        if file_index < 0 or file_index >= len(self.start_samples) - 1:\n",
        "            raise IndexError(f\"Sample index {sample_index} is out of range.\")\n",
        "\n",
        "        file_key = f'arr_{file_index}'\n",
        "        file_data = self.data[file_key]\n",
        "        local_index = sample_index - self.start_samples[file_index]\n",
        "        end_in_file = local_index + self._item_length\n",
        "\n",
        "        if end_in_file <= len(file_data):\n",
        "        # TODO: Avoid repeated np.load calls; cache self.data in __init__ instead\n",
        "            window = file_data[local_index:end_in_file]\n",
        "\n",
        "        else:\n",
        "            # TODO: Optimize cross-file slicing; currently concatenates arrays which copies memory\n",
        "            pieces = [file_data[local_index:]]\n",
        "            remaining = self._item_length - len(pieces[0])\n",
        "            next_file = file_index + 1\n",
        "            while remaining > 0 and next_file < len(self.start_samples) - 1:\n",
        "                chunk = self.data[f'arr_{next_file}']\n",
        "                take = min(len(chunk), remaining)\n",
        "                pieces.append(chunk[:take])\n",
        "                remaining -= take\n",
        "                next_file += 1\n",
        "            if remaining > 0:\n",
        "                raise IndexError(f\"Not enough samples to build an item starting at {sample_index}.\")\n",
        "            window = np.concatenate(pieces, axis=0)\n",
        "        \n",
        "\n",
        "        # TODO: One-hot creation is memory heavy; consider returning integer indices and use nn.Embedding in model\n",
        "        \n",
        "        # fetch window with one more sample than the model input\n",
        "        window = window.astype(np.int64, copy=False)\n",
        "        x_tokens = window[:-1]                      # everything except the last sample\n",
        "        y_tokens = window[1:]                       # everything except the first sample\n",
        "        one_hot = np.zeros((self.classes, x_tokens.size), dtype=np.float32)\n",
        "        time_idx = np.arange(x_tokens.size)\n",
        "        one_hot[x_tokens, time_idx] = 1.0\n",
        "\n",
        "        targets = torch.from_numpy(y_tokens[-self.target_length:]).long()\n",
        "\n",
        "        # window = window.astype(np.int64, copy=False)\n",
        "        # one_hot = np.zeros((self.classes, self._item_length), dtype=np.float32)\n",
        "        # time_idx = np.arange(self._item_length)\n",
        "        # one_hot[window, time_idx] = 1.0\n",
        "\n",
        "        # target = torch.from_numpy(window[-self.target_length:]).long()\n",
        "        one_hot = torch.from_numpy(one_hot)\n",
        "\n",
        "        return one_hot, targets\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # total number of usable start positions across all files\n",
        "        test_length = math.floor(self._length / self._test_stride)\n",
        "        if self.train:\n",
        "            return self._length - test_length\n",
        "        else:\n",
        "            return test_length\n",
        "\n",
        "\n",
        "def quantize_data(data, classes):\n",
        "    encoded = mu_law_encoding(data, classes)\n",
        "    quantized = ((encoded + 1.0) / 2.0 * (classes - 1) + 0.5).astype(np.int64)\n",
        "    return quantized.astype(np.uint8)\n",
        "\n",
        "\n",
        "def list_all_audio_files(location):\n",
        "    audio_files = lr.util.find_files(location, ext=['wav', 'flac', 'ogg', 'mp3', 'aiff', 'm4a'])\n",
        "    return sorted(audio_files)\n",
        "\n",
        "\n",
        "def mu_law_encoding(data, mu):\n",
        "    arr = np.clip(np.asarray(data, dtype=np.float32), -1.0, 1.0)\n",
        "    return np.sign(arr) * np.log1p(mu * np.abs(arr)) / np.log1p(mu)\n",
        "\n",
        "\n",
        "def mu_law_expansion(data, mu):\n",
        "    arr = np.asarray(data, dtype=np.float32)\n",
        "    return np.sign(arr) * (np.expm1(np.abs(arr) * np.log1p(mu)) / mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dilation_depth = 10\n",
        "n_blocks = 2\n",
        "n_dilation_channels = 24\n",
        "n_residual_channels = 24\n",
        "n_skip_channels = 128\n",
        "n_category = 128\n",
        "kernel_size = 2\n",
        "model = Wavenet(dilation_depth,n_blocks,n_dilation_channels ,n_residual_channels,n_skip_channels,n_category,kernel_size)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define data generator\n",
        "\n",
        "10 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def data_generation(data, fr, seq_len_segment, mu, device=device):\n",
        "    ###shape of data is [batch_size,seq_len]\n",
        "    ### we want to randomly choose a starting position and then extract the seq_len_segment\n",
        "    ### now the data is normalized for mu law encoding\n",
        "    _ = fr  # placeholder to keep signature stable\n",
        "\n",
        "    batch_size, total_len = data.shape\n",
        "    if seq_len_segment < 2:\n",
        "        raise ValueError(\"seq_len_segment must be at least 2 to form input/target pairs.\")\n",
        "    max_start = total_len - seq_len_segment\n",
        "    if max_start < 0:\n",
        "        raise ValueError(\"seq_len_segment is longer than available sequence length.\")\n",
        "\n",
        "    # Random start per item in batch\n",
        "    start_positions = torch.randint(0, max_start + 1, (batch_size,), device=device)\n",
        "    segments = torch.stack(\n",
        "        [data[i, s : s + seq_len_segment] for i, s in enumerate(start_positions)],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    # µ-law companding + quantization to integer classes [0, mu]\n",
        "    log_mu = math.log1p(mu)\n",
        "    encoded = torch.sign(segments) * torch.log1p(mu * segments.abs()) / log_mu\n",
        "    quantized = torch.clamp(((encoded + 1.0) * (mu / 2.0) + 0.5), 0, mu).long()\n",
        "\n",
        "    # Teacher-forcing layout: predict t+1 from t\n",
        "    x_tok = quantized[:, :-1]          # (B, T-1)\n",
        "    y_tok = quantized[:, 1:]           # (B, T-1)\n",
        "\n",
        "    # One-hot inputs for categorical softmax (B, classes, T-1)\n",
        "    one_hot = torch.zeros(\n",
        "        batch_size, seq_len_segment - 1, mu + 1, device=device, dtype=torch.float32\n",
        "    )\n",
        "    one_hot.scatter_(2, x_tok.unsqueeze(-1), 1.0)\n",
        "    x_one_hot = one_hot.transpose(1, 2).contiguous()\n",
        "\n",
        "    return x_one_hot, y_tok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize sample of data\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import Audio\n",
        "# import soundfile as sf\n",
        "\n",
        "# Load first stored array; adjust key if needed\n",
        "arr = np.load(\"audio_dataset.npz\", mmap_mode=\"r\")\n",
        "signal = arr[\"arr_0\"][30000:30000+20*4096]  # take a short window\n",
        "\n",
        "# If already quantized ints, invert mu-law for plotting\n",
        "mu = 255\n",
        "if np.issubdtype(signal.dtype, np.integer):\n",
        "    y = 2.0 * signal.astype(np.float32) / mu - 1.0\n",
        "    signal = np.sign(y) * (np.expm1(np.abs(y) * np.log1p(mu)) / mu)\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(signal)\n",
        "plt.title(\"Sample waveform from audio_dataaset.npz\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "Audio(signal, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer and Loss Function\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()  # targets are class indices in [0, mu]\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_wavenet(model,\n",
        "                  dataloader,\n",
        "                  epochs,\n",
        "                  optimizer,\n",
        "                  device,\n",
        "                  print_every=50):\n",
        "    model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running = 0.0\n",
        "        count = 0\n",
        "        \n",
        "        for step, (x, y) in enumerate(dataloader, 1):\n",
        "            x = x.to(device)  # (B, C, T) one-hot encoded\n",
        "            y = y.to(device)  # (B, target_length) target tokens\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)  # (B, T_out, C)\n",
        "            \n",
        "            # The model outputs predictions for the last target_length positions\n",
        "            # Align with targets properly\n",
        "            T_out = logits.size(1)\n",
        "            T_target = y.size(1)\n",
        "            \n",
        "            # Take the last T_target predictions\n",
        "            if T_out >= T_target:\n",
        "                logits = logits[:, -T_target:, :]\n",
        "            else:\n",
        "                # If model output is shorter, pad targets from the end\n",
        "                y = y[:, -T_out:]\n",
        "            \n",
        "            B, T, C = logits.shape\n",
        "            loss = criterion(\n",
        "                logits.reshape(B * T, C),\n",
        "                y.reshape(B * T)\n",
        "            )\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Add gradient clipping\n",
        "            optimizer.step()\n",
        "            \n",
        "            running += loss.item()\n",
        "            count += 1\n",
        "\n",
        "            if step % print_every == 0:\n",
        "                avg = running / count\n",
        "                print(f\"Epoch {epoch} Step {step}: loss {avg:.4f}\")\n",
        "                running = 0.0\n",
        "                count = 0\n",
        "\n",
        "        if count > 0:\n",
        "            epoch_loss = running / count\n",
        "            train_losses.append(epoch_loss)\n",
        "            print(f\"Epoch {epoch} done. Avg Loss: {epoch_loss:.4f}\")\n",
        "            \n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "    return train_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot losses\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(trainLoss, Epochs):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(range(1, Epochs + 1), trainLoss, marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Train Loss\")\n",
        "    plt.title(\"WaveNet Training Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model\n",
        "\n",
        "5 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, path=\"wavenet.pt\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Audio using trained model\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_audio(model,\n",
        "                   seed,\n",
        "                   samples,\n",
        "                   device,\n",
        "                   receptive_field,\n",
        "                   classes=256,\n",
        "                   mu=255):\n",
        "    model.eval()\n",
        "    seed = seed.to(device)\n",
        "    generated = seed.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(samples):\n",
        "            # x = torch.zeros(1, generated.numel(), classes, device=device)\n",
        "            # idx = torch.arange(generated.numel(), device=device)\n",
        "            # x[0, idx, generated] = 1.0  \n",
        "            # --- FIX & OPTIMIZATION ---\n",
        "            # 1. Take only the last `receptive_field` samples as input context.\n",
        "            #    This is much more efficient than processing the whole sequence.\n",
        "            context = generated[-receptive_field:]\n",
        "            \n",
        "            # 2. Correctly one-hot encode and shape the input tensor.\n",
        "            #    context shape: (receptive_field,)\n",
        "            #    F.one_hot -> (receptive_field, classes)\n",
        "            #    .unsqueeze(0) -> (1, receptive_field, classes)\n",
        "            #    .transpose(1, 2) -> (1, classes, receptive_field) -> This is the correct shape!\n",
        "            x = F.one_hot(context, num_classes=classes).float().unsqueeze(0).transpose(1, 2)\n",
        "            \n",
        "            # --- END FIX ---\n",
        "\n",
        "\n",
        "            logits = model(x).squeeze(0)   # (T, classes)\n",
        "            next_logits = logits[-1]       # final timestep, shape (classes,)\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1)\n",
        "\n",
        "            generated = torch.cat([generated, next_token], dim=0)\n",
        "\n",
        "    return generated.cpu()\n",
        "\n",
        "def inv_mulaw_int_to_float(x_int, mu=255):\n",
        "    \"\"\"x_int: Long/Int tensor in [0, mu]; returns float tensor in [-1, 1].\"\"\"\n",
        "    y = 2.0 * x_int.float() / mu - 1.0\n",
        "    return torch.sign(y) * (torch.expm1(torch.abs(y) * math.log1p(mu)) / mu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_audio_optimized(model, seed, num_samples, device, receptive_field, temperature=1.0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start with seed\n",
        "        current_sequence = seed.clone().to(device)\n",
        "        generated = []\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            # Only use the last receptive_field samples as context\n",
        "            if len(current_sequence) > receptive_field:\n",
        "                context = current_sequence[-receptive_field:]\n",
        "            else:\n",
        "                context = current_sequence\n",
        "            \n",
        "            # One-hot encode and shape for model\n",
        "            x = F.one_hot(context, num_classes=256).float().unsqueeze(0).transpose(1, 2)\n",
        "            \n",
        "            # Get predictions\n",
        "            logits = model(x)[0, -1]  # Last timestep\n",
        "            logits = logits / temperature\n",
        "            \n",
        "            # Sample next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1)\n",
        "            \n",
        "            generated.append(next_token.item())\n",
        "            current_sequence = torch.cat([current_sequence, next_token.cpu()])\n",
        "            \n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f\"Generated {i+1}/{num_samples} samples\")\n",
        "    \n",
        "    return torch.tensor(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_receptive_field(dilation_depth, n_blocks, kernel_size):\n",
        "    rf = 1\n",
        "    for _ in range(n_blocks):\n",
        "        for i in range(dilation_depth):\n",
        "            rf += (kernel_size - 1) * (2 ** i)\n",
        "    return rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dilation_depth = 10\n",
        "n_blocks = 4\n",
        "n_dilation_channels = 128\n",
        "n_residual_channels = 128  \n",
        "n_skip_channels = 256\n",
        "n_category = 256\n",
        "kernel_size = 2\n",
        "\n",
        "receptive_field = calculate_receptive_field(dilation_depth, n_blocks, kernel_size)\n",
        "target_length = 2048\n",
        "item_length = receptive_field + target_length\n",
        "\n",
        "print(f\"Receptive field: {receptive_field} samples\")\n",
        "print(f\"Item length: {item_length} samples\")\n",
        "print(f\"Context: {receptive_field/16000:.3f} seconds\")\n",
        "\n",
        "model = Wavenet(dilation_depth, n_blocks, n_dilation_channels,\n",
        "                     n_residual_channels, n_skip_channels, n_category, kernel_size)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Dataloader from your WavenetDataset\n",
        "train_ds = WavenetDataset(\"audio_dataset.npz\",\n",
        "                          item_length=receptive_field + target_length,\n",
        "                          target_length=target_length,\n",
        "                          train=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True, drop_last=True)\n",
        "\n",
        "# Check what the dataset returns\n",
        "x_sample, y_sample = next(iter(train_loader))\n",
        "print(f\"Input shape: {x_sample.shape}\")  # Should be (B, C=256, T)\n",
        "print(f\"Target shape: {y_sample.shape}\")  # Should be (B, target_length)\n",
        "print(f\"Target range: [{y_sample.min()}, {y_sample.max()}]\")  # Should be [0, 255]\n",
        "\n",
        "# Train\n",
        "train_losses = train_wavenet(model, train_loader, epochs, optimizer, device)\n",
        "\n",
        "# Plot\n",
        "plot_losses(train_losses, epochs)\n",
        "\n",
        "# Save\n",
        "save_model(model, \"wavenet.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a sample\n",
        "# use first window of dataset as seed\n",
        "seed_one_hot, seed_targets = next(iter(train_loader))\n",
        "seed_indices = seed_targets[0]  # last target_length tokens\n",
        "generated_tokens = generate_audio(model,\n",
        "                                  seed_indices,  # or a longer seed tensor\n",
        "                                  samples=16000*2,  # generate 1s at 16kHz\n",
        "                                  device=device, receptive_field=receptive_field)\n",
        "\n",
        "# Convert to waveform (float in [-1,1])\n",
        "waveform = inv_mulaw_int_to_float(generated_tokens, mu=255).numpy()\n",
        "\n",
        "# Optional: write to WAV\n",
        "import soundfile as sf\n",
        "sf.write(\"generated.wav\", waveform, samplerate=16000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Audio(waveform, rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Fix 1: Calculate correct receptive field\n",
        "# def calculate_receptive_field(dilation_depth, n_blocks, kernel_size):\n",
        "#     rf_per_block = sum((kernel_size - 1) * (2 ** i) for i in range(dilation_depth))\n",
        "#     total_rf = n_blocks * rf_per_block + 1\n",
        "#     return total_rf\n",
        "\n",
        "# dilation_depth = 10\n",
        "# n_blocks = 2\n",
        "# kernel_size = 2\n",
        "\n",
        "# receptive_field = calculate_receptive_field(dilation_depth, n_blocks, kernel_size)\n",
        "# print(f\"Actual receptive field: {receptive_field}\")  # Should be 2047\n",
        "\n",
        "# target_length = 1024  # Reduce this for faster training\n",
        "\n",
        "# # Fix 2: Corrected WaveNet forward pass\n",
        "# class WavenetFixed(nn.Module):\n",
        "#     def __init__(self, dilation_depth, n_blocks, n_dil_channels, \n",
        "#                  n_residual_channels, n_skip_channels, n_category, kernel_size):\n",
        "#         super(WavenetFixed, self).__init__()\n",
        "#         self.dilation_depth = dilation_depth\n",
        "#         self.n_blocks = n_blocks\n",
        "#         self.n_dil_channels = n_dil_channels\n",
        "#         self.n_residual_channels = n_residual_channels\n",
        "#         self.n_skip_channels = n_skip_channels\n",
        "#         self.n_category = n_category\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.One_hot = OneHot(n_category)\n",
        "\n",
        "#         # Causal convolution - NO padding here, we'll handle it manually\n",
        "#         self.causal = nn.Conv1d(n_category, n_residual_channels, kernel_size)\n",
        "\n",
        "#         # Dilated convolution stacks\n",
        "#         self.filter_convs = nn.ModuleList()\n",
        "#         self.gate_convs = nn.ModuleList()\n",
        "#         self.res_convs = nn.ModuleList()\n",
        "#         self.skip_convs = nn.ModuleList()\n",
        "        \n",
        "#         for _ in range(n_blocks):\n",
        "#             for i in range(dilation_depth):\n",
        "#                 dilation = 2 ** i\n",
        "#                 # Use padding=0, we'll handle causal padding manually\n",
        "#                 self.filter_convs.append(\n",
        "#                     nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "#                               kernel_size, dilation=dilation, padding=0))\n",
        "#                 self.gate_convs.append(\n",
        "#                     nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "#                               kernel_size, dilation=dilation, padding=0))\n",
        "#                 self.res_convs.append(\n",
        "#                     nn.Conv1d(n_dil_channels, n_residual_channels, 1))\n",
        "#                 self.skip_convs.append(\n",
        "#                     nn.Conv1d(n_dil_channels, n_skip_channels, 1))\n",
        "\n",
        "#         # Post-processing\n",
        "#         self.post = nn.Sequential(\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv1d(n_skip_channels, n_skip_channels, 1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv1d(n_skip_channels, n_category, 1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Handle input format\n",
        "#         if x.dim() == 2:\n",
        "#             x_oh = self.One_hot(x)\n",
        "#             x_oh = x_oh.transpose(1, 2)\n",
        "#         else:\n",
        "#             x_oh = x\n",
        "        \n",
        "#         # Apply causal front-end with manual left-padding\n",
        "#         x = F.pad(x_oh, (self.kernel_size - 1, 0))  # Left pad only\n",
        "#         x = self.causal(x)  # Now output length = input length\n",
        "\n",
        "#         skip_connections = []\n",
        "        \n",
        "#         for f, g, r, s in zip(self.filter_convs, self.gate_convs,\n",
        "#                               self.res_convs, self.skip_convs):\n",
        "#             # Manual causal padding\n",
        "#             pad_amount = (self.kernel_size - 1) * f.dilation[0]\n",
        "#             x_padded = F.pad(x, (pad_amount, 0))  # Left pad only\n",
        "            \n",
        "#             # Apply gated activation\n",
        "#             tanh_out = torch.tanh(f(x_padded))\n",
        "#             sigm_out = torch.sigmoid(g(x_padded))\n",
        "#             z = tanh_out * sigm_out\n",
        "            \n",
        "#             # Skip connection\n",
        "#             skip = s(z)\n",
        "#             skip_connections.append(skip)\n",
        "            \n",
        "#             # Residual connection\n",
        "#             res = r(z)\n",
        "#             x = x + res  # Now they should have the same length!\n",
        "\n",
        "#         # Sum skip connections and generate output\n",
        "#         x = sum(skip_connections)\n",
        "#         x = self.post(x)  # (B, n_category, T)\n",
        "#         x = x.transpose(1, 2)  # (B, T, n_category)\n",
        "        \n",
        "#         return x\n",
        "\n",
        "# # Fix 3: Corrected training loop\n",
        "# def train_wavenet_fixed(model, dataloader, epochs, optimizer, device, print_every=50):\n",
        "#     model.train()\n",
        "#     criterion = torch.nn.CrossEntropyLoss()\n",
        "#     train_losses = []\n",
        "\n",
        "#     for epoch in range(1, epochs + 1):\n",
        "#         epoch_loss = 0.0\n",
        "#         num_batches = 0\n",
        "        \n",
        "#         for step, (x, y) in enumerate(dataloader, 1):\n",
        "#             x = x.to(device)  # (B, C, T)\n",
        "#             y = y.to(device)  # (B, target_length)\n",
        "            \n",
        "#             optimizer.zero_grad()\n",
        "#             logits = model(x)  # (B, T_out, C)\n",
        "            \n",
        "#             # Only use the last target_length predictions\n",
        "#             logits = logits[:, -y.size(1):, :]\n",
        "            \n",
        "#             B, T, C = logits.shape\n",
        "            \n",
        "#             # Reshape for loss calculation\n",
        "#             loss = criterion(\n",
        "#                 logits.contiguous().view(B * T, C),\n",
        "#                 y.contiguous().view(B * T)\n",
        "#             )\n",
        "            \n",
        "#             loss.backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#             optimizer.step()\n",
        "            \n",
        "#             epoch_loss += loss.item()\n",
        "#             num_batches += 1\n",
        "\n",
        "#             if step % print_every == 0:\n",
        "#                 avg_loss = epoch_loss / num_batches\n",
        "#                 print(f\"Epoch {epoch} Step {step}: loss {avg_loss:.4f}\")\n",
        "\n",
        "#         avg_epoch_loss = epoch_loss / num_batches\n",
        "#         train_losses.append(avg_epoch_loss)\n",
        "#         print(f\"Epoch {epoch} complete. Average loss: {avg_epoch_loss:.4f}\\n\")\n",
        "\n",
        "#     return train_losses\n",
        "\n",
        "# # Fix 4: Use correct hyperparameters\n",
        "# n_category = 256\n",
        "# n_dilation_channels = 32\n",
        "# n_residual_channels = 32\n",
        "# n_skip_channels = 256\n",
        "\n",
        "# model = WavenetFixed(dilation_depth, n_blocks, n_dilation_channels,\n",
        "#                      n_residual_channels, n_skip_channels, n_category, kernel_size)\n",
        "# model.to(device)\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-5)\n",
        "\n",
        "# # Create dataset with CORRECT receptive field\n",
        "# train_ds = WavenetDataset(\n",
        "#     \"audio_dataset.npz\",\n",
        "#     item_length=receptive_field + target_length,  # Remove the +1\n",
        "#     target_length=target_length,\n",
        "#     train=True\n",
        "# )\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     train_ds, batch_size=8, shuffle=True, drop_last=True, num_workers=0\n",
        "# )\n",
        "\n",
        "# # Train\n",
        "# print(f\"Starting training with receptive field={receptive_field}, target_length={target_length}\")\n",
        "# train_losses = train_wavenet_fixed(model, train_loader, epochs=20, optimizer=optimizer, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Generate a sample\n",
        "# # use first window of dataset as seed\n",
        "# seed_one_hot, seed_targets = next(iter(train_loader))\n",
        "# seed_indices = seed_targets[0]  # last target_length tokens\n",
        "# generated_tokens = generate_audio(model,\n",
        "#                                   seed_indices,  # or a longer seed tensor\n",
        "#                                   samples=16000,  # generate 1s at 16kHz\n",
        "#                                   device=device, receptive_field=receptive_field)\n",
        "\n",
        "# # Convert to waveform (float in [-1,1])\n",
        "# waveform = inv_mulaw_int_to_float(generated_tokens, mu=255).numpy()\n",
        "\n",
        "# # Optional: write to WAV\n",
        "# import soundfile as sf\n",
        "# sf.write(\"generated.wav\", waveform, samplerate=16000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audio(waveform, rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot\n",
        "# plot_losses(train_losses, epochs)\n",
        "\n",
        "# # Save\n",
        "# save_model(model, \"wavenet.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "libreface_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
