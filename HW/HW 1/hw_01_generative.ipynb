{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55K0twjjjFLo"
      },
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "In The Name of God\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Sharif University of Technology - Department of Electrical Engineering\n",
        "</font>\n",
        "<br>\n",
        "<font color=\"#008080\" size=6>\n",
        "Deep Generative Models\n",
        "</font>\n",
        "<hr/>\n",
        "<font color=\"#800080\" size=5>\n",
        "Assignment 1 : Deep Autoregressive Models\n",
        "<br>\n",
        "</font>\n",
        "<font size=5>\n",
        "Instructor: Dr. S. Amini\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Fall 2025\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "</font>\n",
        "<hr>\n",
        "<font color='red'  size=4>\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Feel free to ask your questions in Telegram : @imoonamm\n",
        "</font>\n",
        "<br>\n",
        "<hr>\n",
        "</div></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You should only change the blank sections, marked with TODO**\n",
        "\n",
        "Pay attention to docstrings, as they may drastically help with your implementation.\n",
        "\n",
        "You are advised to read all related papers and material, to help you better understand the design of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1: WaveNet : A generative model for raw audio\n",
        "\n",
        "WaveNet is a general purpose technology that has allowed us to unlock a range of new applications, from improving video calls on even the weakest connections to helping people regain their original voice after losing the ability to speak.\n",
        "\n",
        "![Local GIF](unnamed.gif)\n",
        "\n",
        "WaveNet models raw audio waveforms autoregressively:\n",
        "$\n",
        "p(x_{1:T}) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$\n",
        "\\\n",
        "Instead of predicting spectrograms or using a vocoder, WaveNet predicts the next audio sample directly (often after µ-law companding and 8-bit/256-way quantization). This yields very natural sounding speech and music because the network learns the waveform structure end-to-end.\n",
        "\n",
        "## 2. Core\n",
        "\n",
        "### 2.1 Causal convolutions\n",
        "A causal 1D convolution ensures predictions at time \\(t\\) don't depend on future samples:\n",
        "- Achieved by padding only on the left (past).\n",
        "- In practice, use `padding = dilation * (kernel_size - 1)` and then trim the rightmost elements (or use `Chomp1d` cropping).\n",
        "\n",
        "### 2.2 Dilated convolutions\n",
        "Dilations `[1, 2, 4, 8, ...]` in stacked layers let the receptive field grow exponentially with depth while keeping the number of parameters manageable. A stack of several cycles of dilations covers long temporal context (hundreds to thousands of samples).\n",
        "\n",
        "### 2.3 Gated activation units\n",
        "Each residual block uses a gated unit:\n",
        "$\n",
        "\\text{z} = \\tanh(W_f * x) \\odot \\sigma(W_g * x)\n",
        "$\n",
        "where `*` is dilated causal conv, `W_f` and `W_g` are convolution filters. The output splits to (1) residual connection and (2) skip connection that is added to final output.\n",
        "\n",
        "### 2.4 Residual & skip connections\n",
        "- Residual: `x_{l+1} = x_l + \\text{residual\\_out}` to ease training.\n",
        "- Skip: every block outputs a skip tensor; all skip outputs are summed, then passed through post-processing (ReLU, Conv, softmax) to produce the final distribution over quantized samples.\n",
        "\n",
        "### 2.5 Output quantization / softmax\n",
        "Audio samples are often µ-law quantized to 256 values; WaveNet predicts a categorical distribution (`softmax(256)`) over those values for each time step. Continuous outputs are also possible (mixture of logistics / Gaussians) but discrete softmax is standard in the original paper.\n",
        "\n",
        "### 2.6 Conditioning\n",
        "- **Global conditioning**: a per-utterance vector (e.g., speaker id embedding) is added to layer activations.\n",
        "- **Local conditioning**: low-rate features (e.g., mel spectrogram) are upsampled (transposed conv / nearest) and added at each time step.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training & sampling\n",
        "\n",
        "### Training\n",
        "- Teacher forcing: at training time the network receives true previous samples and learns to predict the next sample with cross-entropy loss (if quantized).\n",
        "- Batch size and optimization: use Adam, gradient clipping recommended for stability.\n",
        "- Preprocess: µ-law companding + 256-level quantization is common.\n",
        "\n",
        "### Sampling (inference)\n",
        "- Autoregressive generation: sample one sample at a time and feed it back.\n",
        "- Slow by default — many acceleration techniques exist (distillation → Parallel Wavenet, caching convolutions, WaveRNN, etc.).\n",
        "\n",
        "\n",
        "\n",
        "**Reference:** [WaveNet: A Generative Model for Raw Audio* (van den Oord et al., DeepMind, 2016)](https://arxiv.org/abs/1609.03499)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchaudio\n",
        "# !pip install git+git://github.com/pytorch/audio\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import ModuleList\n",
        "from IPython.display import Audio\n",
        "from torch.autograd import Variable\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### at first you must convert input into One Hot vector\n",
        "\n",
        "5 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneHot(nn.Module):\n",
        "    def __init__(self, MU):\n",
        "        super(OneHot,self).__init__()\n",
        "        self.MU = MU\n",
        "        self.ones = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.ones is None:\n",
        "            self.ones = torch.ones(self.MU, device=x.device)\n",
        "        return self.ones[x]\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"({})\".format(self.MU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before diving into the WaveNet model itself, it’s important to understand what kind of **data** we are modeling — **raw audio waveforms**.  \n",
        "Neural networks can only process numbers, so we need to represent sound in a numerical form that captures its essential structure and variation.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Sampling Rate and Bit Depth\n",
        "\n",
        "### Sound as a Signal\n",
        "Sound in the real world is a **continuous analog waveform** — a pressure signal that varies smoothly over time.  \n",
        "To process it digitally, we **sample** it: we take discrete measurements of the amplitude at equally spaced time intervals.\n",
        "\n",
        "This process converts a continuous signal into a **time series of numbers**:\n",
        "$\n",
        "x = [x_1, x_2, \\dots, x_T]\n",
        "$\n",
        "where each \\(x_t\\) represents the air pressure (or voltage) at time step \\(t\\).\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling Rate\n",
        "The **sampling rate** defines **how many times per second** we measure the amplitude of the signal.  \n",
        "Typical values:\n",
        "- CD quality audio: 44,100 Hz (samples per second)\n",
        "- Speech datasets: 16,000 Hz or 22,050 Hz\n",
        "\n",
        "A higher sampling rate means more detail, but also more data to process.  \n",
        "WaveNet typically models 16 kHz or 22 kHz audio for speech generation tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### Bit Depth\n",
        "Each recorded sample must be represented by a finite number of bits.  \n",
        "The **bit depth** determines the range and precision of these values:\n",
        "- 8-bit → 256 possible levels  \n",
        "- 16-bit → 65,536 levels  \n",
        "- 24-bit → 16,777,216 levels  \n",
        "\n",
        "The bit depth affects the **signal-to-noise ratio (SNR)** — higher bit depth means less quantization noise and a larger dynamic range:\n",
        "$\n",
        "\\text{SNR} \\approx 6.02 \\times \\text{bit depth} + 1.76\\ \\text{dB}\n",
        "$\n",
        "\n",
        "For example:\n",
        "- 8-bit ≈ 49.9 dB SNR  \n",
        "- 16-bit ≈ 98 dB SNR\n",
        "\n",
        "---\n",
        "\n",
        "## 2. From Audio to Time Series\n",
        "\n",
        "After digitization, the audio signal becomes a **sequence of numbers over time**, also known as a **time series**.  \n",
        "Each point depends on previous ones — this temporal dependency is what WaveNet models autoregressively:\n",
        "$\n",
        "p(x_{1:T}) = \\prod_{t=1}^{T} p(x_t \\mid x_{<t})\n",
        "$\n",
        "\n",
        "However, raw 16-bit audio contains **too many values** (65,536 possible amplitudes).  \n",
        "Predicting the exact next value among that huge range is extremely hard for a neural network.\n",
        "\n",
        "To simplify, we reduce this dynamic range to a smaller set — typically **256 levels** — using a technique called **µ-law companding**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. µ-law Quantization (Companding Transformation)\n",
        "\n",
        "### Motivation\n",
        "Human hearing follows the **Weber–Fechner law** — our perception of changes in loudness is **logarithmic**, not linear.  \n",
        "That means we are more sensitive to small amplitude changes in quiet sounds than in loud ones.\n",
        "\n",
        "µ-law companding exploits this property by **compressing the amplitude range logarithmically** —  \n",
        "it allocates more resolution to small signals (quiet sounds) and less to large ones.\n",
        "\n",
        "---\n",
        "\n",
        "### µ-law Formula\n",
        "\n",
        "$\n",
        "f(x) = \\text{sign}(x) \\cdot \\frac{\\ln(1 + \\mu |x|)}{\\ln(1 + \\mu)}, \\quad -1 \\le x \\le 1\n",
        "$\n",
        "\n",
        "where:\n",
        "- \\(x\\) = input waveform (normalized to [-1, 1])  \n",
        "- \\(\\mu\\) = companding constant (usually 255 for 8-bit quantization)\n",
        "\n",
        "After applying µ-law, we map the range \\([-1, 1]\\) to discrete integer values \\([0, 255]\\).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Wavenet\n",
        "\n",
        "20 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following figure describes the overall architecture of WaveNet. Note that Transformers were non-existent in 2016!\n",
        "\n",
        "![Wavenet Arch](WaveNet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Wavenet(nn.Module):\n",
        "  def __init__(self,dilation_depth, n_blocks, n_dil_channels, n_residual_channels,n_skip_channels, n_category, kernel_size):\n",
        "    super(Wavenet,self).__init__()\n",
        "    self.dilation_depth = dilation_depth\n",
        "    self.n_blocks = n_blocks\n",
        "    self.n_dil_channels = n_dil_channels\n",
        "    self.n_residual_channels = n_residual_channels\n",
        "    self.n_skip_channels = n_skip_channels\n",
        "    self.n_category = n_category\n",
        "    self.kernel_size = kernel_size\n",
        "    self.One_hot = OneHot(n_category)\n",
        "\n",
        "    ###Building the model###\n",
        "    \n",
        "    ##creating first channels##\n",
        "    self.input_conv = nn.Conv1d(n_category, n_residual_channels, kernel_size=1)\n",
        "    \n",
        "    ###Creating wavenet blocks stacks###\n",
        "    # Build dilation rates: [1, 2, 4, 8, ..., 2^(dilation_depth-1)] repeated n_blocks times\n",
        "    self.dilations = []\n",
        "    for block in range(n_blocks):\n",
        "      for layer in range(dilation_depth):\n",
        "        self.dilations.append(2 ** layer)\n",
        "    \n",
        "    # Create WaveNet blocks\n",
        "    self.blocks = nn.ModuleList()\n",
        "    for i, dilation in enumerate(self.dilations):\n",
        "      # Filter and gate convolutions\n",
        "      filter_conv = nn.Conv1d(n_residual_channels, n_dil_channels, \n",
        "                              kernel_size=kernel_size, \n",
        "                              dilation=dilation,\n",
        "                              padding=dilation * (kernel_size - 1))\n",
        "      gate_conv = nn.Conv1d(n_residual_channels, n_dil_channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            dilation=dilation,\n",
        "                            padding=dilation * (kernel_size - 1))\n",
        "      \n",
        "      # Residual connection: 1x1 conv from dilated channels to residual channels\n",
        "      residual_conv = nn.Conv1d(n_dil_channels, n_residual_channels, kernel_size=1)\n",
        "      \n",
        "      # Skip connection: 1x1 conv from dilated channels to skip channels\n",
        "      skip_conv = nn.Conv1d(n_dil_channels, n_skip_channels, kernel_size=1)\n",
        "      \n",
        "      self.blocks.append(nn.ModuleDict({\n",
        "        'filter': filter_conv,\n",
        "        'gate': gate_conv,\n",
        "        'residual': residual_conv,\n",
        "        'skip': skip_conv\n",
        "      }))\n",
        "    \n",
        "    ##post convoluions\n",
        "    # Post-processing: ReLU -> conv -> ReLU -> output conv\n",
        "    self.post_conv1 = nn.Conv1d(n_skip_channels, n_skip_channels, kernel_size=1)\n",
        "    self.post_conv2 = nn.Conv1d(n_skip_channels, n_category, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape: [batch_size, seq_len] with integer values in [0, n_category-1]\n",
        "    \n",
        "    # Convert to one-hot: [batch_size, seq_len] -> [batch_size, seq_len, n_category]\n",
        "    x_onehot = self.One_hot(x)  # [batch_size, seq_len, n_category]\n",
        "    \n",
        "    # Transpose for conv1d: [batch_size, n_category, seq_len]\n",
        "    x_onehot = x_onehot.transpose(1, 2)\n",
        "    \n",
        "    # Input convolution\n",
        "    x = self.input_conv(x_onehot)  # [batch_size, n_residual_channels, seq_len]\n",
        "    \n",
        "    # Initialize skip connections accumulator\n",
        "    skip_outputs = []\n",
        "    \n",
        "    # Pass through all WaveNet blocks\n",
        "    for block in self.blocks:\n",
        "      # Store input for residual connection\n",
        "      residual_input = x\n",
        "      \n",
        "      # Dilated causal convolutions\n",
        "      filter_out = block['filter'](x)  # [batch_size, n_dil_channels, seq_len]\n",
        "      gate_out = block['gate'](x)      # [batch_size, n_dil_channels, seq_len]\n",
        "      \n",
        "      # Gated activation: tanh(filter) * sigmoid(gate)\n",
        "      gated = torch.tanh(filter_out) * torch.sigmoid(gate_out)\n",
        "      \n",
        "      # Residual connection: add to input\n",
        "      residual = block['residual'](gated)  # [batch_size, n_residual_channels, seq_len]\n",
        "      x = residual_input + residual\n",
        "      \n",
        "      # Skip connection: collect for later\n",
        "      skip = block['skip'](gated)  # [batch_size, n_skip_channels, seq_len]\n",
        "      skip_outputs.append(skip)\n",
        "    \n",
        "    # Sum all skip connections\n",
        "    skip_sum = sum(skip_outputs)  # [batch_size, n_skip_channels, seq_len]\n",
        "    \n",
        "    # Post-processing\n",
        "    out = torch.relu(skip_sum)\n",
        "    out = self.post_conv1(out)\n",
        "    out = torch.relu(out)\n",
        "    out = self.post_conv2(out)  # [batch_size, n_category, seq_len]\n",
        "    \n",
        "    # Transpose back: [batch_size, seq_len, n_category]\n",
        "    out = out.transpose(1, 2)\n",
        "    \n",
        "    # Return logits: [batch_size, seq_len, n_category]\n",
        "    return out\n",
        "  \n",
        "  ###Function to generate samples###\n",
        "  def generate(self, input, num_samples=100):\n",
        "      # input shape: [batch_size, seq_len] or [seq_len] with integer values\n",
        "      self.eval()\n",
        "      gen_list = []\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        # Ensure input is 2D and on correct device\n",
        "        if input.dim() == 1:\n",
        "          input = input.unsqueeze(0)\n",
        "        if isinstance(input, torch.Tensor):\n",
        "          input = input.to(next(self.parameters()).device)\n",
        "        else:\n",
        "          input = torch.tensor(input, device=next(self.parameters()).device)\n",
        "        \n",
        "        # Start with input sequence\n",
        "        current_seq = input.clone()\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "          # Forward pass to get logits for next sample\n",
        "          logits = self.forward(current_seq)  # [batch_size, seq_len, n_category]\n",
        "          \n",
        "          # Get logits for the last time step\n",
        "          next_logits = logits[:, -1, :]  # [batch_size, n_category]\n",
        "          \n",
        "          # Sample from categorical distribution\n",
        "          probs = torch.softmax(next_logits, dim=-1)\n",
        "          next_sample = torch.multinomial(probs, 1)  # [batch_size, 1]\n",
        "          \n",
        "          # Append to generation list\n",
        "          if next_sample.size(0) == 1:\n",
        "            gen_list.append(next_sample.item())\n",
        "          else:\n",
        "            gen_list.append(next_sample.squeeze().cpu().numpy().tolist())\n",
        "          \n",
        "          # Append to current sequence\n",
        "          current_seq = torch.cat([current_seq, next_sample], dim=1)\n",
        "      \n",
        "      return gen_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantization of 16 bit audio\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mulaw_quantize(x, qc):\n",
        "    # µ-law companding constant\n",
        "    mu = qc - 1\n",
        "    \n",
        "    # Ensure input is in [-1, 1] range\n",
        "    x = np.clip(x, -1.0, 1.0)\n",
        "    \n",
        "    # Apply µ-law companding: sign(x) * ln(1 + μ|x|) / ln(1 + μ)\n",
        "    sign_x = np.sign(x)\n",
        "    abs_x = np.abs(x)\n",
        "    x_mu = sign_x * np.log(1 + mu * abs_x) / np.log(1 + mu)\n",
        "    \n",
        "    # Scale from [-1, 1] to [0, qc-1] and quantize\n",
        "    x_mu = (x_mu + 1) / 2 * (qc - 1)\n",
        "    x_mu = np.round(x_mu).astype(np.int32)\n",
        "    x_mu = np.clip(x_mu, 0, qc - 1)\n",
        "    \n",
        "    return x_mu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inverse quantization\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inv_mulaw_quantize(x_mu, quantization_channels=256, device = device):\n",
        "    # Handle both numpy arrays and torch tensors\n",
        "    is_torch = False\n",
        "    if isinstance(x_mu, torch.Tensor):\n",
        "        is_torch = True\n",
        "        x_mu_np = x_mu.detach().cpu().numpy()\n",
        "    else:\n",
        "        x_mu_np = np.asarray(x_mu)\n",
        "    \n",
        "    # µ-law companding constant\n",
        "    mu = quantization_channels - 1\n",
        "    \n",
        "    # Ensure input is in valid range\n",
        "    x_mu_np = np.clip(x_mu_np, 0, quantization_channels - 1)\n",
        "    \n",
        "    # Convert from [0, qc-1] back to [-1, 1] range\n",
        "    y = (x_mu_np / (quantization_channels - 1)) * 2.0 - 1.0\n",
        "    \n",
        "    # Apply inverse µ-law: x = sign(y) * ((1 + μ)^|y| - 1) / μ\n",
        "    sign_y = np.sign(y)\n",
        "    abs_y = np.abs(y)\n",
        "    x = sign_y * ((1 + mu) ** abs_y - 1) / mu\n",
        "    \n",
        "    # Ensure output is in [-1, 1] range\n",
        "    x = np.clip(x, -1.0, 1.0)\n",
        "    \n",
        "    # Convert back to torch tensor if input was torch tensor\n",
        "    if is_torch:\n",
        "        x = torch.from_numpy(x).float().to(device)\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "In this part you must load the audio_dataset.npz that exist in zip file of homework\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import math\n",
        "import threading\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import librosa as lr\n",
        "import bisect\n",
        "\n",
        "\n",
        "class WavenetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_file,\n",
        "                 item_length,\n",
        "                 target_length,\n",
        "                 file_location=None,\n",
        "                 classes=256,\n",
        "                 sampling_rate=16000,\n",
        "                 mono=True,\n",
        "                 normalize=False,\n",
        "                 dtype=np.uint8,\n",
        "                 train=True,\n",
        "                 test_stride=100):\n",
        "\n",
        "        #           |----receptive_field----|\n",
        "        #                                 |--output_length--|\n",
        "        # example:  | | | | | | | | | | | | | | | | | | | | |\n",
        "        # target:                           | | | | | | | | | |\n",
        "\n",
        "        self.dataset_file = dataset_file\n",
        "        self._item_length = item_length\n",
        "        self._test_stride = test_stride\n",
        "        self.target_length = target_length\n",
        "        self.classes = classes\n",
        "\n",
        "        if not os.path.isfile(dataset_file):\n",
        "            assert file_location is not None, \"no location for dataset files specified\"\n",
        "            self.mono = mono\n",
        "            self.normalize = normalize\n",
        "\n",
        "            self.sampling_rate = sampling_rate\n",
        "            self.dtype = dtype\n",
        "            self.create_dataset(file_location, dataset_file)\n",
        "        else:\n",
        "            # Unknown parameters of the stored dataset\n",
        "            # TODO Can these parameters be stored, too?\n",
        "            self.mono = None\n",
        "            self.normalize = None\n",
        "\n",
        "            self.sampling_rate = None\n",
        "            self.dtype = None\n",
        "\n",
        "        self.data = np.load(self.dataset_file, mmap_mode='r')\n",
        "        self.start_samples = [0]\n",
        "        self._length = 0\n",
        "        self.calculate_length()\n",
        "        self.train = train\n",
        "        print(\"one hot input\")\n",
        "        # assign every *test_stride*th item to the test set\n",
        "\n",
        "\n",
        "    def calculate_length(self):\n",
        "        start_samples = [0]\n",
        "        for i in range(len(self.data.keys())):\n",
        "            start_samples.append(start_samples[-1] + len(self.data['arr_' + str(i)]))\n",
        "        available_length = start_samples[-1] - (self._item_length - (self.target_length - 1)) - 1\n",
        "        self._length = math.floor(available_length / self.target_length)\n",
        "        self.start_samples = start_samples\n",
        "\n",
        "    def set_item_length(self, l):\n",
        "        self._item_length = l\n",
        "        self.calculate_length()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Calculate sample_index considering train/test split with test_stride\n",
        "        # Every test_stride-th sample (at indices test_stride-1, 2*test_stride-1, ...) goes to test set\n",
        "        if self.train:\n",
        "            # For training: find the idx-th sample that is NOT a test sample\n",
        "            # Test samples are at: test_stride-1, 2*test_stride-1, 3*test_stride-1, ...\n",
        "            # Formula: sample_idx = idx + floor((idx + test_stride - 1) / (test_stride - 1))\n",
        "            sample_idx = idx + ((idx + self._test_stride - 1) // (self._test_stride - 1))\n",
        "        else:\n",
        "            # For test: use every test_stride-th sample\n",
        "            # Test samples are at: test_stride-1, 2*test_stride-1, 3*test_stride-1, ...\n",
        "            sample_idx = (idx + 1) * self._test_stride - 1\n",
        "        \n",
        "        # Calculate the actual position in the concatenated data\n",
        "        sample_position = sample_idx * self.target_length\n",
        "        \n",
        "        # Find which file (array) contains this sample using bisect\n",
        "        file_index = bisect.bisect_right(self.start_samples, sample_position) - 1\n",
        "        \n",
        "        # Check file_index bounds\n",
        "        if file_index < 0 or file_index >= len(self.start_samples) - 1:\n",
        "            raise IndexError(f\"file_index {file_index} out of bounds for sample_position {sample_position}\")\n",
        "        \n",
        "        # Calculate offset within the file\n",
        "        offset_in_file = sample_position - self.start_samples[file_index]\n",
        "        \n",
        "        # Get the data array (already cached in self.data)\n",
        "        file_data = self.data['arr_' + str(file_index)]\n",
        "        \n",
        "        # Check if we need data from multiple files (cross-file case)\n",
        "        end_position = sample_position + self._item_length\n",
        "        end_file_index = bisect.bisect_right(self.start_samples, end_position) - 1\n",
        "        \n",
        "        if file_index == end_file_index:\n",
        "            # Single file case: extract directly\n",
        "            segment = file_data[offset_in_file:offset_in_file + self._item_length]\n",
        "        else:\n",
        "            # Cross-file case: concatenate arrays\n",
        "            segments = [file_data[offset_in_file:]]\n",
        "            for fi in range(file_index + 1, end_file_index + 1):\n",
        "                if fi < len(self.start_samples) - 1:\n",
        "                    segments.append(self.data['arr_' + str(fi)])\n",
        "            # Take only what we need from the last file\n",
        "            last_file_data = self.data['arr_' + str(end_file_index)]\n",
        "            last_offset = end_position - self.start_samples[end_file_index]\n",
        "            segments.append(last_file_data[:last_offset])\n",
        "            segment = np.concatenate(segments)\n",
        "        \n",
        "        # Ensure we have the right length\n",
        "        if len(segment) < self._item_length:\n",
        "            # Pad with zeros if needed (shouldn't happen normally)\n",
        "            segment = np.pad(segment, (0, self._item_length - len(segment)), mode='constant')\n",
        "        segment = segment[:self._item_length]\n",
        "        \n",
        "        # Split into input (receptive field) and target (output)\n",
        "        # Based on the diagram: input is receptive_field, target is output_length at the end\n",
        "        # Input: first (item_length - target_length + 1) samples (receptive field)\n",
        "        # Target: samples that come after the receptive field (next target_length samples)\n",
        "        receptive_field_length = self._item_length - self.target_length + 1\n",
        "        input_segment = segment[:receptive_field_length]\n",
        "        # Target should be the next samples after the receptive field\n",
        "        target_segment = segment[receptive_field_length:receptive_field_length + self.target_length]\n",
        "        \n",
        "        # Ensure target has correct length\n",
        "        if len(target_segment) < self.target_length:\n",
        "            target_segment = np.pad(target_segment, (0, self.target_length - len(target_segment)), mode='edge')\n",
        "        target_segment = target_segment[:self.target_length]\n",
        "        \n",
        "        # Convert to torch tensors\n",
        "        input_tensor = torch.from_numpy(input_segment).long()\n",
        "        target_tensor = torch.from_numpy(target_segment).long()\n",
        "        \n",
        "        # Create one-hot encoding for input (target stays as integers for loss calculation)\n",
        "        # Note: One-hot is memory heavy, but required by current model architecture\n",
        "        one_hot = torch.zeros(len(input_tensor), self.classes, dtype=torch.float32)\n",
        "        one_hot.scatter_(1, input_tensor.unsqueeze(1), 1.0)\n",
        "        \n",
        "        return one_hot, target_tensor\n",
        "    \n",
        "    def create_dataset(self, file_location, dataset_file):\n",
        "        \"\"\"Create dataset npz file from audio files in file_location.\"\"\"\n",
        "        audio_files = list_all_audio_files(file_location)\n",
        "        all_data = []\n",
        "        \n",
        "        for audio_file in audio_files:\n",
        "            # Load audio file\n",
        "            audio_data, sr = lr.load(audio_file, sr=self.sampling_rate, mono=self.mono)\n",
        "            \n",
        "            # Normalize if requested\n",
        "            if self.normalize:\n",
        "                audio_data = audio_data / np.max(np.abs(audio_data)) if np.max(np.abs(audio_data)) > 0 else audio_data\n",
        "            \n",
        "            # Apply mu-law encoding and quantization\n",
        "            mu = self.classes - 1\n",
        "            quantized = mu_law_encoding(audio_data, mu)\n",
        "            quantized = quantize_data(quantized, self.classes)\n",
        "            \n",
        "            all_data.append(quantized)\n",
        "        \n",
        "        # Save as npz with multiple arrays\n",
        "        np.savez_compressed(dataset_file, *all_data)\n",
        "        print(f\"Created dataset with {len(all_data)} audio files\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        test_length = math.floor(self._length / self._test_stride)\n",
        "        if self.train:\n",
        "            return self._length - test_length\n",
        "        else:\n",
        "            return test_length\n",
        "\n",
        "\n",
        "def quantize_data(data, classes):\n",
        "    \"\"\"\n",
        "    Quantize data to discrete classes in range [0, classes-1].\n",
        "    \n",
        "    Args:\n",
        "        data: Input data (typically mu-law encoded values in [-1, 1])\n",
        "        classes: Number of quantization classes\n",
        "    \n",
        "    Returns:\n",
        "        quantized: Quantized data as integers in [0, classes-1]\n",
        "    \"\"\"\n",
        "    # Ensure data is in [-1, 1] range\n",
        "    data = np.clip(data, -1.0, 1.0)\n",
        "    \n",
        "    # Scale from [-1, 1] to [0, classes-1] and quantize\n",
        "    quantized = (data + 1.0) / 2.0 * (classes - 1)\n",
        "    quantized = np.round(quantized).astype(np.int32)\n",
        "    quantized = np.clip(quantized, 0, classes - 1)\n",
        "    \n",
        "    return quantized\n",
        "\n",
        "\n",
        "def list_all_audio_files(location):\n",
        "    \"\"\"\n",
        "    List all audio files in the given location directory.\n",
        "    \n",
        "    Args:\n",
        "        location: Directory path containing audio files\n",
        "    \n",
        "    Returns:\n",
        "        audio_files: List of paths to audio files\n",
        "    \"\"\"\n",
        "    audio_files = []\n",
        "    # Common audio file extensions\n",
        "    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac']\n",
        "    \n",
        "    if os.path.isdir(location):\n",
        "        for root, dirs, files in os.walk(location):\n",
        "            for file in files:\n",
        "                if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
        "                    audio_files.append(os.path.join(root, file))\n",
        "    elif os.path.isfile(location):\n",
        "        # If it's a single file, return it\n",
        "        if any(location.lower().endswith(ext) for ext in audio_extensions):\n",
        "            audio_files.append(location)\n",
        "    \n",
        "    return sorted(audio_files)\n",
        "\n",
        "\n",
        "def mu_law_encoding(data, mu):\n",
        "    \"\"\"\n",
        "    Apply mu-law encoding (companding) to audio data.\n",
        "    \n",
        "    Args:\n",
        "        data: Input audio data (normalized to [-1, 1])\n",
        "        mu: Mu-law companding constant (typically 255 for 8-bit)\n",
        "    \n",
        "    Returns:\n",
        "        mu_x: Mu-law encoded data in [-1, 1] range\n",
        "    \"\"\"\n",
        "    # Ensure input is in [-1, 1] range\n",
        "    data = np.clip(data, -1.0, 1.0)\n",
        "    \n",
        "    # Apply mu-law companding: sign(x) * ln(1 + μ|x|) / ln(1 + μ)\n",
        "    sign_data = np.sign(data)\n",
        "    abs_data = np.abs(data)\n",
        "    mu_x = sign_data * np.log(1 + mu * abs_data) / np.log(1 + mu)\n",
        "    \n",
        "    return mu_x\n",
        "\n",
        "\n",
        "def mu_law_expansion(data, mu):\n",
        "    \"\"\"\n",
        "    Apply inverse mu-law (expansion) to recover original audio data.\n",
        "    \n",
        "    Args:\n",
        "        data: Mu-law encoded data in [-1, 1] range\n",
        "        mu: Mu-law companding constant (typically 255 for 8-bit)\n",
        "    \n",
        "    Returns:\n",
        "        s: Recovered audio data in [-1, 1] range\n",
        "    \"\"\"\n",
        "    # Ensure input is in [-1, 1] range\n",
        "    data = np.clip(data, -1.0, 1.0)\n",
        "    \n",
        "    # Apply inverse mu-law: sign(y) * ((1 + μ)^|y| - 1) / μ\n",
        "    sign_data = np.sign(data)\n",
        "    abs_data = np.abs(data)\n",
        "    s = sign_data * ((1 + mu) ** abs_data - 1) / mu\n",
        "    \n",
        "    # Ensure output is in [-1, 1] range\n",
        "    s = np.clip(s, -1.0, 1.0)\n",
        "    \n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dilation_depth = 10\n",
        "n_blocks = 2\n",
        "n_dilation_channels = 24\n",
        "n_residual_channels = 24\n",
        "n_skip_channels = 128\n",
        "n_category = 128\n",
        "kernel_size = 2\n",
        "model = Wavenet(dilation_depth,n_blocks,n_dilation_channels ,n_residual_channels,n_skip_channels,n_category,kernel_size)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define data generator\n",
        "\n",
        "10 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_generation(data,fr,seq_len_segment , mu, device = device):\n",
        "    \"\"\"\n",
        "    Generate random segments from audio data for training.\n",
        "    \n",
        "    Args:\n",
        "        data: Audio data with shape [batch_size, seq_len] (normalized to [-1, 1])\n",
        "        fr: Not used (kept for compatibility)\n",
        "        seq_len_segment: Length of segment to extract\n",
        "        mu: Mu-law constant (typically 255)\n",
        "        device: Device to place tensors on\n",
        "    \n",
        "    Returns:\n",
        "        input_segments: One-hot encoded input segments [batch_size, seq_len_segment, n_category]\n",
        "        target_segments: Target segments as integers [batch_size, seq_len_segment]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = data.shape\n",
        "    \n",
        "    # Randomly choose starting positions for each sample in batch\n",
        "    max_start = seq_len - seq_len_segment\n",
        "    start_positions = np.random.randint(0, max(1, max_start), size=batch_size)\n",
        "    \n",
        "    # Extract segments\n",
        "    segments = []\n",
        "    for i in range(batch_size):\n",
        "        start = start_positions[i]\n",
        "        segment = data[i, start:start + seq_len_segment]\n",
        "        segments.append(segment)\n",
        "    \n",
        "    segments = np.array(segments)  # [batch_size, seq_len_segment]\n",
        "    \n",
        "    # Apply mu-law encoding and quantization\n",
        "    mu_encoded = mu_law_encoding(segments, mu)\n",
        "    quantized = quantize_data(mu_encoded, mu + 1)  # mu + 1 = number of classes\n",
        "    \n",
        "    # Convert to torch tensors\n",
        "    quantized_tensor = torch.from_numpy(quantized).long().to(device)\n",
        "    \n",
        "    # Split into input and target (target is shifted by 1 for next-step prediction)\n",
        "    input_segments = quantized_tensor[:, :-1]  # [batch_size, seq_len_segment - 1]\n",
        "    target_segments = quantized_tensor[:, 1:]  # [batch_size, seq_len_segment - 1]\n",
        "    \n",
        "    # Create one-hot encoding for input\n",
        "    n_classes = mu + 1\n",
        "    one_hot = torch.zeros(batch_size, input_segments.size(1), n_classes, dtype=torch.float32, device=device)\n",
        "    one_hot.scatter_(2, input_segments.unsqueeze(2), 1.0)\n",
        "    \n",
        "    return one_hot, target_segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize sample of data\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample from the dataset to visualize\n",
        "# Assuming dataset is loaded (you'll need to load it first)\n",
        "# Example visualization code:\n",
        "\n",
        "# Load dataset (you'll need to provide actual paths)\n",
        "# dataset = WavenetDataset(dataset_file='audio_dataset.npz', \n",
        "#                          item_length=1000, \n",
        "#                          target_length=100,\n",
        "#                          classes=128,\n",
        "#                          train=True)\n",
        "\n",
        "# Get a sample\n",
        "# one_hot, target = dataset[0]\n",
        "\n",
        "# Convert one-hot back to integers for visualization\n",
        "# input_ints = torch.argmax(one_hot, dim=1).cpu().numpy()\n",
        "# target_ints = target.cpu().numpy()\n",
        "\n",
        "# Plot\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.subplot(2, 1, 1)\n",
        "# plt.plot(input_ints[:200])  # Plot first 200 samples\n",
        "# plt.title('Input Audio Sample (Quantized)')\n",
        "# plt.ylabel('Quantized Value')\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.subplot(2, 1, 2)\n",
        "# plt.plot(target_ints[:200])  # Plot first 200 samples\n",
        "# plt.title('Target Audio Sample (Quantized)')\n",
        "# plt.xlabel('Time Step')\n",
        "# plt.ylabel('Quantized Value')\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "print(\"Visualization code ready. Uncomment and provide dataset to visualize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer and Loss Function\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Optimizer: Adam optimizer with learning rate\n",
        "learning_rate = 1e-3\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loss function: Cross-entropy for categorical prediction\n",
        "# Input: logits [batch_size, seq_len, n_category]\n",
        "# Target: integers [batch_size, seq_len] in range [0, n_category-1]\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "print(f\"Optimizer: Adam with lr={learning_rate}\")\n",
        "print(f\"Loss function: CrossEntropyLoss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 4\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Load dataset (adjust paths as needed)\n",
        "# dataset = WavenetDataset(dataset_file='audio_dataset.npz',\n",
        "#                          item_length=1000,\n",
        "#                          target_length=100,\n",
        "#                          classes=n_category,\n",
        "#                          train=True)\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Uncomment when dataset is loaded:\n",
        "    # for batch_idx, (one_hot, target) in enumerate(dataloader):\n",
        "    #     # Move to device\n",
        "    #     one_hot = one_hot.to(device)\n",
        "    #     target = target.to(device)\n",
        "    #     \n",
        "    #     # Forward pass\n",
        "    #     # Convert one-hot to integer indices for model input\n",
        "    #     input_ints = torch.argmax(one_hot, dim=2).long()  # [batch_size, seq_len]\n",
        "    #     logits = model(input_ints)  # [batch_size, seq_len, n_category]\n",
        "    #     \n",
        "    #     # Reshape for loss calculation\n",
        "    #     logits_flat = logits.view(-1, n_category)  # [batch_size * seq_len, n_category]\n",
        "    #     target_flat = target.view(-1)  # [batch_size * seq_len]\n",
        "    #     \n",
        "    #     # Calculate loss\n",
        "    #     loss = criterion(logits_flat, target_flat)\n",
        "    #     \n",
        "    #     # Backward pass\n",
        "    #     optimizer.zero_grad()\n",
        "    #     loss.backward()\n",
        "    #     \n",
        "    #     # Gradient clipping\n",
        "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    #     \n",
        "    #     optimizer.step()\n",
        "    #     \n",
        "    #     epoch_loss += loss.item()\n",
        "    #     num_batches += 1\n",
        "    #     \n",
        "    #     if batch_idx % 10 == 0:\n",
        "    #         print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "    \n",
        "    # avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    # train_losses.append(avg_loss)\n",
        "    # print(f'Epoch {epoch} completed. Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "print(\"Training loop ready. Uncomment and provide dataset to start training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot losses\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_losses(trainLoss, Epochs):\n",
        "    \"\"\"\n",
        "    Plot training losses over epochs.\n",
        "    \n",
        "    Args:\n",
        "        trainLoss: List of training losses (one per epoch)\n",
        "        Epochs: List or range of epoch numbers\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(Epochs, trainLoss, 'b-', label='Training Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.title('Training Loss over Epochs', fontsize=14)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Final training loss: {trainLoss[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model\n",
        "\n",
        "5 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "save_path = 'wavenet_model.pth'\n",
        "\n",
        "# Save model state dict\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'dilation_depth': dilation_depth,\n",
        "    'n_blocks': n_blocks,\n",
        "    'n_dilation_channels': n_dilation_channels,\n",
        "    'n_residual_channels': n_residual_channels,\n",
        "    'n_skip_channels': n_skip_channels,\n",
        "    'n_category': n_category,\n",
        "    'kernel_size': kernel_size,\n",
        "}, save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# To load the model later:\n",
        "# checkpoint = torch.load(save_path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Audio using trained model\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate audio using trained model\n",
        "model.eval()\n",
        "\n",
        "# Start with a seed sequence (random or from dataset)\n",
        "# seed_length = 100\n",
        "# seed = torch.randint(0, n_category, (1, seed_length)).to(device)\n",
        "\n",
        "# Or load a seed from dataset:\n",
        "# dataset = WavenetDataset(dataset_file='audio_dataset.npz',\n",
        "#                          item_length=100,\n",
        "#                          target_length=10,\n",
        "#                          classes=n_category,\n",
        "#                          train=False)\n",
        "# one_hot, _ = dataset[0]\n",
        "# seed = torch.argmax(one_hot, dim=1).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate samples\n",
        "num_samples_to_generate = 1000\n",
        "# generated_samples = model.generate(seed, num_samples=num_samples_to_generate)\n",
        "\n",
        "# Convert quantized samples back to audio\n",
        "# generated_array = np.array(generated_samples, dtype=np.int32)\n",
        "# \n",
        "# # Inverse mu-law quantization\n",
        "# mu = n_category - 1\n",
        "# audio_samples = inv_mulaw_quantize(generated_array, quantization_channels=n_category, device=device)\n",
        "# \n",
        "# # Convert to numpy if needed\n",
        "# if isinstance(audio_samples, torch.Tensor):\n",
        "#     audio_samples = audio_samples.cpu().numpy()\n",
        "# \n",
        "# # Save as audio file\n",
        "# output_path = 'generated_audio.wav'\n",
        "# import soundfile as sf\n",
        "# sf.write(output_path, audio_samples, 16000)  # 16kHz sampling rate\n",
        "# \n",
        "# # Or play directly in notebook\n",
        "# from IPython.display import Audio\n",
        "# Audio(audio_samples, rate=16000)\n",
        "\n",
        "print(\"Audio generation code ready. Uncomment and provide seed to generate audio.\")\n",
        "print(\"Make sure model is trained before generating!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 2: Pixel CNN / Pixel RNN (Optional)\n",
        "\n",
        "In this question, we explore **deep autoregressive models** for image generation, focusing on PixelCNN and PixelRNN.  \n",
        "\n",
        "Natural images are high-dimensional and exhibit strong spatial correlations between neighboring pixels. The **PixelRNN** aims to model the **full joint distribution** of pixel intensities, enabling exact likelihood computation and realistic image generation.  \n",
        "\n",
        "Unlike VAEs or GANs, which rely on latent variables or adversarial training, PixelRNN treats the **image as a sequence of pixels** and directly models:  \n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}) = \\prod_{i=1}^{N} p(x_i \\mid x_1, x_2, \\ldots, x_{i-1})\n",
        "$$\n",
        "\n",
        "Here, each pixel \\(x_i\\) is conditioned on all previously generated pixels, following a fixed **raster-scan order** (top-left → bottom-right). This transforms image modeling into a **sequence modeling task**, allowing recurrent neural networks (RNNs) to capture long-range spatial dependencies.  \n",
        "\n",
        "---\n",
        "\n",
        "### Core Idea\n",
        "\n",
        "PixelRNN introduces **two-dimensional LSTM architectures** to preserve autoregressive dependencies in images:\n",
        "\n",
        "- **Row LSTM:** Processes the image row by row (left-to-right), with recurrent connections across both rows and columns.  \n",
        "- **Diagonal Bi-LSTM:** Processes pixels along diagonals, increasing the receptive field to include all previously generated pixels.  \n",
        "\n",
        "To maintain causality across color channels, each pixel’s RGB values are generated sequentially (R → G → B).  \n",
        "\n",
        "![PixelRNN Architecture](image.png)\n",
        "\n",
        "---\n",
        "\n",
        "### Architectural Components\n",
        "\n",
        "1. **Masked Convolutions**  \n",
        "   Convolutional filters are masked (type A or B) to prevent access to future pixels or channels during training.  \n",
        "\n",
        "2. **Residual Connections**  \n",
        "   Stacked recurrent layers use residual links to facilitate gradient flow in deep networks.  \n",
        "\n",
        "3. **Discrete Softmax Output**  \n",
        "   Each pixel channel is modeled as a categorical distribution over 256 intensity values, allowing exact log-likelihood computation.  \n",
        "\n",
        "4. **Loss Function**  \n",
        "   The training objective is the **negative log-likelihood** of the true pixels under the predicted distributions:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_i \\log p(x_i \\mid x_{<i})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling and Generation\n",
        "\n",
        "Image generation in PixelRNN is **fully autoregressive**:\n",
        "\n",
        "1. Start at the top-left pixel.  \n",
        "2. Sequentially sample each pixel from its predicted conditional distribution.  \n",
        "3. Repeat until the entire image is generated.  \n",
        "\n",
        "This method produces coherent, high-quality images but is computationally intensive, as each pixel depends on all previously generated pixels.  \n",
        "\n",
        "---\n",
        "\n",
        "**Reference:** [PixelRNN Paper (2016)](https://arxiv.org/abs/1601.06759)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQss1Y83nYf1"
      },
      "source": [
        "## load Cifar10\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9vwArxJJj1z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import struct\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "# CIFAR-10 constants\n",
        "IMAGE_SIZE = 24  # CIFAR-10 images are 32x32, but we crop to 24x24\n",
        "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
        "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000  # CIFAR-10 training set has 50,000 images\n",
        "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000  # CIFAR-10 test set has 10,000 images\n",
        "\n",
        "\n",
        "def _read_cifar10_binary_file(path: str) -> List[Tuple[int, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Reads a single CIFAR-10 binary file and returns a list of (label, image_array)\n",
        "    where image_array is HxWxC uint8 (32x32x3).\n",
        "    CIFAR-10 binary format: 1 byte label followed by 3072 bytes image (R(1024), G(1024), B(1024))\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    record_size = 1 + 32 * 32 * 3\n",
        "    with open(path, 'rb') as f:\n",
        "        while True:\n",
        "            bytes_read = f.read(record_size)\n",
        "            if not bytes_read:\n",
        "                break\n",
        "            if len(bytes_read) != record_size:\n",
        "                raise ValueError(f\"Unexpected record size in {path}: {len(bytes_read)} bytes\")\n",
        "            # first byte is label\n",
        "            label = bytes_read[0]\n",
        "            img_flat = np.frombuffer(bytes_read[1:], dtype=np.uint8)\n",
        "            # image is stored as [R..1024, G..1024, B..1024] each row-major for 32x32\n",
        "            # reshape to (3, 32, 32) then transpose to (32, 32, 3)\n",
        "            depth_major = img_flat.reshape((3, 32, 32))\n",
        "            img = np.transpose(depth_major, (1, 2, 0))  # H, W, C\n",
        "            records.append((int(label), img))\n",
        "    return records\n",
        "\n",
        "class CIFAR10BinaryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that reads the CIFAR-10 binary files into memory (list of (label, image)).\n",
        "    Applies transforms (train/eval) provided in init.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir: str, files: List[str], transform=None):\n",
        "        \"\"\"\n",
        "        data_dir: directory containing the CIFAR-10 binary files.\n",
        "        files: list of filenames (basename) to read from data_dir.\n",
        "        transform: torchvision transform to apply to PIL image (or custom transform).\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.files = files\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all records into memory (50k images is fine)\n",
        "        self.records: List[Tuple[int, np.ndarray]] = []\n",
        "        for fname in files:\n",
        "            path = os.path.join(data_dir, fname)\n",
        "            if not os.path.exists(path):\n",
        "                raise ValueError(f\"Failed to find file: {path}\")\n",
        "            self.records.extend(_read_cifar10_binary_file(path))\n",
        "\n",
        "        if len(self.records) == 0:\n",
        "            raise ValueError(\"No records loaded from CIFAR files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, img_np = self.records[idx]\n",
        "        # Convert to PIL Image (expects HxW or HxWxC)\n",
        "        img = Image.fromarray(img_np)  # mode='RGB'\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # After transforms, we expect a torch.FloatTensor image of shape [C, H, W]\n",
        "        # Return label as long tensor\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "class PerImageStandardization:\n",
        "    \"\"\"\n",
        "    Mimics TensorFlow's per_image_standardization:\n",
        "      (x - mean) / max(stddev, 1.0/sqrt(N))\n",
        "    Works on a torch tensor image with shape (C, H, W) and dtype=float32 in range [0,1] or raw values.\n",
        "    We'll expect inputs are float tensors in range [0,1].\n",
        "    \"\"\"\n",
        "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        # tensor: C x H x W\n",
        "        if not torch.is_floating_point(tensor):\n",
        "            tensor = tensor.float()\n",
        "        mean = tensor.mean()\n",
        "        std = tensor.std()\n",
        "        # N = number of pixels * channels\n",
        "        N = tensor.numel()\n",
        "        std_min = 1.0 / math.sqrt(N)\n",
        "        std_adj = max(std.item(), std_min)\n",
        "        return (tensor - mean) / std_adj\n",
        "\n",
        "\n",
        "def _train_transform():\n",
        "    \"\"\"\n",
        "    Returns torchvision transform for training (distortions similar to TF code):\n",
        "    - Random crop to IMAGE_SIZE x IMAGE_SIZE from 32x32 image\n",
        "    - Random horizontal flip\n",
        "    - Random brightness and contrast (approximate TF random_brightness/random_contrast)\n",
        "    - Convert to tensor (0..1)\n",
        "    - Per-image standardization\n",
        "    \"\"\"\n",
        "    # ColorJitter's brightness and contrast ranges take a factor. TF used:\n",
        "    # random_brightness with max_delta=63 (on [0,255]) => approx +/- 0.25 in normalized 0..1\n",
        "    # random_contrast lower=0.2 upper=1.8 => matches contrast jitter factor\n",
        "    color_jitter = transforms.ColorJitter(brightness=0.25, contrast=(0.2, 1.8))\n",
        "    return transforms.Compose([\n",
        "        transforms.RandomCrop(IMAGE_SIZE),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        color_jitter,\n",
        "        transforms.ToTensor(),  # gives C x H x W, float in [0,1]\n",
        "        PerImageStandardization(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def _eval_transform():\n",
        "    \"\"\"\n",
        "    Returns transform for evaluation:\n",
        "     - center crop / pad to IMAGE_SIZE x IMAGE_SIZE (TF used resize_image_with_crop_or_pad which crops center)\n",
        "     - Convert to tensor and per-image standardize\n",
        "    \"\"\"\n",
        "    # For 32->24 center crop:\n",
        "    return transforms.Compose([\n",
        "        transforms.CenterCrop(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        PerImageStandardization(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_train_loader(data_dir: str, batch_size: int, num_workers: int = 16, shuffle: bool = True):\n",
        "    \"\"\"\n",
        "    Returns a DataLoader for training (reads data_batch_1..5).\n",
        "    \"\"\"\n",
        "    # CIFAR-10 training files\n",
        "    train_files = [f'data_batch_{i}.bin' for i in range(1, 6)]\n",
        "    \n",
        "    # Create dataset with training transforms\n",
        "    train_transform = _train_transform()\n",
        "    dataset = CIFAR10BinaryDataset(data_dir, train_files, transform=train_transform)\n",
        "    \n",
        "    # Create DataLoader\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_eval_loader(data_dir: str, batch_size: int, num_workers: int = 8, eval_data: bool = True):\n",
        "    \"\"\"\n",
        "    Returns a DataLoader for evaluation.\n",
        "    If eval_data==False it reads training batches (data_batch_1..5) else test_batch.bin\n",
        "    \"\"\"\n",
        "    if eval_data:\n",
        "        # Use test set\n",
        "        eval_files = ['test_batch.bin']\n",
        "    else:\n",
        "        # Use training set for evaluation\n",
        "        eval_files = [f'data_batch_{i}.bin' for i in range(1, 6)]\n",
        "    \n",
        "    # Create dataset with evaluation transforms\n",
        "    eval_transform = _eval_transform()\n",
        "    dataset = CIFAR10BinaryDataset(data_dir, eval_files, transform=eval_transform)\n",
        "    \n",
        "    # Create DataLoader (no shuffling for evaluation)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P0P04WSKWo0"
      },
      "source": [
        "## Diagonal LSTM and Masked Convolution\n",
        "\n",
        "20 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lez2ELjKWU9"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Optional, Tuple\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DEFAULT_DATA_FORMAT = \"NHWC\"\n",
        "\n",
        "def _is_nchw(x: torch.Tensor) -> bool:\n",
        "    return x.dim() == 4 and x.shape[1] <= 4 and x.shape[2] > 4  # heuristic; not perfect\n",
        "\n",
        "def ensure_nhwc(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Return tensor in NHWC layout (B,H,W,C).\"\"\"\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(\"Expect 4D tensor\")\n",
        "    # If likely NCHW (N,C,H,W) convert to NHWC\n",
        "    if x.shape[1] <= 4 and x.shape[2] > 4:\n",
        "        return x.permute(0, 2, 3, 1).contiguous()\n",
        "    return x\n",
        "\n",
        "def ensure_nchw(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Return tensor in NCHW layout (B,C,H,W).\"\"\"\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(\"Expect 4D tensor\")\n",
        "    # If likely NHWC convert to NCHW\n",
        "    if x.shape[3] <= 4 and x.shape[1] > 4:\n",
        "        return x.permute(0, 3, 1, 2).contiguous()\n",
        "    if x.shape[1] <= 4 and x.shape[2] > 4:\n",
        "        # ambiguous, assume NCHW already\n",
        "        return x\n",
        "    # assume NHWC (B,H,W,C)\n",
        "    if x.shape[3] <= 4:\n",
        "        return x.permute(0, 3, 1, 2).contiguous()\n",
        "    return x\n",
        "\n",
        "def get_shape(inputs: torch.Tensor):\n",
        "    \"\"\"Return python list-style shape similar to TF's .as_list().\"\"\"\n",
        "    return list(inputs.size())\n",
        "\n",
        "# ---------------------------\n",
        "# skew / unskew (supports NHWC and NCHW; works on NHWC primarily)\n",
        "# ---------------------------\n",
        "def skew(inputs: torch.Tensor, scope: str = \"skew\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    The skew function performs a transformation on a 4D tensor,\n",
        "    either in NHWC format (Batch, Height, Width, Channels) or NCHW format (Batch, Channels, Height, Width). \n",
        "    The goal of skewing is to shift the data from its original height-wise structure into a diagonal format. \n",
        "    Specifically, the function combines the height and width dimensions in a way that the resulting tensor has a width of W + H - 1,\n",
        "    where W is the width of the input tensor and H is the height.\n",
        "    Accepts NHWC ([B,H,W,C]) or NCHW ([B,C,H,W]) tensors.\n",
        "    Returns NHWC tensor with new width = W + H - 1\n",
        "    \"\"\"\n",
        "    # Check if input is NCHW and convert to NHWC\n",
        "    was_nchw = _is_nchw(inputs)\n",
        "    if was_nchw:\n",
        "        inputs = ensure_nhwc(inputs)\n",
        "    \n",
        "    B, H, W, C = inputs.shape\n",
        "    new_width = W + H - 1\n",
        "    \n",
        "    # Create output tensor with new width\n",
        "    outputs = torch.zeros(B, H, new_width, C, dtype=inputs.dtype, device=inputs.device)\n",
        "    \n",
        "    # Skew operation: shift each row by its row index\n",
        "    for h in range(H):\n",
        "        outputs[:, h, h:h+W, :] = inputs[:, h, :, :]\n",
        "    \n",
        "    logger.debug(f\"[skew] {scope} : {inputs.shape} -> {outputs.shape}\")\n",
        "\n",
        "    # if input was NCHW convert outputs back to NCHW style? The code returns NHWC.\n",
        "    if was_nchw:\n",
        "        # convert to NCHW before returning to be consistent with conv2d upstream expectations\n",
        "        return outputs.permute(0, 3, 1, 2).contiguous()  # [B, C, H, new_width]\n",
        "    return outputs  # NHWC\n",
        "\n",
        "def unskew(inputs: torch.Tensor, width: Optional[int] = None, scope: str = \"unskew\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    The unskew function performs the reverse of the skew operation. \n",
        "    It takes a skewed tensor and \"un-skews\" it back to its original shape.\n",
        "    If NCHW is provided, it will be converted to NHWC internally and returns NHWC.\n",
        "    \"\"\"\n",
        "    # Check if input is NCHW and convert to NHWC\n",
        "    was_nchw = _is_nchw(inputs)\n",
        "    if was_nchw:\n",
        "        inputs = ensure_nhwc(inputs)\n",
        "    \n",
        "    B, H, skewed_width, C = inputs.shape\n",
        "    \n",
        "    # If width not provided, infer from skewed width: W = skewed_width - H + 1\n",
        "    if width is None:\n",
        "        width = skewed_width - H + 1\n",
        "    \n",
        "    # Create output tensor with original width\n",
        "    outputs = torch.zeros(B, H, width, C, dtype=inputs.dtype, device=inputs.device)\n",
        "    \n",
        "    # Unskew operation: extract each row starting from its row index\n",
        "    for h in range(H):\n",
        "        outputs[:, h, :, :] = inputs[:, h, h:h+width, :]\n",
        "    \n",
        "    if was_nchw:\n",
        "        return outputs.permute(0, 3, 1, 2).contiguous()\n",
        "    logger.debug(f\"[unskew] {scope} : -> {outputs.shape}\")\n",
        "    return outputs\n",
        "\n",
        "# ---------------------------\n",
        "# conv2d with optional mask (mask type None/'A'/'B')\n",
        "# ---------------------------\n",
        "class MaskedConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    A conv2d that mimics TF behavior with variable creation and masking.\n",
        "    The module expects inputs either in NHWC or NCHW; internally we run conv in NCHW.\n",
        "    weights_shape = [kernel_h, kernel_w, in_channels, out_channels] (TF style).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 out_channels: int,\n",
        "                 kernel_size: Tuple[int, int],\n",
        "                 mask_type: Optional[str] = None,\n",
        "                 stride: Tuple[int, int] = (1, 1),\n",
        "                 padding: str = \"SAME\",\n",
        "                 bias: bool = True,\n",
        "                 weights_initializer=None,   \n",
        "                 name: str = \"conv2d\"):\n",
        "        super().__init__()\n",
        "        self.kernel_h, self.kernel_w = kernel_size\n",
        "        self.mask_type = mask_type.lower() if mask_type is not None else None\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.name = name\n",
        "\n",
        "        # Create weight parameter in PyTorch conv format: [out_channels, in_channels, kh, kw]\n",
        "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, self.kernel_h, self.kernel_w))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Initialize weights (Xavier uniform by default)\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        # Precompute mask if needed (create in TF ordering then transpose to PyTorch ordering)\n",
        "        if self.mask_type is not None:\n",
        "            mask = np.ones((self.kernel_h, self.kernel_w, in_channels, out_channels), dtype=np.float32)\n",
        "            center_h = self.kernel_h // 2\n",
        "            center_w = self.kernel_w // 2\n",
        "            mask[center_h, center_w + 1:, :, :] = 0.\n",
        "            mask[center_h + 1:, :, :, :] = 0.\n",
        "            if self.mask_type == 'a':\n",
        "                mask[center_h, center_w, :, :] = 0.\n",
        "            # convert mask to shape [out, in, kh, kw] for direct multiplication with self.weight\n",
        "            mask = mask.transpose(3, 2, 0, 1).copy()\n",
        "            self.register_buffer('mask', torch.tensor(mask))\n",
        "        else:\n",
        "            self.mask = None\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        # Accept NHWC or NCHW. Convert to NCHW for conv\n",
        "        is_nchw = _is_nchw(inputs)\n",
        "        if not is_nchw:\n",
        "            # Convert NHWC to NCHW\n",
        "            x = inputs.permute(0, 3, 1, 2).contiguous()\n",
        "        else:\n",
        "            x = inputs\n",
        "        \n",
        "        # Apply mask to weights if needed\n",
        "        if self.mask is not None:\n",
        "            weight = self.weight * self.mask\n",
        "        else:\n",
        "            weight = self.weight\n",
        "        \n",
        "        # Calculate padding for \"SAME\" padding\n",
        "        if self.padding == \"SAME\":\n",
        "            pad_h = (self.kernel_h - 1) // 2\n",
        "            pad_w = (self.kernel_w - 1) // 2\n",
        "            padding = (pad_w, pad_w, pad_h, pad_h)  # (left, right, top, bottom)\n",
        "            x = F.pad(x, padding, mode='constant', value=0)\n",
        "            padding = (0, 0)  # PyTorch conv2d uses (pad_h, pad_w)\n",
        "        else:\n",
        "            padding = 0\n",
        "        \n",
        "        # Perform convolution\n",
        "        out = F.conv2d(x, weight, self.bias, stride=self.stride, padding=padding)\n",
        "\n",
        "        # return in same layout as input\n",
        "        if not is_nchw:\n",
        "            out = out.permute(0, 2, 3, 1).contiguous()\n",
        "        logger.debug(f\"[conv2d_{self.mask_type}] {self.name} : {inputs.shape} -> {out.shape}\")\n",
        "        return out\n",
        "\n",
        "# conv1d implemented via conv2d with kernel_w = 1\n",
        "class MaskedConv1d(MaskedConv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=\"SAME\", mask_type=None, bias=True, name=\"conv1d\"):\n",
        "        # kernel_size is int (height)\n",
        "        super().__init__(in_channels=in_channels,\n",
        "                         out_channels=out_channels,\n",
        "                         kernel_size=(kernel_size, 1),\n",
        "                         mask_type=mask_type,\n",
        "                         stride=stride,\n",
        "                         padding=padding,\n",
        "                         bias=bias,\n",
        "                         name=name)\n",
        "\n",
        "# ---------------------------\n",
        "# Diagonal LSTM Cell\n",
        "# ---------------------------\n",
        "class DiagonalLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Diagonal LSTM Cell equivalent converted from TF.\n",
        "    - hidden_dims: number of hidden channels per spatial row\n",
        "    - height: number of rows\n",
        "    - channel: input channels (for conv1d s_to_s)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dims: int, height: int, channel: int):\n",
        "        super().__init__()\n",
        "        self._hidden_dims = hidden_dims\n",
        "        self._height = height\n",
        "        self._channel = channel\n",
        "        self._num_units = hidden_dims * height\n",
        "        self._state_size = self._num_units * 2\n",
        "        self._output_size = self._num_units\n",
        "\n",
        "        # conv to compute s_to_s (2x1 conv as in TF conv1d)\n",
        "        # We'll implement as a Conv2d with kernel (2,1) operating on [B, hidden_dims, height, 1]\n",
        "        # But conv1d in TF used input channels = hidden_dims; output = 4*hidden_dims\n",
        "        # Use groups=1\n",
        "        self.s_to_s_conv = MaskedConv1d(in_channels=self._hidden_dims, out_channels=4*self._hidden_dims, kernel_size=2, padding=\"SAME\", name='s_to_s')\n",
        "\n",
        "    def forward(self, i_to_s: torch.Tensor, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        i_to_s: [B, 4 * height * hidden_dims]\n",
        "        state: [B, 2 * num_units] where num_units = height * hidden_dims\n",
        "        returns: h (B, height*hidden_dims), new_state (B, 2*num_units)\n",
        "        \"\"\"\n",
        "        B = i_to_s.size(0)\n",
        "        num_units = self._num_units\n",
        "        \n",
        "        # Split state into c (cell state) and h (hidden state)\n",
        "        c_prev = state[:, :num_units]  # [B, num_units]\n",
        "        h_prev = state[:, num_units:]  # [B, num_units]\n",
        "        \n",
        "        # Reshape h_prev to [B, hidden_dims, height, 1] for conv1d\n",
        "        h_prev_reshaped = h_prev.view(B, self._hidden_dims, self._height, 1)\n",
        "        \n",
        "        # Compute s_to_s using conv1d (state to state)\n",
        "        s_to_s = self.s_to_s_conv(h_prev_reshaped)  # [B, 4*hidden_dims, height, 1]\n",
        "        s_to_s = s_to_s.squeeze(-1)  # [B, 4*hidden_dims, height]\n",
        "        s_to_s = s_to_s.view(B, 4 * num_units)  # [B, 4 * height * hidden_dims]\n",
        "        \n",
        "        # Combine input-to-state and state-to-state\n",
        "        gates = i_to_s + s_to_s  # [B, 4 * num_units]\n",
        "        \n",
        "        # Split gates into i, f, o, g\n",
        "        i, f, o, g = torch.chunk(gates, 4, dim=1)  # Each: [B, num_units]\n",
        "        \n",
        "        # Apply activations\n",
        "        i = torch.sigmoid(i)  # input gate\n",
        "        f = torch.sigmoid(f)  # forget gate\n",
        "        o = torch.sigmoid(o)  # output gate\n",
        "        g = torch.tanh(g)     # candidate values\n",
        "        \n",
        "        # Update cell state and hidden state\n",
        "        c_new = f * c_prev + i * g  # [B, num_units]\n",
        "        h_new = o * torch.tanh(c_new)  # [B, num_units]\n",
        "        \n",
        "        # Concatenate new state\n",
        "        new_state = torch.cat([c_new, h_new], dim=1)  # [B, 2*num_units]\n",
        "        \n",
        "        return h_new, new_state\n",
        "\n",
        "# ---------------------------\n",
        "# diagonal_lstm (sequence loop)\n",
        "# ---------------------------\n",
        "def diagonal_lstm(inputs: torch.Tensor, conf, scope: str = 'diagonal_lstm') -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    inputs: NHWC or NCHW. The function follows TF flow:\n",
        "      - skew inputs -> conv2d 1x1 to produce input_to_state (skewed), transpose to [B, W, H, 4*hidden_dims]\n",
        "      - reshape to rnn_inputs [B, W, H*4*hidden_dims] and iterate over width dimension\n",
        "    conf must provide: hidden_dims (int), use_dynamic_rnn (bool optional)\n",
        "    Returns: outputs in NHWC shape [B, H, W, hidden_dims]\n",
        "    \"\"\"\n",
        "    # Check if input is NCHW and convert to NHWC\n",
        "    was_nchw = _is_nchw(inputs)\n",
        "    if was_nchw:\n",
        "        inputs = ensure_nhwc(inputs)\n",
        "    \n",
        "    B, H, W, C = inputs.shape\n",
        "    hidden_dims = conf.hidden_dims\n",
        "    \n",
        "    # Skew the inputs\n",
        "    skewed = skew(inputs, scope=f\"{scope}/skew\")  # [B, H, W+H-1, C]\n",
        "    _, _, skewed_width, _ = skewed.shape\n",
        "    \n",
        "    # 1x1 conv to produce input_to_state (4*hidden_dims channels)\n",
        "    # Create a 1x1 conv layer for input_to_state\n",
        "    i_to_s_conv = nn.Conv2d(C, 4 * hidden_dims, kernel_size=1).to(inputs.device)\n",
        "    i_to_s_skewed = i_to_s_conv(skewed.permute(0, 3, 1, 2))  # [B, 4*hidden_dims, H, skewed_width]\n",
        "    i_to_s_skewed = i_to_s_skewed.permute(0, 2, 3, 1)  # [B, H, skewed_width, 4*hidden_dims]\n",
        "    \n",
        "    # Transpose to [B, skewed_width, H, 4*hidden_dims]\n",
        "    i_to_s_transposed = i_to_s_skewed.permute(0, 2, 1, 3)  # [B, skewed_width, H, 4*hidden_dims]\n",
        "    \n",
        "    # Reshape to [B, skewed_width, H*4*hidden_dims]\n",
        "    rnn_inputs = i_to_s_transposed.reshape(B, skewed_width, H * 4 * hidden_dims)\n",
        "    \n",
        "    # Create LSTM cell\n",
        "    lstm_cell = DiagonalLSTMCell(hidden_dims, H, C).to(inputs.device)\n",
        "    \n",
        "    # Initialize state\n",
        "    num_units = hidden_dims * H\n",
        "    state = torch.zeros(B, 2 * num_units, device=inputs.device)\n",
        "    \n",
        "    # Process sequence over width dimension\n",
        "    outputs_list = []\n",
        "    for w in range(skewed_width):\n",
        "        i_to_s_w = rnn_inputs[:, w, :]  # [B, H*4*hidden_dims]\n",
        "        h, state = lstm_cell(i_to_s_w, state)\n",
        "        outputs_list.append(h)\n",
        "    \n",
        "    # Stack outputs: [B, skewed_width, num_units]\n",
        "    outputs_skewed = torch.stack(outputs_list, dim=1)\n",
        "    \n",
        "    # Reshape to [B, skewed_width, H, hidden_dims]\n",
        "    outputs_skewed = outputs_skewed.view(B, skewed_width, H, hidden_dims)\n",
        "    \n",
        "    # Transpose back to [B, H, skewed_width, hidden_dims]\n",
        "    outputs_skewed = outputs_skewed.permute(0, 2, 1, 3)\n",
        "    \n",
        "    # Unskew to get original width\n",
        "    outputs = unskew(outputs_skewed, width=W, scope=f\"{scope}/unskew\")  # [B, H, W, hidden_dims]\n",
        "\n",
        "    if was_nchw:\n",
        "        # convert back to NCHW to be consistent with upstream expectations\n",
        "        return outputs.permute(0, 3, 1, 2).contiguous()\n",
        "    return outputs\n",
        "\n",
        "# ---------------------------\n",
        "# diagonal_bilstm\n",
        "# ---------------------------\n",
        "def diagonal_bilstm(inputs: torch.Tensor, conf, scope: str = 'diagonal_bilstm') -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compose forward diagonal_lstm and backward diagonal_lstm then sum appropriately (as TF).\n",
        "    If conf.use_residual is True, apply 1x1 conv residuals and add to inputs.\n",
        "    Returns: sum of forward and backward outputs with backward last column zeroed and shifted similar to TF.\n",
        "    \"\"\"\n",
        "    # Check if input is NCHW and convert to NHWC\n",
        "    was_nchw = _is_nchw(inputs)\n",
        "    if was_nchw:\n",
        "        inputs = ensure_nhwc(inputs)\n",
        "    \n",
        "    B, H, W, C = inputs.shape\n",
        "    \n",
        "    # Forward pass\n",
        "    out_fw = diagonal_lstm(inputs, conf, scope=f\"{scope}/fw\")\n",
        "    \n",
        "    # Backward pass: flip width dimension\n",
        "    inputs_bw = torch.flip(inputs, dims=[2])  # Flip along width\n",
        "    out_bw = diagonal_lstm(inputs_bw, conf, scope=f\"{scope}/bw\")\n",
        "    out_bw = torch.flip(out_bw, dims=[2])  # Flip back\n",
        "    \n",
        "    # Zero out the last column of backward output (as in TF implementation)\n",
        "    out_bw[:, :, -1, :] = 0\n",
        "    \n",
        "    # Apply residual connection if specified\n",
        "    if hasattr(conf, 'use_residual') and conf.use_residual:\n",
        "        # 1x1 conv for residual\n",
        "        residual_conv = nn.Conv2d(C, conf.hidden_dims, kernel_size=1).to(inputs.device)\n",
        "        residual = residual_conv(inputs.permute(0, 3, 1, 2))\n",
        "        residual = residual.permute(0, 2, 3, 1)\n",
        "        out_fw = out_fw + residual\n",
        "    \n",
        "    # Sum forward and backward\n",
        "    output = out_fw + out_bw\n",
        "    \n",
        "    if was_nchw:\n",
        "        output = output.permute(0, 3, 1, 2).contiguous()\n",
        "    \n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaEY8A2xKeWX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import logging\n",
        "from typing import Optional, Dict, Any, Iterable\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "class Statistic:\n",
        "    \"\"\"\n",
        "    the Statistic helper.\n",
        "\n",
        "    Args:\n",
        "        model: torch.nn.Module to save/load state_dict from.\n",
        "        optimizer: optional torch.optim.Optimizer to save/load state.\n",
        "        data_tag: str label used in TensorBoard scalar names .\n",
        "        model_dir: directory where checkpoints and logs will be written.\n",
        "        test_step: not used internally here but kept for API similarity (you can use it externally).\n",
        "        max_to_keep: how many checkpoints to keep; older are deleted.\n",
        "        device: device to load tensors to when loading checkpoints (default: cpu).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: Optional[torch.optim.Optimizer],\n",
        "        data_tag: str,\n",
        "        model_dir: str,\n",
        "        test_step: int,\n",
        "        max_to_keep: int = 20,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.data_tag = data_tag\n",
        "        self.model_dir = model_dir\n",
        "        self.test_step = test_step\n",
        "        self.max_to_keep = max_to_keep\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "\n",
        "        # internal counter t\n",
        "        self.t = 0\n",
        "\n",
        "        # tensorboard writer\n",
        "        log_dir = os.path.join(\"logs\", self.model_dir)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        self.writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "        # Ensure model_dir exists for checkpoints\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "        # Optionally keep a list of saved checkpoints on disk (sorted)\n",
        "        self._refresh_checkpoint_list()\n",
        "\n",
        "        # call reset to initialize whatever you want (kept for API parity)\n",
        "        self.reset()\n",
        "\n",
        "        logger.info(\"Statistic initialized. logs -> %s, checkpoints -> %s\", log_dir, self.model_dir)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Override or extend if you need to accumulate running statistics.\n",
        "        \"\"\"\n",
        "        # user can extend this method if they want to keep rolling averages, etc.\n",
        "        pass\n",
        "\n",
        "    def _refresh_checkpoint_list(self):\n",
        "        \"\"\"\n",
        "        Internal helper: update the cached list of checkpoint paths sorted by step (ascending).\n",
        "        \"\"\"\n",
        "        pattern = os.path.join(self.model_dir, \"checkpoint_*.pt\")\n",
        "        ckpts = glob.glob(pattern)\n",
        "        # parse t from filename checkpoint_{t}.pt\n",
        "        def _t_from_name(p):\n",
        "            try:\n",
        "                base = os.path.basename(p)\n",
        "                t_str = base.replace(\"checkpoint_\", \"\").replace(\".pt\", \"\")\n",
        "                return int(t_str)\n",
        "            except Exception:\n",
        "                return -1\n",
        "        ckpts_sorted = sorted(ckpts, key=_t_from_name)\n",
        "        self._checkpoints = ckpts_sorted\n",
        "\n",
        "    def _prune_checkpoints(self):\n",
        "        \"\"\"\n",
        "        Keep only the last `max_to_keep` checkpoints, remove older ones.\n",
        "        \"\"\"\n",
        "        self._refresh_checkpoint_list()\n",
        "        if self.max_to_keep is None:\n",
        "            return\n",
        "        while len(self._checkpoints) > self.max_to_keep:\n",
        "            old = self._checkpoints.pop(0)\n",
        "            try:\n",
        "                os.remove(old)\n",
        "                logger.info(\"Removed old checkpoint: %s\", old)\n",
        "            except OSError:\n",
        "                logger.warning(\"Failed to remove old checkpoint: %s\", old)\n",
        "\n",
        "    def on_step(self, train_l: float, test_l: float):\n",
        "        \"\"\"\n",
        "        Called at end of a step (or epoch) to increment counter, write summaries, save checkpoints and reset accumulators.\n",
        "        \"\"\"\n",
        "        # increment counter first\n",
        "        self.t += 1\n",
        "\n",
        "        # write summaries\n",
        "        self.inject_summary({'train_l': train_l, 'test_l': test_l}, self.t)\n",
        "\n",
        "        # save model\n",
        "        self.save_model(self.t)\n",
        "\n",
        "        # reset accumulators if any\n",
        "        self.reset()\n",
        "\n",
        "    def get_t(self) -> int:\n",
        "        \"\"\"Return current step counter.\"\"\"\n",
        "        return int(self.t)\n",
        "\n",
        "    def inject_summary(self, tag_dict: Dict[str, float], t: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        Write scalar summaries to TensorBoard.\n",
        "        tag_dict: mapping of tag->value, e.g. {'train_l': 0.5}\n",
        "        t: step (if None uses current internal t)\n",
        "        \"\"\"\n",
        "        step = self.t if t is None else int(t)\n",
        "        for tag, value in tag_dict.items():\n",
        "            full_tag = f\"{self.data_tag}/{tag}\"\n",
        "            # ensure value is a python float\n",
        "            try:\n",
        "                val = float(value)\n",
        "            except Exception:\n",
        "                val = float(value.item()) if hasattr(value, \"item\") else float(value)\n",
        "            self.writer.add_scalar(full_tag, val, step)\n",
        "        # flush may be helpful to make summary available quickly\n",
        "        self.writer.flush()\n",
        "\n",
        "    def save_model(self, t: Optional[int] = None, extra_state: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Save checkpoint for model (+ optimizer if supplied) and the internal counter 't'.\n",
        "        extra_state: optional mapping of additional items to store (e.g., scheduler state).\n",
        "        \"\"\"\n",
        "        step = self.t if t is None else int(t)\n",
        "        ckpt_name = os.path.join(self.model_dir, f\"checkpoint_{step}.pt\")\n",
        "        state = {\n",
        "            't': step,\n",
        "            'model_state': self.model.state_dict()\n",
        "        }\n",
        "        if self.optimizer is not None:\n",
        "            state['optimizer_state'] = self.optimizer.state_dict()\n",
        "        if extra_state:\n",
        "            state['extra'] = extra_state\n",
        "\n",
        "        # save atomically\n",
        "        tmp_name = ckpt_name + \".tmp\"\n",
        "        torch.save(state, tmp_name)\n",
        "        os.replace(tmp_name, ckpt_name)\n",
        "        logger.info(\"Saved checkpoint: %s\", ckpt_name)\n",
        "\n",
        "        # refresh and prune\n",
        "        self._refresh_checkpoint_list()\n",
        "        self._prune_checkpoints()        \n",
        "\n",
        "    def load_model(self, checkpoint_path: Optional[str] = None, map_location: Optional[torch.device] = None) -> bool:\n",
        "        \"\"\"\n",
        "        Load the latest checkpoint (if checkpoint_path is None) or load the provided checkpoint file.\n",
        "        Returns True if load succeeded, False otherwise.\n",
        "        map_location: device to map the checkpoint tensors onto (default: self.device)\n",
        "        \"\"\"\n",
        " \n",
        "        map_location = map_location or self.device\n",
        "\n",
        "        if checkpoint_path is None:\n",
        "            # find latest checkpoint\n",
        "            self._refresh_checkpoint_list()\n",
        "            if not self._checkpoints:\n",
        "                logger.info(\"No checkpoints found in %s\", self.model_dir)\n",
        "                return False\n",
        "            checkpoint_path = self._checkpoints[-1]\n",
        "\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            logger.warning(\"Checkpoint path does not exist: %s\", checkpoint_path)\n",
        "            return False\n",
        "\n",
        "        logger.info(\"Loading checkpoint: %s\", checkpoint_path)\n",
        "        ckpt = torch.load(checkpoint_path, map_location=map_location)\n",
        "\n",
        "        # load model state\n",
        "        if 'model_state' in ckpt:\n",
        "            try:\n",
        "                self.model.load_state_dict(ckpt['model_state'])\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Failed to load model_state: %s\", e)\n",
        "                return False\n",
        "        else:\n",
        "            logger.warning(\"Checkpoint missing 'model_state' key\")\n",
        "\n",
        "        # load optimizer state if present and optimizer available\n",
        "        if 'optimizer_state' in ckpt and self.optimizer is not None:\n",
        "            try:\n",
        "                self.optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Failed to load optimizer_state: %s\", e)\n",
        "                # continue, not fatal\n",
        "\n",
        "        # restore t if present\n",
        "        if 't' in ckpt:\n",
        "            self.t = int(ckpt['t'])\n",
        "        else:\n",
        "            # try to infer from filename\n",
        "            base = os.path.basename(checkpoint_path)\n",
        "            try:\n",
        "                self.t = int(base.replace(\"checkpoint_\", \"\").replace(\".pt\", \"\"))\n",
        "            except Exception:\n",
        "                self.t = 0\n",
        "\n",
        "        logger.info(\"Load SUCCESS: %s (t=%d)\", checkpoint_path, int(self.t))\n",
        "        return True\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close writer etc.\"\"\"\n",
        "        try:\n",
        "            self.writer.close()\n",
        "        except Exception:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZPkQoBaKjRT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pprint\n",
        "import tarfile\n",
        "import hashlib\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "from types import SimpleNamespace\n",
        "from typing import Any, Dict, Iterable, Optional\n",
        "\n",
        "# Python3 urllib\n",
        "import urllib.request\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "pp = pprint.PrettyPrinter().pprint\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------\n",
        "# Small helpers\n",
        "# -------------------------\n",
        "def mprint(matrix: Iterable[Iterable[float]], pivot: float = 0.5):\n",
        "    \"\"\"Pretty-print a binary-style matrix using '#' and ' ' like the original.\"\"\"\n",
        "    for array in matrix:\n",
        "        print(\"\".join(\"#\" if i > pivot else \" \" for i in array))\n",
        "\n",
        "\n",
        "def get_timestamp() -> str:\n",
        "    \"\"\"Return a filesystem-friendly timestamp string with local timezone info.\"\"\"\n",
        "    now = datetime.datetime.now().astimezone()\n",
        "    return now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "\n",
        "\n",
        "def binarize(images):\n",
        "    \"\"\"\n",
        "    Binarize image(s) by sampling from Uniform(0,1) < pixel_value.\n",
        "    Accepts a NumPy array or a PyTorch tensor. Returns same type as input (numpy or torch.FloatTensor).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        is_torch = torch.is_tensor(images)\n",
        "    except Exception:\n",
        "        is_torch = False\n",
        "\n",
        "    if is_torch:\n",
        "        # produce a tensor of same shape & device\n",
        "        rand = torch.rand_like(images)\n",
        "        return (rand < images).float()\n",
        "    else:\n",
        "        # assume numpy\n",
        "        return (np.random.uniform(size=images.shape) < images).astype('float32')\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Image saving\n",
        "# -------------------------\n",
        "def save_images(images, height: int, width: int, n_row: int, n_col: int,\n",
        "                cmin: float = 0.0, cmax: float = 1.0, directory: str = \"./\", prefix: str = \"sample\"):\n",
        "    \"\"\"\n",
        "    Save a grid of images to a single image file.\n",
        "    - images: numpy array of shape (n_row*n_col, H, W) or (n_row*n_col, H, W, C)\n",
        "              OR shaped (n_row, n_col, H, W) etc. This function will attempt to reshape sensibly.\n",
        "    - height, width: single image H,W\n",
        "    - n_row, n_col: grid layout\n",
        "    \"\"\"\n",
        "    # convert torch -> numpy if needed\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.is_tensor(images):\n",
        "            images = images.detach().cpu().numpy()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    imgs = np.asarray(images)\n",
        "    # handle shapes:\n",
        "    # If flat list of images: (N, H, W) or (N, H, W, C)\n",
        "    if imgs.ndim == 3:\n",
        "        # grayscale stack\n",
        "        N, H, W = imgs.shape\n",
        "        C = 1\n",
        "        imgs = imgs.reshape((N, H, W))\n",
        "        imgs = imgs.reshape((n_row, n_col, H, W))\n",
        "        imgs = imgs.transpose(1, 2, 0, 3).reshape((H * n_row, W * n_col))\n",
        "        mode = 'L'\n",
        "    elif imgs.ndim == 4:\n",
        "        N, H, W, C = imgs.shape\n",
        "        if C == 1:\n",
        "            imgs = imgs.reshape((n_row, n_col, H, W))\n",
        "            imgs = imgs.transpose(1, 2, 0, 3).reshape((H * n_row, W * n_col))\n",
        "            mode = 'L'\n",
        "        elif C == 3:\n",
        "            # arrange into grid with channels last\n",
        "            imgs = imgs.reshape((n_row, n_col, H, W, C))\n",
        "            imgs = imgs.transpose(1, 2, 0, 3, 4).reshape((H * n_row, W * n_col, C))\n",
        "            mode = 'RGB'\n",
        "        else:\n",
        "            # unsupported channel count: collapse or take first channel\n",
        "            imgs = imgs[..., 0]\n",
        "            imgs = imgs.reshape((n_row, n_col, H, W))\n",
        "            imgs = imgs.transpose(1, 2, 0, 3).reshape((H * n_row, W * n_col))\n",
        "            mode = 'L'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported image array shape: %s\" % (imgs.shape,))\n",
        "\n",
        "    # scale pixels from [cmin,cmax] to [0,255]\n",
        "    if mode == 'L':\n",
        "        norm = (imgs - cmin) / max((cmax - cmin), 1e-8)\n",
        "        arr = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
        "        pil_img = Image.fromarray(arr, mode='L')\n",
        "    else:\n",
        "        norm = (imgs - cmin) / max((cmax - cmin), 1e-8)\n",
        "        arr = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
        "        pil_img = Image.fromarray(arr, mode='RGB')\n",
        "\n",
        "    filename = f'{prefix}_{get_timestamp()}.jpg'\n",
        "    out_path = os.path.join(directory, filename)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    pil_img.save(out_path)\n",
        "    logger.info(\"Saved image grid to %s\", out_path)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Model / config helpers\n",
        "# -------------------------\n",
        "def get_model_dir(config, exceptions=None):\n",
        "    \"\"\"\n",
        "    Robust replacement for TF-specific get_model_dir:\n",
        "      - Accepts TF flags object (which stores flags under __flags) or argparse/Namespace/SimpleNamespace.\n",
        "      - Builds a readable model dir name from config key/value pairs excluding `exceptions`.\n",
        "      - If the generated name is too long, falls back to an md5 hash to keep path sane.\n",
        "    \"\"\"\n",
        "    exceptions = set(exceptions or [])\n",
        "    # Try TF-style flags first\n",
        "    try:\n",
        "        if hasattr(config, '__dict__') and '__flags' in config.__dict__:\n",
        "            attrs = dict(config.__dict__['__flags'])\n",
        "        else:\n",
        "            # argparse.Namespace / SimpleNamespace / plain object\n",
        "            attrs = dict(vars(config))\n",
        "    except Exception:\n",
        "        # as a last resort, try to use __dict__\n",
        "        try:\n",
        "            attrs = dict(config.__dict__)\n",
        "        except Exception:\n",
        "            attrs = {}\n",
        "\n",
        "    # Filter out exceptions and None/empty values that are not informative\n",
        "    items = []\n",
        "    for k in sorted(attrs.keys()):\n",
        "        if k in exceptions:\n",
        "            continue\n",
        "        if k.startswith('_'):\n",
        "            continue\n",
        "        v = attrs[k]\n",
        "        if callable(v):\n",
        "            vstr = v.__name__\n",
        "        else:\n",
        "            try:\n",
        "                vstr = str(v)\n",
        "            except Exception:\n",
        "                vstr = repr(v)\n",
        "        # shorten long values\n",
        "        if len(vstr) > 40:\n",
        "            vstr = hashlib.md5(vstr.encode('utf-8')).hexdigest()[:8]\n",
        "        items.append(f\"{k}={vstr}\")\n",
        "\n",
        "    if not items:\n",
        "        name = \"default\"\n",
        "    else:\n",
        "        name = \"_\".join(items)\n",
        "\n",
        "    # sanitize name (remove spaces, slashes)\n",
        "    name = name.replace(\" \", \"\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "    # If too long, shorten using md5\n",
        "    if len(name) > 180:\n",
        "        name = hashlib.md5(name.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "    return os.path.join('checkpoints', name) + '/'\n",
        "\n",
        "\n",
        "def preprocess_conf(conf):\n",
        "    \"\"\"\n",
        "    Placeholder to keep API parity with your previous code.\n",
        "    If you need to normalize or canonicalize flags/options, do it here.\n",
        "    \"\"\"\n",
        "    # For argparse/Namespace we don't need to do anything by default.\n",
        "    return conf\n",
        "\n",
        "\n",
        "def check_and_create_dir(directory: str):\n",
        "    \"\"\"Create dir if not exists (logs)\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        logger.info('Creating directory: %s' % directory)\n",
        "        os.makedirs(directory)\n",
        "    else:\n",
        "        logger.info('Skip creating directory: %s' % directory)\n",
        "\n",
        "\n",
        "def show_all_variables(model: Optional[Any] = None):\n",
        "    \"\"\"\n",
        "    Print all trainable variables.\n",
        "    If `model` is provided (torch.nn.Module), iterate its parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        if model is None:\n",
        "            logger.warning(\"No model passed to show_all_variables(model). Nothing to show.\")\n",
        "            return\n",
        "        total_count = 0\n",
        "        for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "            shape = tuple(param.shape)\n",
        "            count = int(np.prod(shape))\n",
        "            print(\"[%2d] %s %s = %s\" % (idx, name, shape, count))\n",
        "            total_count += count\n",
        "        print(\"[Total] variable size: %s\" % \"{:,}\".format(total_count))\n",
        "    except Exception as e:\n",
        "        logger.exception(\"show_all_variables failed: %s\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_IBau6nKCg7"
      },
      "source": [
        "## Network\n",
        "in this part we will impelement a neural network that can be configured to perform pixel-wise predictions, specifically for image generation tasks such as those in PixelCNN or PixelRNN models. This class supports both training and inference, and can generate outputs pixel-by-pixel in a raster scan order, leveraging various types of convolutions (standard and masked) and recurrent layers (e.g., LSTM).\n",
        "\n",
        "25 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IioQxy6J_QF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import logging\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch Network class.\n",
        "    \"\"\"\n",
        "    def __init__(self, conf, height: int, width: int, channel: int, device: Optional[torch.device] = None):\n",
        "        super().__init__()\n",
        "        logger.info(\"Building %s starts!\" % conf.model)\n",
        "        self.conf = conf\n",
        "        self.data = conf.data\n",
        "        self.height, self.width, self.channel = height, width, channel\n",
        "        self.device = device if device is not None else (torch.device(\"cuda\") if conf.use_gpu and torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "\n",
        "        # Data format decisions\n",
        "        self.data_format = \"NHWC\" if conf.use_gpu else \"NCHW\"\n",
        "\n",
        "        # Build layers - FIXED CHANNEL DIMENSIONS\n",
        "        in_channels = channel\n",
        "\n",
        "        # Calculate conv_out_channels based on residual connections\n",
        "        if conf.use_residual and conf.model == \"pixel_rnn\":\n",
        "            conv_out_channels = conf.hidden_dims * 2\n",
        "        else:\n",
        "            conv_out_channels = conf.hidden_dims\n",
        "\n",
        "        # Input convolution\n",
        "        self.conv_inputs = nn.Conv2d(in_channels, conv_out_channels, kernel_size=7, padding=3)\n",
        "\n",
        "        # Build recurrent/conv layers\n",
        "        self.recurrent_length = conf.recurrent_length\n",
        "        self.out_recurrent_length = conf.out_recurrent_length\n",
        "\n",
        "        if conf.model == \"pixel_cnn\":\n",
        "            # For pixel_cnn, use masked convolutions with proper channel dimensions\n",
        "            self.conv_blocks = nn.ModuleList()\n",
        "            for idx in range(self.recurrent_length):\n",
        "                in_ch = conv_out_channels if idx == 0 else conf.hidden_dims\n",
        "                # Use regular conv2d for now - replace with masked conv if available\n",
        "                self.conv_blocks.append(nn.Conv2d(in_ch, conf.hidden_dims, kernel_size=3, padding=1))\n",
        "        else:\n",
        "            # For pixel_rnn we will call diagonal_bilstm in forward()\n",
        "            self.conv_blocks = None\n",
        "\n",
        "        # Output recurrent layers (1x1 convs + ReLU)\n",
        "        self.out_convs = nn.ModuleList()\n",
        "        for idx in range(self.out_recurrent_length):\n",
        "            in_ch = conf.hidden_dims if idx == 0 else conf.out_hidden_dims\n",
        "            self.out_convs.append(nn.Conv2d(in_ch, conf.out_hidden_dims, kernel_size=1))\n",
        "\n",
        "        # Final logits conv - FIXED: output should match input channels for binary prediction\n",
        "        if channel == 1:\n",
        "            # Single output logit per pixel\n",
        "            in_ch = conf.out_hidden_dims if self.out_recurrent_length > 0 else conv_out_channels\n",
        "            self.conv2d_out_logits = nn.Conv2d(in_ch, 1, kernel_size=1)\n",
        "            self.loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "        else:\n",
        "            # For RGB/categorical (not implemented in original)\n",
        "            raise NotImplementedError(\"RGB branch not implemented (same as original).\")\n",
        "\n",
        "        # Optimizer (RMSProp) and grad clipping\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=conf.learning_rate)\n",
        "        self.grad_clip = conf.grad_clip\n",
        "\n",
        "        # Move to device\n",
        "        self.to(self.device)\n",
        "        logger.info(\"Building %s finished!\" % conf.model)\n",
        "\n",
        "    def _to_nchw(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert incoming tensor to NCHW for internal conv ops.\"\"\"\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x)\n",
        "        x = x.to(self.device)\n",
        "        if x.dim() != 4:\n",
        "            raise ValueError(\"Input must be 4D tensor\")\n",
        "        if self.data_format == \"NHWC\":\n",
        "            # [B,H,W,C] -> [B,C,H,W]\n",
        "            x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x.float()\n",
        "\n",
        "    def _to_output_layout(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert internal outputs (NCHW) back to requested external layout.\"\"\"\n",
        "        if self.data_format == \"NHWC\":\n",
        "            return x.permute(0, 2, 3, 1).contiguous()\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # Convert to NCHW for internal processing\n",
        "        x = self._to_nchw(inputs)  # [B, C, H, W]\n",
        "        \n",
        "        # Input convolution\n",
        "        x = self.conv_inputs(x)  # [B, conv_out_channels, H, W]\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Process through recurrent/conv layers\n",
        "        if self.conf.model == \"pixel_cnn\":\n",
        "            # PixelCNN: masked convolutions\n",
        "            for conv_block in self.conv_blocks:\n",
        "                residual = x\n",
        "                x = F.relu(conv_block(x))\n",
        "                # Add residual connection if channels match\n",
        "                if x.shape[1] == residual.shape[1]:\n",
        "                    x = x + residual\n",
        "        elif self.conf.model == \"pixel_rnn\":\n",
        "            # PixelRNN: diagonal bidirectional LSTM\n",
        "            # Convert to NHWC for diagonal_bilstm\n",
        "            x_nhwc = x.permute(0, 2, 3, 1).contiguous()  # [B, H, W, C]\n",
        "            \n",
        "            # Process through recurrent layers (diagonal_bilstm)\n",
        "            for _ in range(self.recurrent_length):\n",
        "                x_nhwc = diagonal_bilstm(x_nhwc, self.conf, scope='diagonal_bilstm')\n",
        "                x_nhwc = F.relu(x_nhwc)\n",
        "            \n",
        "            # Convert back to NCHW\n",
        "            x = x_nhwc.permute(0, 3, 1, 2).contiguous()  # [B, C, H, W]\n",
        "        \n",
        "        # Output recurrent layers (1x1 convs + ReLU)\n",
        "        for out_conv in self.out_convs:\n",
        "            x = F.relu(out_conv(x))\n",
        "        \n",
        "        # Final logits convolution\n",
        "        logits = self.conv2d_out_logits(x)  # [B, 1, H, W] for binary\n",
        "\n",
        "        # Return in external layout\n",
        "        return self._to_output_layout(logits)\n",
        "\n",
        "    def predict(self, images):\n",
        "        \"\"\"Predict probabilities for given images.\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Convert to tensor if needed\n",
        "            if isinstance(images, np.ndarray):\n",
        "                images = torch.from_numpy(images).float()\n",
        "            \n",
        "            # Move to device\n",
        "            images = images.to(self.device)\n",
        "            \n",
        "            # Forward pass to get logits\n",
        "            logits = self.forward(images)  # [B, H, W, 1] or [B, 1, H, W]\n",
        "            \n",
        "            # Convert to probabilities using sigmoid (for binary classification)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            \n",
        "            # Convert to numpy\n",
        "            probs_t = probs.detach().cpu()\n",
        "        \n",
        "        return probs_t.numpy()\n",
        "\n",
        "    def test(self, images, with_update: bool = False):\n",
        "        \"\"\"Compute loss for images, optionally update weights.\"\"\"\n",
        "        if with_update:\n",
        "            self.train()\n",
        "            self.optimizer.zero_grad()\n",
        "        else:\n",
        "            self.eval()\n",
        "        \n",
        "        # Convert to tensor if needed\n",
        "        if isinstance(images, np.ndarray):\n",
        "            images = torch.from_numpy(images).float()\n",
        "        \n",
        "        # Move to device\n",
        "        images = images.to(self.device)\n",
        "        \n",
        "        # For binary images, targets are the same as inputs (predict next pixel)\n",
        "        # In autoregressive models, we predict each pixel given previous ones\n",
        "        # For simplicity, we use the image as both input and target\n",
        "        # In practice, you'd shift the target by one pixel\n",
        "        targets = images\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = self.forward(images)  # [B, H, W, 1] or [B, 1, H, W]\n",
        "        \n",
        "        # Ensure logits and targets have same shape\n",
        "        if logits.dim() == 4:\n",
        "            if self.data_format == \"NHWC\":\n",
        "                # [B, H, W, 1] -> [B, 1, H, W] for loss\n",
        "                logits = logits.permute(0, 3, 1, 2)\n",
        "            # Now logits is [B, 1, H, W]\n",
        "            if targets.dim() == 3:\n",
        "                targets = targets.unsqueeze(1)  # [B, 1, H, W]\n",
        "            elif targets.dim() == 4 and targets.shape[1] != 1:\n",
        "                # If targets are [B, H, W], add channel dimension\n",
        "                targets = targets.unsqueeze(1)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss_val = self.loss_fn(logits, targets)\n",
        "        \n",
        "        if with_update:\n",
        "            loss_val.backward()\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)\n",
        "            self.optimizer.step()\n",
        "        \n",
        "        return float(loss_val.item())\n",
        "\n",
        "    def generate(self, batch_size: int = 100):\n",
        "        \"\"\"Generate binary samples in raster order.\"\"\"\n",
        "        self.eval()\n",
        "        H, W = self.height, self.width\n",
        "        \n",
        "        # Initialize image with zeros (or random values)\n",
        "        samples = torch.zeros(batch_size, self.channel, H, W, device=self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Generate pixels in raster scan order (top-left to bottom-right)\n",
        "            for h in range(H):\n",
        "                for w in range(W):\n",
        "                    # Get current partial image\n",
        "                    current_image = samples.clone()\n",
        "                    \n",
        "                    # Convert to appropriate format for forward pass\n",
        "                    if self.data_format == \"NHWC\":\n",
        "                        current_nhwc = current_image.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
        "                    else:\n",
        "                        current_nhwc = current_image  # [B, C, H, W]\n",
        "                    \n",
        "                    # Forward pass to get logits for current pixel\n",
        "                    logits = self.forward(current_nhwc)  # [B, H, W, 1] or [B, 1, H, W]\n",
        "                    \n",
        "                    # Extract logit for current pixel position\n",
        "                    if self.data_format == \"NHWC\":\n",
        "                        # logits is [B, H, W, 1]\n",
        "                        pixel_logits = logits[:, h, w, :]  # [B, 1]\n",
        "                    else:\n",
        "                        # logits is [B, 1, H, W]\n",
        "                        pixel_logits = logits[:, :, h, w]  # [B, 1]\n",
        "                    \n",
        "                    # Convert to probability\n",
        "                    pixel_probs = torch.sigmoid(pixel_logits)  # [B, 1]\n",
        "                    \n",
        "                    # Sample binary value from Bernoulli distribution\n",
        "                    pixel_values = torch.bernoulli(pixel_probs)  # [B, 1]\n",
        "                    \n",
        "                    # Update the pixel in the sample\n",
        "                    if self.data_format == \"NHWC\":\n",
        "                        samples[:, :, h, w] = pixel_values.squeeze(-1)  # [B, C]\n",
        "                    else:\n",
        "                        samples[:, :, h, w] = pixel_values  # [B, C]\n",
        "        \n",
        "        # Convert to numpy if needed, or return as tensor\n",
        "        # Return in format [B, H, W] or [B, C, H, W]\n",
        "        if self.channel == 1:\n",
        "            samples = samples.squeeze(1)  # [B, H, W]\n",
        "        \n",
        "        return samples.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3XTaYW_KvVa"
      },
      "source": [
        "## Main\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YTDJFRmtKwjd",
        "outputId": "a1777f51-51bb-4d27-b941-50a82d3efb19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import argparse\n",
        "import math\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "from itertools import islice\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# logging\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "logger = logging.getLogger()\n",
        "# default level set later by args\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # network\n",
        "    parser.add_argument(\"--model\", type=str, default=\"pixel_cnn\", help=\"name of model [pixel_rnn, pixel_cnn]\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=100, help=\"size of a batch\")\n",
        "    parser.add_argument(\"--hidden_dims\", type=int, default=16, help=\"dimesion of hidden states of LSTM or Conv layers\")\n",
        "    parser.add_argument(\"--recurrent_length\", type=int, default=7, help=\"the length of LSTM or Conv layers\")\n",
        "    parser.add_argument(\"--out_hidden_dims\", type=int, default=32, help=\"dimesion of hidden states of output Conv layers\")\n",
        "    parser.add_argument(\"--out_recurrent_length\", type=int, default=2, help=\"the length of output Conv layers\")\n",
        "    parser.add_argument(\"--use_residual\", action=\"store_true\", default=False, help=\"whether to use residual connections or not\")\n",
        "\n",
        "    # training\n",
        "    parser.add_argument(\"--max_epoch\", type=int, default=100000, help=\"# of step in an epoch\")\n",
        "    parser.add_argument(\"--test_step\", type=int, default=100, help=\"# of step to test a model\")\n",
        "    parser.add_argument(\"--save_step\", type=int, default=1000, help=\"# of step to save a model\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"learning rate\")\n",
        "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"value of gradient to be used for clipping\")\n",
        "    parser.add_argument(\"--use_gpu\", action=\"store_true\", default=True, help=\"whether to use gpu for training\")\n",
        "\n",
        "    # data\n",
        "    parser.add_argument(\"--data\", type=str, default=\"mnist\", help=\"name of dataset [mnist, cifar]\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"data\", help=\"name of data directory\")\n",
        "    parser.add_argument(\"--sample_dir\", type=str, default=\"samples\", help=\"name of sample directory\")\n",
        "\n",
        "    # debug / misc\n",
        "    parser.add_argument(\"--is_train\", action=\"store_true\", default=True, help=\"training or testing\")\n",
        "    parser.add_argument(\"--display\", action=\"store_true\", default=False, help=\"whether to display the training results or not\")\n",
        "    parser.add_argument(\"--log_level\", type=str, default=\"INFO\", help=\"log level [DEBUG, INFO, WARNING, ERROR, CRITICAL]\")\n",
        "    parser.add_argument(\"--random_seed\", type=int, default=123, help=\"random seed for python\")\n",
        "\n",
        "    return parser.parse_args([]) # Pass an empty list to parse_args\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    # convert argparse Namespace to a conf-like SimpleNamespace (so older code expecting attributes works)\n",
        "    conf = SimpleNamespace(**vars(args))\n",
        "\n",
        "    # logging level\n",
        "    logger.setLevel(conf.log_level)\n",
        "\n",
        "    # random seeds\n",
        "    np.random.seed(conf.random_seed)\n",
        "    torch.manual_seed(conf.random_seed)\n",
        "    if conf.use_gpu and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(conf.random_seed)\n",
        "\n",
        "    # Build model dir and preprocess config if helper exists\n",
        "    model_dir = get_model_dir(conf,\n",
        "                              ['data_dir', 'sample_dir', 'max_epoch', 'test_step', 'save_step',\n",
        "                               'is_train', 'random_seed', 'log_level', 'display'])\n",
        "    preprocess_conf(conf)\n",
        "\n",
        "    DATA_DIR = os.path.join(conf.data_dir, conf.data)\n",
        "    SAMPLE_DIR = os.path.join(conf.sample_dir, conf.data, model_dir)\n",
        "\n",
        "    check_and_create_dir(DATA_DIR)\n",
        "    check_and_create_dir(SAMPLE_DIR)\n",
        "\n",
        "    device = torch.device(\"cuda\" if (conf.use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
        "    logger.info(\"Using device: %s\", device)\n",
        "\n",
        "    # 0. prepare datasets\n",
        "    if conf.data == \"cifar\":\n",
        "        # Load CIFAR-10 datasets\n",
        "        train_loader = get_train_loader(DATA_DIR, conf.batch_size, num_workers=4, shuffle=True)\n",
        "        test_loader = get_eval_loader(DATA_DIR, conf.batch_size, num_workers=2, eval_data=True)\n",
        "        \n",
        "        # CIFAR-10 image dimensions\n",
        "        height, width, channel = IMAGE_SIZE, IMAGE_SIZE, 3\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset: %s\" % conf.data)\n",
        "\n",
        "    # 1. Build network\n",
        "    network = Network(conf, height, width, channel, device=device)\n",
        "    logger.info(\"Network created with %d parameters\", sum(p.numel() for p in network.parameters()))\n",
        "\n",
        "    try:\n",
        "        stat = Statistic(network, network.optimizer, conf.data, model_dir, conf.test_step)\n",
        "    except TypeError:\n",
        "        # older Statistic signature variations: try with optimizer attr\n",
        "        stat = Statistic(network, network.optimizer if hasattr(network, 'optimizer') else None, conf.data, model_dir, conf.test_step)\n",
        "\n",
        "    # Try loading existing checkpoint\n",
        "    stat.load_model()\n",
        "\n",
        "    if conf.is_train:\n",
        "        logger.info(\"Training starts!\")\n",
        "        initial_step = stat.get_t() if stat else 0\n",
        "        iterator = trange(conf.max_epoch, ncols=70, initial=initial_step)\n",
        "\n",
        "        for epoch in iterator:\n",
        "            # 1. train\n",
        "            network.train()\n",
        "            train_loss = 0.0\n",
        "            train_count = 0\n",
        "            \n",
        "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "                # Convert images to binary if needed (for binary image generation)\n",
        "                # CIFAR-10 images are already normalized, but we might want to binarize\n",
        "                # For now, we'll use the normalized images directly\n",
        "                images = images.to(device)\n",
        "                \n",
        "                # Convert to appropriate format (NCHW)\n",
        "                if network.data_format == \"NHWC\":\n",
        "                    images = images.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]\n",
        "                \n",
        "                # Compute loss and update\n",
        "                loss = network.test(images, with_update=True)\n",
        "                train_loss += loss\n",
        "                train_count += 1\n",
        "                \n",
        "                if batch_idx % 10 == 0:\n",
        "                    logger.debug(\"Epoch %d, Batch %d, Loss: %.4f\", epoch, batch_idx, loss)\n",
        "            \n",
        "            avg_train_loss = train_loss / train_count if train_count > 0 else 0.0\n",
        "\n",
        "            # 2. test\n",
        "            network.eval()\n",
        "            test_loss = 0.0\n",
        "            test_count = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for images, labels in islice(test_loader, 100):  # Limit test batches\n",
        "                    images = images.to(device)\n",
        "                    \n",
        "                    if network.data_format == \"NHWC\":\n",
        "                        images = images.permute(0, 2, 3, 1)\n",
        "                    \n",
        "                    loss = network.test(images, with_update=False)\n",
        "                    test_loss += loss\n",
        "                    test_count += 1\n",
        "            \n",
        "            avg_test_loss = test_loss / test_count if test_count > 0 else 0.0\n",
        "\n",
        "            # Update statistics\n",
        "            stat.on_step(avg_train_loss, avg_test_loss)\n",
        "            \n",
        "            # Update progress bar\n",
        "            iterator.set_description(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "            # 3. generate samples\n",
        "            if epoch % conf.save_step == 0 or epoch == conf.max_epoch - 1:\n",
        "                logger.info(\"Generating samples at epoch %d\", epoch)\n",
        "                network.eval()\n",
        "                samples = network.generate(batch_size=16)  # Generate 16 samples\n",
        "                \n",
        "                # Save samples\n",
        "                save_images(samples, height, width, n_row=4, n_col=4, \n",
        "                           cmin=0.0, cmax=1.0, directory=SAMPLE_DIR, \n",
        "                           prefix=f\"epoch_{epoch}\")\n",
        "                \n",
        "                logger.info(\"Samples saved to %s\", SAMPLE_DIR)\n",
        "    else:\n",
        "        # Testing/inference mode\n",
        "        logger.info(\"Running in test mode\")\n",
        "        network.eval()\n",
        "        \n",
        "        # Generate samples\n",
        "        samples = network.generate(batch_size=64)\n",
        "        save_images(samples, height, width, n_row=8, n_col=8,\n",
        "                   cmin=0.0, cmax=1.0, directory=SAMPLE_DIR,\n",
        "                   prefix=\"test_samples\")\n",
        "        \n",
        "        logger.info(\"Test samples saved to %s\", SAMPLE_DIR)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
