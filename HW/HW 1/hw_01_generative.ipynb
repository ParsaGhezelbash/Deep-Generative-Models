{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55K0twjjjFLo"
      },
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "In The Name of God\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Sharif University of Technology - Department of Electrical Engineering\n",
        "</font>\n",
        "<br>\n",
        "<font color=\"#008080\" size=6>\n",
        "Deep Generative Models\n",
        "</font>\n",
        "<hr/>\n",
        "<font color=\"#800080\" size=5>\n",
        "Assignment 1 : Deep Autoregressive Models\n",
        "<br>\n",
        "</font>\n",
        "<font size=5>\n",
        "Instructor: Dr. S. Amini\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Fall 2025\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "</font>\n",
        "<hr>\n",
        "<font color='red'  size=4>\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Feel free to ask your questions in Telegram : @imoonamm\n",
        "</font>\n",
        "<br>\n",
        "<hr>\n",
        "</div></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You should only change the blank sections, marked with TODO**\n",
        "\n",
        "Pay attention to docstrings, as they may drastically help with your implementation.\n",
        "\n",
        "You are advised to read all related papers and material, to help you better understand the design of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1: WaveNet : A generative model for raw audio\n",
        "\n",
        "WaveNet is a general purpose technology that has allowed us to unlock a range of new applications, from improving video calls on even the weakest connections to helping people regain their original voice after losing the ability to speak.\n",
        "\n",
        "![Local GIF](unnamed.gif)\n",
        "\n",
        "WaveNet models raw audio waveforms autoregressively:\n",
        "$\n",
        "p(x_{1:T}) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$\n",
        "\\\n",
        "Instead of predicting spectrograms or using a vocoder, WaveNet predicts the next audio sample directly (often after µ-law companding and 8-bit/256-way quantization). This yields very natural sounding speech and music because the network learns the waveform structure end-to-end.\n",
        "\n",
        "## 2. Core\n",
        "\n",
        "### 2.1 Causal convolutions\n",
        "A causal 1D convolution ensures predictions at time \\(t\\) don't depend on future samples:\n",
        "- Achieved by padding only on the left (past).\n",
        "- In practice, use `padding = dilation * (kernel_size - 1)` and then trim the rightmost elements (or use `Chomp1d` cropping).\n",
        "\n",
        "### 2.2 Dilated convolutions\n",
        "Dilations `[1, 2, 4, 8, ...]` in stacked layers let the receptive field grow exponentially with depth while keeping the number of parameters manageable. A stack of several cycles of dilations covers long temporal context (hundreds to thousands of samples).\n",
        "\n",
        "### 2.3 Gated activation units\n",
        "Each residual block uses a gated unit:\n",
        "$\n",
        "\\text{z} = \\tanh(W_f * x) \\odot \\sigma(W_g * x)\n",
        "$\n",
        "where `*` is dilated causal conv, `W_f` and `W_g` are convolution filters. The output splits to (1) residual connection and (2) skip connection that is added to final output.\n",
        "\n",
        "### 2.4 Residual & skip connections\n",
        "- Residual: `x_{l+1} = x_l + \\text{residual\\_out}` to ease training.\n",
        "- Skip: every block outputs a skip tensor; all skip outputs are summed, then passed through post-processing (ReLU, Conv, softmax) to produce the final distribution over quantized samples.\n",
        "\n",
        "### 2.5 Output quantization / softmax\n",
        "Audio samples are often µ-law quantized to 256 values; WaveNet predicts a categorical distribution (`softmax(256)`) over those values for each time step. Continuous outputs are also possible (mixture of logistics / Gaussians) but discrete softmax is standard in the original paper.\n",
        "\n",
        "### 2.6 Conditioning\n",
        "- **Global conditioning**: a per-utterance vector (e.g., speaker id embedding) is added to layer activations.\n",
        "- **Local conditioning**: low-rate features (e.g., mel spectrogram) are upsampled (transposed conv / nearest) and added at each time step.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training & sampling\n",
        "\n",
        "### Training\n",
        "- Teacher forcing: at training time the network receives true previous samples and learns to predict the next sample with cross-entropy loss (if quantized).\n",
        "- Batch size and optimization: use Adam, gradient clipping recommended for stability.\n",
        "- Preprocess: µ-law companding + 256-level quantization is common.\n",
        "\n",
        "### Sampling (inference)\n",
        "- Autoregressive generation: sample one sample at a time and feed it back.\n",
        "- Slow by default — many acceleration techniques exist (distillation → Parallel Wavenet, caching convolutions, WaveRNN, etc.).\n",
        "\n",
        "\n",
        "\n",
        "**Reference:** [WaveNet: A Generative Model for Raw Audio* (van den Oord et al., DeepMind, 2016)](https://arxiv.org/abs/1609.03499)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchaudio\n",
        "# !pip install git+git://github.com/pytorch/audio\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import ModuleList\n",
        "from IPython.display import Audio\n",
        "from torch.autograd import Variable\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### at first you must convert input into One Hot vector\n",
        "\n",
        "5 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneHot(nn.Module):\n",
        "    def __init__(self, MU):\n",
        "        super(OneHot,self).__init__()\n",
        "        self.MU = MU\n",
        "        self.ones = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.ones is None:\n",
        "            self.ones = torch.eye(self.MU).to(x.device)\n",
        "        return self.ones[x]\n",
        "\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"({})\".format(self.MU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before diving into the WaveNet model itself, it’s important to understand what kind of **data** we are modeling — **raw audio waveforms**.  \n",
        "Neural networks can only process numbers, so we need to represent sound in a numerical form that captures its essential structure and variation.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Sampling Rate and Bit Depth\n",
        "\n",
        "### Sound as a Signal\n",
        "Sound in the real world is a **continuous analog waveform** — a pressure signal that varies smoothly over time.  \n",
        "To process it digitally, we **sample** it: we take discrete measurements of the amplitude at equally spaced time intervals.\n",
        "\n",
        "This process converts a continuous signal into a **time series of numbers**:\n",
        "$\n",
        "x = [x_1, x_2, \\dots, x_T]\n",
        "$\n",
        "where each \\(x_t\\) represents the air pressure (or voltage) at time step \\(t\\).\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling Rate\n",
        "The **sampling rate** defines **how many times per second** we measure the amplitude of the signal.  \n",
        "Typical values:\n",
        "- CD quality audio: 44,100 Hz (samples per second)\n",
        "- Speech datasets: 16,000 Hz or 22,050 Hz\n",
        "\n",
        "A higher sampling rate means more detail, but also more data to process.  \n",
        "WaveNet typically models 16 kHz or 22 kHz audio for speech generation tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### Bit Depth\n",
        "Each recorded sample must be represented by a finite number of bits.  \n",
        "The **bit depth** determines the range and precision of these values:\n",
        "- 8-bit → 256 possible levels  \n",
        "- 16-bit → 65,536 levels  \n",
        "- 24-bit → 16,777,216 levels  \n",
        "\n",
        "The bit depth affects the **signal-to-noise ratio (SNR)** — higher bit depth means less quantization noise and a larger dynamic range:\n",
        "$\n",
        "\\text{SNR} \\approx 6.02 \\times \\text{bit depth} + 1.76\\ \\text{dB}\n",
        "$\n",
        "\n",
        "For example:\n",
        "- 8-bit ≈ 49.9 dB SNR  \n",
        "- 16-bit ≈ 98 dB SNR\n",
        "\n",
        "---\n",
        "\n",
        "## 2. From Audio to Time Series\n",
        "\n",
        "After digitization, the audio signal becomes a **sequence of numbers over time**, also known as a **time series**.  \n",
        "Each point depends on previous ones — this temporal dependency is what WaveNet models autoregressively:\n",
        "$\n",
        "p(x_{1:T}) = \\prod_{t=1}^{T} p(x_t \\mid x_{<t})\n",
        "$\n",
        "\n",
        "However, raw 16-bit audio contains **too many values** (65,536 possible amplitudes).  \n",
        "Predicting the exact next value among that huge range is extremely hard for a neural network.\n",
        "\n",
        "To simplify, we reduce this dynamic range to a smaller set — typically **256 levels** — using a technique called **µ-law companding**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. µ-law Quantization (Companding Transformation)\n",
        "\n",
        "### Motivation\n",
        "Human hearing follows the **Weber–Fechner law** — our perception of changes in loudness is **logarithmic**, not linear.  \n",
        "That means we are more sensitive to small amplitude changes in quiet sounds than in loud ones.\n",
        "\n",
        "µ-law companding exploits this property by **compressing the amplitude range logarithmically** —  \n",
        "it allocates more resolution to small signals (quiet sounds) and less to large ones.\n",
        "\n",
        "---\n",
        "\n",
        "### µ-law Formula\n",
        "\n",
        "$\n",
        "f(x) = \\text{sign}(x) \\cdot \\frac{\\ln(1 + \\mu |x|)}{\\ln(1 + \\mu)}, \\quad -1 \\le x \\le 1\n",
        "$\n",
        "\n",
        "where:\n",
        "- \\(x\\) = input waveform (normalized to [-1, 1])  \n",
        "- \\(\\mu\\) = companding constant (usually 255 for 8-bit quantization)\n",
        "\n",
        "After applying µ-law, we map the range \\([-1, 1]\\) to discrete integer values \\([0, 255]\\).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Wavenet\n",
        "\n",
        "20 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following figure describes the overall architecture of WaveNet. Note that Transformers were non-existent in 2016!\n",
        "\n",
        "![Wavenet Arch](WaveNet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Wavenet(nn.Module):\n",
        "  def __init__(self, dilation_depth, n_blocks, n_dil_channnels, n_residual_channels, n_skip_channels, n_category, kernel_size):\n",
        "    super(Wavenet,self).__init__()\n",
        "    self.dilation_depth = dilation_depth\n",
        "    self.n_blocks = n_blocks\n",
        "    self.n_dil_channnels = n_dil_channnels\n",
        "    self.n_residual_channels = n_residual_channels\n",
        "    self.n_skip_channels = n_skip_channels\n",
        "    self.n_category = n_category\n",
        "    self.kernel_size = kernel_size\n",
        "    self.One_hot = OneHot(n_category)\n",
        "\n",
        "    ###Building the model###\n",
        "    self.filter_convs = ModuleList()\n",
        "    self.gate_convs = ModuleList()\n",
        "    self.residual_convs = ModuleList()\n",
        "    self.skip_convs = ModuleList()\n",
        "    self.dilations = []\n",
        "\n",
        "\n",
        "    ##creating first channels##\n",
        "    self.start_conv = nn.Conv1d(in_channels=self.n_category,\n",
        "                                out_channels=self.n_residual_channels,\n",
        "                                kernel_size=self.kernel_size)\n",
        "\n",
        "    ###Creating wavenet blocks stacks###\n",
        "    for b in range(self.n_blocks):\n",
        "      for i in range(self.dilation_depth):\n",
        "        dilation = 2**i\n",
        "        self.dilations.append(dilation)\n",
        "        \n",
        "        # Dilated causal convolution for the filter part of the gated activation\n",
        "        self.filter_convs.append(nn.Conv1d(in_channels=self.n_residual_channels,\n",
        "                                            out_channels=self.n_dil_channnels,\n",
        "                                            kernel_size=self.kernel_size,\n",
        "                                            dilation=dilation))\n",
        "\n",
        "        # Dilated causal convolution for the gate part of the gated activation\n",
        "        self.gate_convs.append(nn.Conv1d(in_channels=self.n_residual_channels,\n",
        "                                          out_channels=self.n_dil_channnels,\n",
        "                                          kernel_size=self.kernel_size,\n",
        "                                          dilation=dilation))\n",
        "\n",
        "        # 1x1 convolution for the residual connection\n",
        "        self.residual_convs.append(nn.Conv1d(in_channels=self.n_dil_channnels,\n",
        "                                              out_channels=self.n_residual_channels,\n",
        "                                              kernel_size=1))\n",
        "\n",
        "        # 1x1 convolution for the skip connection\n",
        "        self.skip_convs.append(nn.Conv1d(in_channels=self.n_dil_channnels,\n",
        "                                          out_channels=self.n_skip_channels,\n",
        "                                          kernel_size=1))\n",
        "\n",
        "    ##post convoluions\n",
        "    self.end_conv_1 = nn.Conv1d(in_channels=self.n_skip_channels,\n",
        "                                out_channels=self.n_skip_channels,\n",
        "                                kernel_size=1)\n",
        "    self.end_conv_2 = nn.Conv1d(in_channels=self.n_skip_channels,\n",
        "                                out_channels=self.n_category,\n",
        "                                kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.One_hot(x)\n",
        "    x = torch.transpose(x,1,2)  #Batch x Channels x Time\n",
        "    x = nn.functional.pad(x, (self.kernel_size-1,0))\n",
        "    x = self.start_conv(x)\n",
        "\n",
        "    skip_connections = 0\n",
        "\n",
        "    for i in range(self.n_blocks * self.dilation_depth):\n",
        "      #Dilated convolutions\n",
        "      filter = self.filter_convs[i](x)\n",
        "      gate = self.gate_convs[i](x)\n",
        "\n",
        "      #Gated activation unit\n",
        "      x_f = torch.tanh(filter)\n",
        "      x_g = torch.sigmoid(gate)\n",
        "      x = x_f * x_g\n",
        "\n",
        "      #Skip connection\n",
        "      skip = self.skip_convs[i](x)\n",
        "      try:\n",
        "        skip_connections = skip_connections[:, :, -skip.size(2):]\n",
        "      except:\n",
        "        pass\n",
        "      skip_connections = skip + skip_connections\n",
        "\n",
        "      #Residual connection\n",
        "      x = self.residual_convs[i](x)\n",
        "      x = x + x[:, :, -x.size(2):]\n",
        "\n",
        "    x = torch.relu(skip_connections)\n",
        "    x = torch.relu(self.end_conv_1(x))\n",
        "    x = self.end_conv_2(x)\n",
        "\n",
        "    return torch.transpose(x.squeeze(0),0,1)\n",
        "  ###Function to generate samples###\n",
        "  def generate(self, input, num_samples=100):\n",
        "        \n",
        "      \n",
        "        return gen_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantization of 16 bit audio\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mulaw_quantize(x, qc):\n",
        "    #TODO\n",
        "    \n",
        "    return x_mu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inverse quantization\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inv_mulaw_quantize(x_mu, quantization_channels=256, device = device):\n",
        "    #TODO\n",
        "    \n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "In this part you must load the audio_dataset.npz that exist in zip file of homework\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import math\n",
        "import threading\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import librosa as lr\n",
        "import bisect\n",
        "\n",
        "\n",
        "class WavenetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_file,\n",
        "                 item_length,\n",
        "                 target_length,\n",
        "                 file_location=None,\n",
        "                 classes=256,\n",
        "                 sampling_rate=16000,\n",
        "                 mono=True,\n",
        "                 normalize=False,\n",
        "                 dtype=np.uint8,\n",
        "                 train=True,\n",
        "                 test_stride=100):\n",
        "\n",
        "        #           |----receptive_field----|\n",
        "        #                                 |--output_length--|\n",
        "        # example:  | | | | | | | | | | | | | | | | | | | | |\n",
        "        # target:                           | | | | | | | | | |\n",
        "\n",
        "        self.dataset_file = dataset_file\n",
        "        self._item_length = item_length\n",
        "        self._test_stride = test_stride\n",
        "        self.target_length = target_length\n",
        "        self.classes = classes\n",
        "\n",
        "        if not os.path.isfile(dataset_file):\n",
        "            assert file_location is not None, \"no location for dataset files specified\"\n",
        "            self.mono = mono\n",
        "            self.normalize = normalize\n",
        "\n",
        "            self.sampling_rate = sampling_rate\n",
        "            self.dtype = dtype\n",
        "            self.create_dataset(file_location, dataset_file)\n",
        "        else:\n",
        "            # Unknown parameters of the stored dataset\n",
        "            # TODO Can these parameters be stored, too?\n",
        "            self.mono = None\n",
        "            self.normalize = None\n",
        "\n",
        "            self.sampling_rate = None\n",
        "            self.dtype = None\n",
        "\n",
        "        self.data = np.load(self.dataset_file, mmap_mode='r')\n",
        "        self.start_samples = [0]\n",
        "        self._length = 0\n",
        "        self.calculate_length()\n",
        "        self.train = train\n",
        "        print(\"one hot input\")\n",
        "        # assign every *test_stride*th item to the test set\n",
        "\n",
        "\n",
        "    def calculate_length(self):\n",
        "        start_samples = [0]\n",
        "        for i in range(len(self.data.keys())):\n",
        "            start_samples.append(start_samples[-1] + len(self.data['arr_' + str(i)]))\n",
        "        available_length = start_samples[-1] - (self._item_length - (self.target_length - 1)) - 1\n",
        "        self._length = math.floor(available_length / self.target_length)\n",
        "        self.start_samples = start_samples\n",
        "\n",
        "    def set_item_length(self, l):\n",
        "        self._item_length = l\n",
        "        self.calculate_length()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Clarify sample_index calculation for train/test with test_stride\n",
        "\n",
        "        # TODO: Check file_index bounds; currently prints error, consider raising IndexError\n",
        "\n",
        "        if :\n",
        "        # TODO: Avoid repeated np.load calls; cache self.data in __init__ instead\n",
        "\n",
        "        else:\n",
        "            # TODO: Optimize cross-file slicing; currently concatenates arrays which copies memory\n",
        "\n",
        "\n",
        "        \n",
        "        # TODO: One-hot creation is memory heavy; consider returning integer indices and use nn.Embedding in model\n",
        "\n",
        "        return one_hot, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        test_length = math.floor(self._length / self._test_stride)\n",
        "        if self.train:\n",
        "            return self._length - test_length\n",
        "        else:\n",
        "            return test_length\n",
        "\n",
        "\n",
        "def quantize_data(data, classes):\n",
        "    #TODO\n",
        "    \n",
        "    return quantized\n",
        "\n",
        "\n",
        "def list_all_audio_files(location):\n",
        "    #TODO\n",
        "\n",
        "    return audio_files\n",
        "\n",
        "\n",
        "def mu_law_encoding(data, mu):\n",
        "    #TODO\n",
        "\n",
        "    return mu_x\n",
        "\n",
        "\n",
        "def mu_law_expansion(data, mu):\n",
        "    #TODO\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dilation_depth = 10\n",
        "n_blocks = 2\n",
        "n_dilation_channels = 24\n",
        "n_residual_channels = 24\n",
        "n_skip_channels = 128\n",
        "n_category = 128\n",
        "kernel_size = 2\n",
        "model = Wavenet(dilation_depth,n_blocks,n_dilation_channels ,n_residual_channels,n_skip_channels,n_category,kernel_size)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define data generator\n",
        "\n",
        "10 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_generation(data,fr,seq_len_segment , mu, device = device):\n",
        "    ###shape of data is [batch_size,seq_len]\n",
        "    ### we want to randomly choose a starting position and then extract the seq_len_segment\n",
        "    ### now the data is normalized for mu law encoding\n",
        "    #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize sample of data\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer and Loss Function\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot losses\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_losses(trainLoss,Epochs):\n",
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model\n",
        "\n",
        "5 Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Audio using trained model\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 2: Pixel CNN / Pixel RNN (Optional)\n",
        "\n",
        "In this question, we explore **deep autoregressive models** for image generation, focusing on PixelCNN and PixelRNN.  \n",
        "\n",
        "Natural images are high-dimensional and exhibit strong spatial correlations between neighboring pixels. The **PixelRNN** aims to model the **full joint distribution** of pixel intensities, enabling exact likelihood computation and realistic image generation.  \n",
        "\n",
        "Unlike VAEs or GANs, which rely on latent variables or adversarial training, PixelRNN treats the **image as a sequence of pixels** and directly models:  \n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}) = \\prod_{i=1}^{N} p(x_i \\mid x_1, x_2, \\ldots, x_{i-1})\n",
        "$$\n",
        "\n",
        "Here, each pixel \\(x_i\\) is conditioned on all previously generated pixels, following a fixed **raster-scan order** (top-left → bottom-right). This transforms image modeling into a **sequence modeling task**, allowing recurrent neural networks (RNNs) to capture long-range spatial dependencies.  \n",
        "\n",
        "---\n",
        "\n",
        "### Core Idea\n",
        "\n",
        "PixelRNN introduces **two-dimensional LSTM architectures** to preserve autoregressive dependencies in images:\n",
        "\n",
        "- **Row LSTM:** Processes the image row by row (left-to-right), with recurrent connections across both rows and columns.  \n",
        "- **Diagonal Bi-LSTM:** Processes pixels along diagonals, increasing the receptive field to include all previously generated pixels.  \n",
        "\n",
        "To maintain causality across color channels, each pixel’s RGB values are generated sequentially (R → G → B).  \n",
        "\n",
        "![PixelRNN Architecture](image.png)\n",
        "\n",
        "---\n",
        "\n",
        "### Architectural Components\n",
        "\n",
        "1. **Masked Convolutions**  \n",
        "   Convolutional filters are masked (type A or B) to prevent access to future pixels or channels during training.  \n",
        "\n",
        "2. **Residual Connections**  \n",
        "   Stacked recurrent layers use residual links to facilitate gradient flow in deep networks.  \n",
        "\n",
        "3. **Discrete Softmax Output**  \n",
        "   Each pixel channel is modeled as a categorical distribution over 256 intensity values, allowing exact log-likelihood computation.  \n",
        "\n",
        "4. **Loss Function**  \n",
        "   The training objective is the **negative log-likelihood** of the true pixels under the predicted distributions:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_i \\log p(x_i \\mid x_{<i})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling and Generation\n",
        "\n",
        "Image generation in PixelRNN is **fully autoregressive**:\n",
        "\n",
        "1. Start at the top-left pixel.  \n",
        "2. Sequentially sample each pixel from its predicted conditional distribution.  \n",
        "3. Repeat until the entire image is generated.  \n",
        "\n",
        "This method produces coherent, high-quality images but is computationally intensive, as each pixel depends on all previously generated pixels.  \n",
        "\n",
        "---\n",
        "\n",
        "**Reference:** [PixelRNN Paper (2016)](https://arxiv.org/abs/1601.06759)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQss1Y83nYf1"
      },
      "source": [
        "## load Cifar10\n",
        "\n",
        "5 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9vwArxJJj1z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import struct\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "#TODO\n",
        "\n",
        "IMAGE_SIZE = None\n",
        "NUM_CLASSES = None\n",
        "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = None\n",
        "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = None\n",
        "\n",
        "\n",
        "def _read_cifar10_binary_file(path: str) -> List[Tuple[int, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Reads a single CIFAR-10 binary file and returns a list of (label, image_array)\n",
        "    where image_array is HxWxC uint8 (32x32x3).\n",
        "    CIFAR-10 binary format: 1 byte label followed by 3072 bytes image (R(1024), G(1024), B(1024))\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    record_size = 1 + 32 * 32 * 3\n",
        "    with open(path, 'rb') as f:\n",
        "        while True:\n",
        "            bytes_read = f.read(record_size)\n",
        "            if not bytes_read:\n",
        "                break\n",
        "            if len(bytes_read) != record_size:\n",
        "                raise ValueError(f\"Unexpected record size in {path}: {len(bytes_read)} bytes\")\n",
        "            # first byte is label\n",
        "            label = bytes_read[0]\n",
        "            img_flat = np.frombuffer(bytes_read[1:], dtype=np.uint8)\n",
        "            # image is stored as [R..1024, G..1024, B..1024] each row-major for 32x32\n",
        "            # reshape to (3, 32, 32) then transpose to (32, 32, 3)\n",
        "            depth_major = img_flat.reshape((3, 32, 32))\n",
        "            img = np.transpose(depth_major, (1, 2, 0))  # H, W, C\n",
        "            records.append((int(label), img))\n",
        "    return records\n",
        "\n",
        "class CIFAR10BinaryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that reads the CIFAR-10 binary files into memory (list of (label, image)).\n",
        "    Applies transforms (train/eval) provided in init.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir: str, files: List[str], transform=None):\n",
        "        \"\"\"\n",
        "        data_dir: directory containing the CIFAR-10 binary files.\n",
        "        files: list of filenames (basename) to read from data_dir.\n",
        "        transform: torchvision transform to apply to PIL image (or custom transform).\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.files = files\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all records into memory (50k images is fine)\n",
        "        self.records: List[Tuple[int, np.ndarray]] = []\n",
        "        for fname in files:\n",
        "            path = os.path.join(data_dir, fname)\n",
        "            if not os.path.exists(path):\n",
        "                raise ValueError(f\"Failed to find file: {path}\")\n",
        "            self.records.extend(_read_cifar10_binary_file(path))\n",
        "\n",
        "        if len(self.records) == 0:\n",
        "            raise ValueError(\"No records loaded from CIFAR files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, img_np = self.records[idx]\n",
        "        # Convert to PIL Image (expects HxW or HxWxC)\n",
        "        img = Image.fromarray(img_np)  # mode='RGB'\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # After transforms, we expect a torch.FloatTensor image of shape [C, H, W]\n",
        "        # Return label as long tensor\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "class PerImageStandardization:\n",
        "    \"\"\"\n",
        "    Mimics TensorFlow's per_image_standardization:\n",
        "      (x - mean) / max(stddev, 1.0/sqrt(N))\n",
        "    Works on a torch tensor image with shape (C, H, W) and dtype=float32 in range [0,1] or raw values.\n",
        "    We'll expect inputs are float tensors in range [0,1].\n",
        "    \"\"\"\n",
        "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        # tensor: C x H x W\n",
        "        if not torch.is_floating_point(tensor):\n",
        "            tensor = tensor.float()\n",
        "        mean = tensor.mean()\n",
        "        std = tensor.std()\n",
        "        # N = number of pixels * channels\n",
        "        N = tensor.numel()\n",
        "        std_min = 1.0 / math.sqrt(N)\n",
        "        std_adj = max(std.item(), std_min)\n",
        "        return (tensor - mean) / std_adj\n",
        "\n",
        "\n",
        "def _train_transform():\n",
        "    \"\"\"\n",
        "    Returns torchvision transform for training (distortions similar to TF code):\n",
        "    - Random crop to IMAGE_SIZE x IMAGE_SIZE from 32x32 image\n",
        "    - Random horizontal flip\n",
        "    - Random brightness and contrast (approximate TF random_brightness/random_contrast)\n",
        "    - Convert to tensor (0..1)\n",
        "    - Per-image standardization\n",
        "    \"\"\"\n",
        "    # ColorJitter's brightness and contrast ranges take a factor. TF used:\n",
        "    # random_brightness with max_delta=63 (on [0,255]) => approx +/- 0.25 in normalized 0..1\n",
        "    # random_contrast lower=0.2 upper=1.8 => matches contrast jitter factor\n",
        "    color_jitter = transforms.ColorJitter(brightness=0.25, contrast=(0.2, 1.8))\n",
        "    return transforms.Compose([\n",
        "        transforms.RandomCrop(IMAGE_SIZE),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        color_jitter,\n",
        "        transforms.ToTensor(),  # gives C x H x W, float in [0,1]\n",
        "        PerImageStandardization(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def _eval_transform():\n",
        "    \"\"\"\n",
        "    Returns transform for evaluation:\n",
        "     - center crop / pad to IMAGE_SIZE x IMAGE_SIZE (TF used resize_image_with_crop_or_pad which crops center)\n",
        "     - Convert to tensor and per-image standardize\n",
        "    \"\"\"\n",
        "    # For 32->24 center crop:\n",
        "    return transforms.Compose([\n",
        "        transforms.CenterCrop(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        PerImageStandardization(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_train_loader(data_dir: str, batch_size: int, num_workers: int = 16, shuffle: bool = True):\n",
        "    \"\"\"\n",
        "    Returns a DataLoader for training (reads data_batch_1..5).\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_eval_loader(data_dir: str, batch_size: int, num_workers: int = 8, eval_data: bool = True):\n",
        "    \"\"\"\n",
        "    Returns a DataLoader for evaluation.\n",
        "    If eval_data==False it reads training batches (data_batch_1..5) else test_batch.bin\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P0P04WSKWo0"
      },
      "source": [
        "## Diagonal LSTM and Masked Convolution\n",
        "\n",
        "20 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lez2ELjKWU9"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Optional, Tuple\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DEFAULT_DATA_FORMAT = \"NHWC\"\n",
        "\n",
        "def _is_nchw(x: torch.Tensor) -> bool:\n",
        "    return x.dim() == 4 and x.shape[1] <= 4 and x.shape[2] > 4  # heuristic; not perfect\n",
        "\n",
        "def ensure_nhwc(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Return tensor in NHWC layout (B,H,W,C).\"\"\"\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(\"Expect 4D tensor\")\n",
        "    # If likely NCHW (N,C,H,W) convert to NHWC\n",
        "    if x.shape[1] <= 4 and x.shape[2] > 4:\n",
        "        return x.permute(0, 2, 3, 1).contiguous()\n",
        "    return x\n",
        "\n",
        "def ensure_nchw(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Return tensor in NCHW layout (B,C,H,W).\"\"\"\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(\"Expect 4D tensor\")\n",
        "    # If likely NHWC convert to NCHW\n",
        "    if x.shape[3] <= 4 and x.shape[1] > 4:\n",
        "        return x.permute(0, 3, 1, 2).contiguous()\n",
        "    if x.shape[1] <= 4 and x.shape[2] > 4:\n",
        "        # ambiguous, assume NCHW already\n",
        "        return x\n",
        "    # assume NHWC (B,H,W,C)\n",
        "    if x.shape[3] <= 4:\n",
        "        return x.permute(0, 3, 1, 2).contiguous()\n",
        "    return x\n",
        "\n",
        "def get_shape(inputs: torch.Tensor):\n",
        "    \"\"\"Return python list-style shape similar to TF's .as_list().\"\"\"\n",
        "    return list(inputs.size())\n",
        "\n",
        "# ---------------------------\n",
        "# skew / unskew (supports NHWC and NCHW; works on NHWC primarily)\n",
        "# ---------------------------\n",
        "def skew(inputs: torch.Tensor, scope: str = \"skew\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    The skew function performs a transformation on a 4D tensor,\n",
        "    either in NHWC format (Batch, Height, Width, Channels) or NCHW format (Batch, Channels, Height, Width). \n",
        "    The goal of skewing is to shift the data from its original height-wise structure into a diagonal format. \n",
        "    Specifically, the function combines the height and width dimensions in a way that the resulting tensor has a width of W + H - 1,\n",
        "    where W is the width of the input tensor and H is the height.\n",
        "    Accepts NHWC ([B,H,W,C]) or NCHW ([B,C,H,W]) tensors.\n",
        "    Returns NHWC tensor with new width = W + H - 1\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    # if input was NCHW convert outputs back to NCHW style? The code returns NHWC.\n",
        "    if was_nchw:\n",
        "        # convert to NCHW before returning to be consistent with conv2d upstream expectations\n",
        "        return outputs.permute(0, 3, 1, 2).contiguous()  # [B, C, H, new_width]\n",
        "    return outputs  # NHWC\n",
        "\n",
        "def unskew(inputs: torch.Tensor, width: Optional[int] = None, scope: str = \"unskew\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    The unskew function performs the reverse of the skew operation. \n",
        "    It takes a skewed tensor and \"un-skews\" it back to its original shape.\n",
        "    If NCHW is provided, it will be converted to NHWC internally and returns NHWC.\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    if was_nchw:\n",
        "        return outputs.permute(0, 3, 1, 2).contiguous()\n",
        "    logger.debug(f\"[unskew] {scope} : -> {outputs.shape}\")\n",
        "    return outputs\n",
        "\n",
        "# ---------------------------\n",
        "# conv2d with optional mask (mask type None/'A'/'B')\n",
        "# ---------------------------\n",
        "class MaskedConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    A conv2d that mimics TF behavior with variable creation and masking.\n",
        "    The module expects inputs either in NHWC or NCHW; internally we run conv in NCHW.\n",
        "    weights_shape = [kernel_h, kernel_w, in_channels, out_channels] (TF style).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 out_channels: int,\n",
        "                 kernel_size: Tuple[int, int],\n",
        "                 mask_type: Optional[str] = None,\n",
        "                 stride: Tuple[int, int] = (1, 1),\n",
        "                 padding: str = \"SAME\",\n",
        "                 bias: bool = True,\n",
        "                 weights_initializer=None,   \n",
        "                 name: str = \"conv2d\"):\n",
        "        super().__init__()\n",
        "        self.kernel_h, self.kernel_w = kernel_size\n",
        "        self.mask_type = mask_type.lower() if mask_type is not None else None\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.name = name\n",
        "\n",
        "        # Create weight parameter in PyTorch conv format: [out_channels, in_channels, kh, kw]\n",
        "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, self.kernel_h, self.kernel_w))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Initialize weights (Xavier uniform by default)\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        # Precompute mask if needed (create in TF ordering then transpose to PyTorch ordering)\n",
        "        if self.mask_type is not None:\n",
        "            mask = np.ones((self.kernel_h, self.kernel_w, in_channels, out_channels), dtype=np.float32)\n",
        "            center_h = self.kernel_h // 2\n",
        "            center_w = self.kernel_w // 2\n",
        "            mask[center_h, center_w + 1:, :, :] = 0.\n",
        "            mask[center_h + 1:, :, :, :] = 0.\n",
        "            if self.mask_type == 'a':\n",
        "                mask[center_h, center_w, :, :] = 0.\n",
        "            # convert mask to shape [out, in, kh, kw] for direct multiplication with self.weight\n",
        "            mask = mask.transpose(3, 2, 0, 1).copy()\n",
        "            self.register_buffer('mask', torch.tensor(mask))\n",
        "        else:\n",
        "            self.mask = None\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        # Accept NHWC or NCHW. Convert to NCHW for conv\n",
        "        #TODO\n",
        "\n",
        "        # return in same layout as input\n",
        "        if not is_nchw:\n",
        "            out = out.permute(0, 2, 3, 1).contiguous()\n",
        "        logger.debug(f\"[conv2d_{self.mask_type}] {self.name} : {inputs.shape} -> {out.shape}\")\n",
        "        return out\n",
        "\n",
        "# conv1d implemented via conv2d with kernel_w = 1\n",
        "class MaskedConv1d(MaskedConv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=\"SAME\", mask_type=None, bias=True, name=\"conv1d\"):\n",
        "        # kernel_size is int (height)\n",
        "        super().__init__(in_channels=in_channels,\n",
        "                         out_channels=out_channels,\n",
        "                         kernel_size=(kernel_size, 1),\n",
        "                         mask_type=mask_type,\n",
        "                         stride=stride,\n",
        "                         padding=padding,\n",
        "                         bias=bias,\n",
        "                         name=name)\n",
        "\n",
        "# ---------------------------\n",
        "# Diagonal LSTM Cell\n",
        "# ---------------------------\n",
        "class DiagonalLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Diagonal LSTM Cell equivalent converted from TF.\n",
        "    - hidden_dims: number of hidden channels per spatial row\n",
        "    - height: number of rows\n",
        "    - channel: input channels (for conv1d s_to_s)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dims: int, height: int, channel: int):\n",
        "        super().__init__()\n",
        "        self._hidden_dims = hidden_dims\n",
        "        self._height = height\n",
        "        self._channel = channel\n",
        "        self._num_units = hidden_dims * height\n",
        "        self._state_size = self._num_units * 2\n",
        "        self._output_size = self._num_units\n",
        "\n",
        "        # conv to compute s_to_s (2x1 conv as in TF conv1d)\n",
        "        # We'll implement as a Conv2d with kernel (2,1) operating on [B, hidden_dims, height, 1]\n",
        "        # But conv1d in TF used input channels = hidden_dims; output = 4*hidden_dims\n",
        "        # Use groups=1\n",
        "        self.s_to_s_conv = MaskedConv1d(in_channels=self._hidden_dims, out_channels=4*self._hidden_dims, kernel_size=2, padding=\"SAME\", name='s_to_s')\n",
        "\n",
        "    def forward(self, i_to_s: torch.Tensor, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        i_to_s: [B, 4 * height * hidden_dims]\n",
        "        state: [B, 2 * num_units] where num_units = height * hidden_dims\n",
        "        returns: h (B, height*hidden_dims), new_state (B, 2*num_units)\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "\n",
        "        return h, new_state\n",
        "\n",
        "# ---------------------------\n",
        "# diagonal_lstm (sequence loop)\n",
        "# ---------------------------\n",
        "def diagonal_lstm(inputs: torch.Tensor, conf, scope: str = 'diagonal_lstm') -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    inputs: NHWC or NCHW. The function follows TF flow:\n",
        "      - skew inputs -> conv2d 1x1 to produce input_to_state (skewed), transpose to [B, W, H, 4*hidden_dims]\n",
        "      - reshape to rnn_inputs [B, W, H*4*hidden_dims] and iterate over width dimension\n",
        "    conf must provide: hidden_dims (int), use_dynamic_rnn (bool optional)\n",
        "    Returns: outputs in NHWC shape [B, H, W, hidden_dims]\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "\n",
        "    if was_nchw:\n",
        "        # convert back to NCHW to be consistent with upstream expectations\n",
        "        return outputs.permute(0, 3, 1, 2).contiguous()\n",
        "    return outputs\n",
        "\n",
        "# ---------------------------\n",
        "# diagonal_bilstm\n",
        "# ---------------------------\n",
        "def diagonal_bilstm(inputs: torch.Tensor, conf, scope: str = 'diagonal_bilstm') -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compose forward diagonal_lstm and backward diagonal_lstm then sum appropriately (as TF).\n",
        "    If conf.use_residual is True, apply 1x1 conv residuals and add to inputs.\n",
        "    Returns: sum of forward and backward outputs with backward last column zeroed and shifted similar to TF.\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    return out_fw + out_bw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaEY8A2xKeWX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import logging\n",
        "from typing import Optional, Dict, Any, Iterable\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "class Statistic:\n",
        "    \"\"\"\n",
        "    the Statistic helper.\n",
        "\n",
        "    Args:\n",
        "        model: torch.nn.Module to save/load state_dict from.\n",
        "        optimizer: optional torch.optim.Optimizer to save/load state.\n",
        "        data_tag: str label used in TensorBoard scalar names .\n",
        "        model_dir: directory where checkpoints and logs will be written.\n",
        "        test_step: not used internally here but kept for API similarity (you can use it externally).\n",
        "        max_to_keep: how many checkpoints to keep; older are deleted.\n",
        "        device: device to load tensors to when loading checkpoints (default: cpu).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: Optional[torch.optim.Optimizer],\n",
        "        data_tag: str,\n",
        "        model_dir: str,\n",
        "        test_step: int,\n",
        "        max_to_keep: int = 20,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.data_tag = data_tag\n",
        "        self.model_dir = model_dir\n",
        "        self.test_step = test_step\n",
        "        self.max_to_keep = max_to_keep\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "\n",
        "        # internal counter t\n",
        "        self.t = 0\n",
        "\n",
        "        # tensorboard writer\n",
        "        log_dir = os.path.join(\"logs\", self.model_dir)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        self.writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "        # Ensure model_dir exists for checkpoints\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "        # Optionally keep a list of saved checkpoints on disk (sorted)\n",
        "        self._refresh_checkpoint_list()\n",
        "\n",
        "        # call reset to initialize whatever you want (kept for API parity)\n",
        "        self.reset()\n",
        "\n",
        "        logger.info(\"Statistic initialized. logs -> %s, checkpoints -> %s\", log_dir, self.model_dir)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Override or extend if you need to accumulate running statistics.\n",
        "        \"\"\"\n",
        "        # user can extend this method if they want to keep rolling averages, etc.\n",
        "        pass\n",
        "\n",
        "    def _refresh_checkpoint_list(self):\n",
        "        \"\"\"\n",
        "        Internal helper: update the cached list of checkpoint paths sorted by step (ascending).\n",
        "        \"\"\"\n",
        "        pattern = os.path.join(self.model_dir, \"checkpoint_*.pt\")\n",
        "        ckpts = glob.glob(pattern)\n",
        "        # parse t from filename checkpoint_{t}.pt\n",
        "        def _t_from_name(p):\n",
        "            try:\n",
        "                base = os.path.basename(p)\n",
        "                t_str = base.replace(\"checkpoint_\", \"\").replace(\".pt\", \"\")\n",
        "                return int(t_str)\n",
        "            except Exception:\n",
        "                return -1\n",
        "        ckpts_sorted = sorted(ckpts, key=_t_from_name)\n",
        "        self._checkpoints = ckpts_sorted\n",
        "\n",
        "    def _prune_checkpoints(self):\n",
        "        \"\"\"\n",
        "        Keep only the last `max_to_keep` checkpoints, remove older ones.\n",
        "        \"\"\"\n",
        "        self._refresh_checkpoint_list()\n",
        "        if self.max_to_keep is None:\n",
        "            return\n",
        "        while len(self._checkpoints) > self.max_to_keep:\n",
        "            old = self._checkpoints.pop(0)\n",
        "            try:\n",
        "                os.remove(old)\n",
        "                logger.info(\"Removed old checkpoint: %s\", old)\n",
        "            except OSError:\n",
        "                logger.warning(\"Failed to remove old checkpoint: %s\", old)\n",
        "\n",
        "    def on_step(self, train_l: float, test_l: float):\n",
        "        \"\"\"\n",
        "        Called at end of a step (or epoch) to increment counter, write summaries, save checkpoints and reset accumulators.\n",
        "        \"\"\"\n",
        "        # increment counter first\n",
        "        self.t += 1\n",
        "\n",
        "        # write summaries\n",
        "        self.inject_summary({'train_l': train_l, 'test_l': test_l}, self.t)\n",
        "\n",
        "        # save model\n",
        "        self.save_model(self.t)\n",
        "\n",
        "        # reset accumulators if any\n",
        "        self.reset()\n",
        "\n",
        "    def get_t(self) -> int:\n",
        "        \"\"\"Return current step counter.\"\"\"\n",
        "        return int(self.t)\n",
        "\n",
        "    def inject_summary(self, tag_dict: Dict[str, float], t: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        Write scalar summaries to TensorBoard.\n",
        "        tag_dict: mapping of tag->value, e.g. {'train_l': 0.5}\n",
        "        t: step (if None uses current internal t)\n",
        "        \"\"\"\n",
        "        step = self.t if t is None else int(t)\n",
        "        for tag, value in tag_dict.items():\n",
        "            full_tag = f\"{self.data_tag}/{tag}\"\n",
        "            # ensure value is a python float\n",
        "            try:\n",
        "                val = float(value)\n",
        "            except Exception:\n",
        "                val = float(value.item()) if hasattr(value, \"item\") else float(value)\n",
        "            self.writer.add_scalar(full_tag, val, step)\n",
        "        # flush may be helpful to make summary available quickly\n",
        "        self.writer.flush()\n",
        "\n",
        "    def save_model(self, t: Optional[int] = None, extra_state: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Save checkpoint for model (+ optimizer if supplied) and the internal counter 't'.\n",
        "        extra_state: optional mapping of additional items to store (e.g., scheduler state).\n",
        "        \"\"\"\n",
        "        step = self.t if t is None else int(t)\n",
        "        ckpt_name = os.path.join(self.model_dir, f\"checkpoint_{step}.pt\")\n",
        "        state = {\n",
        "            't': step,\n",
        "            'model_state': self.model.state_dict()\n",
        "        }\n",
        "        if self.optimizer is not None:\n",
        "            state['optimizer_state'] = self.optimizer.state_dict()\n",
        "        if extra_state:\n",
        "            state['extra'] = extra_state\n",
        "\n",
        "        # save atomically\n",
        "        tmp_name = ckpt_name + \".tmp\"\n",
        "        torch.save(state, tmp_name)\n",
        "        os.replace(tmp_name, ckpt_name)\n",
        "        logger.info(\"Saved checkpoint: %s\", ckpt_name)\n",
        "\n",
        "        # refresh and prune\n",
        "        self._refresh_checkpoint_list()\n",
        "        self._prune_checkpoints()        \n",
        "\n",
        "    def load_model(self, checkpoint_path: Optional[str] = None, map_location: Optional[torch.device] = None) -> bool:\n",
        "        \"\"\"\n",
        "        Load the latest checkpoint (if checkpoint_path is None) or load the provided checkpoint file.\n",
        "        Returns True if load succeeded, False otherwise.\n",
        "        map_location: device to map the checkpoint tensors onto (default: self.device)\n",
        "        \"\"\"\n",
        " \n",
        "        map_location = map_location or self.device\n",
        "\n",
        "        if checkpoint_path is None:\n",
        "            # find latest checkpoint\n",
        "            self._refresh_checkpoint_list()\n",
        "            if not self._checkpoints:\n",
        "                logger.info(\"No checkpoints found in %s\", self.model_dir)\n",
        "                return False\n",
        "            checkpoint_path = self._checkpoints[-1]\n",
        "\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            logger.warning(\"Checkpoint path does not exist: %s\", checkpoint_path)\n",
        "            return False\n",
        "\n",
        "        logger.info(\"Loading checkpoint: %s\", checkpoint_path)\n",
        "        ckpt = torch.load(checkpoint_path, map_location=map_location)\n",
        "\n",
        "        # load model state\n",
        "        if 'model_state' in ckpt:\n",
        "            try:\n",
        "                self.model.load_state_dict(ckpt['model_state'])\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Failed to load model_state: %s\", e)\n",
        "                return False\n",
        "        else:\n",
        "            logger.warning(\"Checkpoint missing 'model_state' key\")\n",
        "\n",
        "        # load optimizer state if present and optimizer available\n",
        "        if 'optimizer_state' in ckpt and self.optimizer is not None:\n",
        "            try:\n",
        "                self.optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Failed to load optimizer_state: %s\", e)\n",
        "                # continue, not fatal\n",
        "\n",
        "        # restore t if present\n",
        "        if 't' in ckpt:\n",
        "            self.t = int(ckpt['t'])\n",
        "        else:\n",
        "            # try to infer from filename\n",
        "            base = os.path.basename(checkpoint_path)\n",
        "            try:\n",
        "                self.t = int(base.replace(\"checkpoint_\", \"\").replace(\".pt\", \"\"))\n",
        "            except Exception:\n",
        "                self.t = 0\n",
        "\n",
        "        logger.info(\"Load SUCCESS: %s (t=%d)\", checkpoint_path, int(self.t))\n",
        "        return True\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close writer etc.\"\"\"\n",
        "        try:\n",
        "            self.writer.close()\n",
        "        except Exception:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZPkQoBaKjRT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pprint\n",
        "import tarfile\n",
        "import hashlib\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "from types import SimpleNamespace\n",
        "from typing import Any, Dict, Iterable, Optional\n",
        "\n",
        "# Python3 urllib\n",
        "import urllib.request\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "pp = pprint.PrettyPrinter().pprint\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------\n",
        "# Small helpers\n",
        "# -------------------------\n",
        "def mprint(matrix: Iterable[Iterable[float]], pivot: float = 0.5):\n",
        "    \"\"\"Pretty-print a binary-style matrix using '#' and ' ' like the original.\"\"\"\n",
        "    for array in matrix:\n",
        "        print(\"\".join(\"#\" if i > pivot else \" \" for i in array))\n",
        "\n",
        "\n",
        "def get_timestamp() -> str:\n",
        "    \"\"\"Return a filesystem-friendly timestamp string with local timezone info.\"\"\"\n",
        "    now = datetime.datetime.now().astimezone()\n",
        "    return now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "\n",
        "\n",
        "def binarize(images):\n",
        "    \"\"\"\n",
        "    Binarize image(s) by sampling from Uniform(0,1) < pixel_value.\n",
        "    Accepts a NumPy array or a PyTorch tensor. Returns same type as input (numpy or torch.FloatTensor).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        is_torch = torch.is_tensor(images)\n",
        "    except Exception:\n",
        "        is_torch = False\n",
        "\n",
        "    if is_torch:\n",
        "        # produce a tensor of same shape & device\n",
        "        rand = torch.rand_like(images)\n",
        "        return (rand < images).float()\n",
        "    else:\n",
        "        # assume numpy\n",
        "        return (np.random.uniform(size=images.shape) < images).astype('float32')\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Image saving\n",
        "# -------------------------\n",
        "def save_images(images, height: int, width: int, n_row: int, n_col: int,\n",
        "                cmin: float = 0.0, cmax: float = 1.0, directory: str = \"./\", prefix: str = \"sample\"):\n",
        "    \"\"\"\n",
        "    Save a grid of images to a single image file.\n",
        "    - images: numpy array of shape (n_row*n_col, H, W) or (n_row*n_col, H, W, C)\n",
        "              OR shaped (n_row, n_col, H, W) etc. This function will attempt to reshape sensibly.\n",
        "    - height, width: single image H,W\n",
        "    - n_row, n_col: grid layout\n",
        "    \"\"\"\n",
        "    # convert torch -> numpy if needed\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.is_tensor(images):\n",
        "            images = images.detach().cpu().numpy()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    imgs = np.asarray(images)\n",
        "    # handle shapes:\n",
        "    # If flat list of images: (N, H, W) or (N, H, W, C)\n",
        "    if imgs.ndim == 3:\n",
        "        # grayscale stack\n",
        "        N, H, W = imgs.shape\n",
        "        C = 1\n",
        "        imgs = imgs.reshape((N, H, W))\n",
        "        imgs = imgs.reshape((n_row, n_col, H, W))\n",
        "        imgs = imgs.transpose(1, 2, 0, 3).reshape((H * n_row, W * n_col))\n",
        "        mode = 'L'\n",
        "    elif imgs.ndim == 4:\n",
        "        N, H, W, C = imgs.shape\n",
        "        if C == 1:\n",
        "            imgs = imgs.reshape((n_row, n_col, H, W))\n",
        "            imgs = imgs.transpose(1, 2, 0, 3).reshape((H * n_row, W * n_col))\n",
        "            mode = 'L'\n",
        "        elif C == 3:\n",
        "            # arrange into grid with channels last\n",
        "            imgs = imgs.reshape((n_row, n_col, H, W, C))\n",
        "            imgs = imgs.transpose(1, 2, 0, 3, 4).reshape((H * n_row, W * n_col, C))\n",
        "            mode = 'RGB'\n",
        "        else:\n",
        "            # unsupported channel count: collapse or take first channel\n",
        "            imgs = imgs[..., 0]\n",
        "            imgs = imgs.reshape((n_row, n_col, H, W))\n",
        "            imgs = imgs.transpose(1, 2, 0, 3).reshape((H * n_row, W * n_col))\n",
        "            mode = 'L'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported image array shape: %s\" % (imgs.shape,))\n",
        "\n",
        "    # scale pixels from [cmin,cmax] to [0,255]\n",
        "    if mode == 'L':\n",
        "        norm = (imgs - cmin) / max((cmax - cmin), 1e-8)\n",
        "        arr = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
        "        pil_img = Image.fromarray(arr, mode='L')\n",
        "    else:\n",
        "        norm = (imgs - cmin) / max((cmax - cmin), 1e-8)\n",
        "        arr = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
        "        pil_img = Image.fromarray(arr, mode='RGB')\n",
        "\n",
        "    filename = f'{prefix}_{get_timestamp()}.jpg'\n",
        "    out_path = os.path.join(directory, filename)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    pil_img.save(out_path)\n",
        "    logger.info(\"Saved image grid to %s\", out_path)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Model / config helpers\n",
        "# -------------------------\n",
        "def get_model_dir(config, exceptions=None):\n",
        "    \"\"\"\n",
        "    Robust replacement for TF-specific get_model_dir:\n",
        "      - Accepts TF flags object (which stores flags under __flags) or argparse/Namespace/SimpleNamespace.\n",
        "      - Builds a readable model dir name from config key/value pairs excluding `exceptions`.\n",
        "      - If the generated name is too long, falls back to an md5 hash to keep path sane.\n",
        "    \"\"\"\n",
        "    exceptions = set(exceptions or [])\n",
        "    # Try TF-style flags first\n",
        "    try:\n",
        "        if hasattr(config, '__dict__') and '__flags' in config.__dict__:\n",
        "            attrs = dict(config.__dict__['__flags'])\n",
        "        else:\n",
        "            # argparse.Namespace / SimpleNamespace / plain object\n",
        "            attrs = dict(vars(config))\n",
        "    except Exception:\n",
        "        # as a last resort, try to use __dict__\n",
        "        try:\n",
        "            attrs = dict(config.__dict__)\n",
        "        except Exception:\n",
        "            attrs = {}\n",
        "\n",
        "    # Filter out exceptions and None/empty values that are not informative\n",
        "    items = []\n",
        "    for k in sorted(attrs.keys()):\n",
        "        if k in exceptions:\n",
        "            continue\n",
        "        if k.startswith('_'):\n",
        "            continue\n",
        "        v = attrs[k]\n",
        "        if callable(v):\n",
        "            vstr = v.__name__\n",
        "        else:\n",
        "            try:\n",
        "                vstr = str(v)\n",
        "            except Exception:\n",
        "                vstr = repr(v)\n",
        "        # shorten long values\n",
        "        if len(vstr) > 40:\n",
        "            vstr = hashlib.md5(vstr.encode('utf-8')).hexdigest()[:8]\n",
        "        items.append(f\"{k}={vstr}\")\n",
        "\n",
        "    if not items:\n",
        "        name = \"default\"\n",
        "    else:\n",
        "        name = \"_\".join(items)\n",
        "\n",
        "    # sanitize name (remove spaces, slashes)\n",
        "    name = name.replace(\" \", \"\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "    # If too long, shorten using md5\n",
        "    if len(name) > 180:\n",
        "        name = hashlib.md5(name.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "    return os.path.join('checkpoints', name) + '/'\n",
        "\n",
        "\n",
        "def preprocess_conf(conf):\n",
        "    \"\"\"\n",
        "    Placeholder to keep API parity with your previous code.\n",
        "    If you need to normalize or canonicalize flags/options, do it here.\n",
        "    \"\"\"\n",
        "    # For argparse/Namespace we don't need to do anything by default.\n",
        "    return conf\n",
        "\n",
        "\n",
        "def check_and_create_dir(directory: str):\n",
        "    \"\"\"Create dir if not exists (logs)\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        logger.info('Creating directory: %s' % directory)\n",
        "        os.makedirs(directory)\n",
        "    else:\n",
        "        logger.info('Skip creating directory: %s' % directory)\n",
        "\n",
        "\n",
        "def show_all_variables(model: Optional[Any] = None):\n",
        "    \"\"\"\n",
        "    Print all trainable variables.\n",
        "    If `model` is provided (torch.nn.Module), iterate its parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        if model is None:\n",
        "            logger.warning(\"No model passed to show_all_variables(model). Nothing to show.\")\n",
        "            return\n",
        "        total_count = 0\n",
        "        for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "            shape = tuple(param.shape)\n",
        "            count = int(np.prod(shape))\n",
        "            print(\"[%2d] %s %s = %s\" % (idx, name, shape, count))\n",
        "            total_count += count\n",
        "        print(\"[Total] variable size: %s\" % \"{:,}\".format(total_count))\n",
        "    except Exception as e:\n",
        "        logger.exception(\"show_all_variables failed: %s\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_IBau6nKCg7"
      },
      "source": [
        "## Network\n",
        "in this part we will impelement a neural network that can be configured to perform pixel-wise predictions, specifically for image generation tasks such as those in PixelCNN or PixelRNN models. This class supports both training and inference, and can generate outputs pixel-by-pixel in a raster scan order, leveraging various types of convolutions (standard and masked) and recurrent layers (e.g., LSTM).\n",
        "\n",
        "25 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IioQxy6J_QF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import logging\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch Network class.\n",
        "    \"\"\"\n",
        "    def __init__(self, conf, height: int, width: int, channel: int, device: Optional[torch.device] = None):\n",
        "        super().__init__()\n",
        "        logger.info(\"Building %s starts!\" % conf.model)\n",
        "        self.conf = conf\n",
        "        self.data = conf.data\n",
        "        self.height, self.width, self.channel = height, width, channel\n",
        "        self.device = device if device is not None else (torch.device(\"cuda\") if conf.use_gpu and torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "\n",
        "        # Data format decisions\n",
        "        self.data_format = \"NHWC\" if conf.use_gpu else \"NCHW\"\n",
        "\n",
        "        # Build layers - FIXED CHANNEL DIMENSIONS\n",
        "        in_channels = channel\n",
        "\n",
        "        # Calculate conv_out_channels based on residual connections\n",
        "        if conf.use_residual and conf.model == \"pixel_rnn\":\n",
        "            conv_out_channels = conf.hidden_dims * 2\n",
        "        else:\n",
        "            conv_out_channels = conf.hidden_dims\n",
        "\n",
        "        # Input convolution\n",
        "        self.conv_inputs = nn.Conv2d(in_channels, conv_out_channels, kernel_size=7, padding=3)\n",
        "\n",
        "        # Build recurrent/conv layers\n",
        "        self.recurrent_length = conf.recurrent_length\n",
        "        self.out_recurrent_length = conf.out_recurrent_length\n",
        "\n",
        "        if conf.model == \"pixel_cnn\":\n",
        "            # For pixel_cnn, use masked convolutions with proper channel dimensions\n",
        "            self.conv_blocks = nn.ModuleList()\n",
        "            for idx in range(self.recurrent_length):\n",
        "                in_ch = conv_out_channels if idx == 0 else conf.hidden_dims\n",
        "                # Use regular conv2d for now - replace with masked conv if available\n",
        "                self.conv_blocks.append(nn.Conv2d(in_ch, conf.hidden_dims, kernel_size=3, padding=1))\n",
        "        else:\n",
        "            # For pixel_rnn we will call diagonal_bilstm in forward()\n",
        "            self.conv_blocks = None\n",
        "\n",
        "        # Output recurrent layers (1x1 convs + ReLU)\n",
        "        self.out_convs = nn.ModuleList()\n",
        "        for idx in range(self.out_recurrent_length):\n",
        "            in_ch = conf.hidden_dims if idx == 0 else conf.out_hidden_dims\n",
        "            self.out_convs.append(nn.Conv2d(in_ch, conf.out_hidden_dims, kernel_size=1))\n",
        "\n",
        "        # Final logits conv - FIXED: output should match input channels for binary prediction\n",
        "        if channel == 1:\n",
        "            # Single output logit per pixel\n",
        "            in_ch = conf.out_hidden_dims if self.out_recurrent_length > 0 else conv_out_channels\n",
        "            self.conv2d_out_logits = nn.Conv2d(in_ch, 1, kernel_size=1)\n",
        "            self.loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "        else:\n",
        "            # For RGB/categorical (not implemented in original)\n",
        "            raise NotImplementedError(\"RGB branch not implemented (same as original).\")\n",
        "\n",
        "        # Optimizer (RMSProp) and grad clipping\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=conf.learning_rate)\n",
        "        self.grad_clip = conf.grad_clip\n",
        "\n",
        "        # Move to device\n",
        "        self.to(self.device)\n",
        "        logger.info(\"Building %s finished!\" % conf.model)\n",
        "\n",
        "    def _to_nchw(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert incoming tensor to NCHW for internal conv ops.\"\"\"\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x)\n",
        "        x = x.to(self.device)\n",
        "        if x.dim() != 4:\n",
        "            raise ValueError(\"Input must be 4D tensor\")\n",
        "        if self.data_format == \"NHWC\":\n",
        "            # [B,H,W,C] -> [B,C,H,W]\n",
        "            x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x.float()\n",
        "\n",
        "    def _to_output_layout(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert internal outputs (NCHW) back to requested external layout.\"\"\"\n",
        "        if self.data_format == \"NHWC\":\n",
        "            return x.permute(0, 2, 3, 1).contiguous()\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        #TODO\n",
        "\n",
        "        # Return in external layout\n",
        "        return self._to_output_layout(logits)\n",
        "\n",
        "    def predict(self, images):\n",
        "        \"\"\"Predict probabilities for given images.\"\"\"\n",
        "        #TODO\n",
        "        return probs_t.detach().cpu().numpy()\n",
        "\n",
        "    def test(self, images, with_update: bool = False):\n",
        "        \"\"\"Compute loss for images, optionally update weights.\"\"\"\n",
        "        #TODO\n",
        "\n",
        "        return float(loss_val.item())\n",
        "\n",
        "    def generate(self, batch_size: int = 100):\n",
        "        \"\"\"Generate binary samples in raster order.\"\"\"\n",
        "        #TODO\n",
        "\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3XTaYW_KvVa"
      },
      "source": [
        "## Main\n",
        "\n",
        "10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YTDJFRmtKwjd",
        "outputId": "a1777f51-51bb-4d27-b941-50a82d3efb19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import argparse\n",
        "import math\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "from itertools import islice\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# logging\n",
        "logging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%m-%d %H:%M:%S\")\n",
        "logger = logging.getLogger()\n",
        "# default level set later by args\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # network\n",
        "    parser.add_argument(\"--model\", type=str, default=\"pixel_cnn\", help=\"name of model [pixel_rnn, pixel_cnn]\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=100, help=\"size of a batch\")\n",
        "    parser.add_argument(\"--hidden_dims\", type=int, default=16, help=\"dimesion of hidden states of LSTM or Conv layers\")\n",
        "    parser.add_argument(\"--recurrent_length\", type=int, default=7, help=\"the length of LSTM or Conv layers\")\n",
        "    parser.add_argument(\"--out_hidden_dims\", type=int, default=32, help=\"dimesion of hidden states of output Conv layers\")\n",
        "    parser.add_argument(\"--out_recurrent_length\", type=int, default=2, help=\"the length of output Conv layers\")\n",
        "    parser.add_argument(\"--use_residual\", action=\"store_true\", default=False, help=\"whether to use residual connections or not\")\n",
        "\n",
        "    # training\n",
        "    parser.add_argument(\"--max_epoch\", type=int, default=100000, help=\"# of step in an epoch\")\n",
        "    parser.add_argument(\"--test_step\", type=int, default=100, help=\"# of step to test a model\")\n",
        "    parser.add_argument(\"--save_step\", type=int, default=1000, help=\"# of step to save a model\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"learning rate\")\n",
        "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"value of gradient to be used for clipping\")\n",
        "    parser.add_argument(\"--use_gpu\", action=\"store_true\", default=True, help=\"whether to use gpu for training\")\n",
        "\n",
        "    # data\n",
        "    parser.add_argument(\"--data\", type=str, default=\"mnist\", help=\"name of dataset [mnist, cifar]\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"data\", help=\"name of data directory\")\n",
        "    parser.add_argument(\"--sample_dir\", type=str, default=\"samples\", help=\"name of sample directory\")\n",
        "\n",
        "    # debug / misc\n",
        "    parser.add_argument(\"--is_train\", action=\"store_true\", default=True, help=\"training or testing\")\n",
        "    parser.add_argument(\"--display\", action=\"store_true\", default=False, help=\"whether to display the training results or not\")\n",
        "    parser.add_argument(\"--log_level\", type=str, default=\"INFO\", help=\"log level [DEBUG, INFO, WARNING, ERROR, CRITICAL]\")\n",
        "    parser.add_argument(\"--random_seed\", type=int, default=123, help=\"random seed for python\")\n",
        "\n",
        "    return parser.parse_args([]) # Pass an empty list to parse_args\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    # convert argparse Namespace to a conf-like SimpleNamespace (so older code expecting attributes works)\n",
        "    conf = SimpleNamespace(**vars(args))\n",
        "\n",
        "    # logging level\n",
        "    logger.setLevel(conf.log_level)\n",
        "\n",
        "    # random seeds\n",
        "    np.random.seed(conf.random_seed)\n",
        "    torch.manual_seed(conf.random_seed)\n",
        "    if conf.use_gpu and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(conf.random_seed)\n",
        "\n",
        "    # Build model dir and preprocess config if helper exists\n",
        "    model_dir = get_model_dir(conf,\n",
        "                              ['data_dir', 'sample_dir', 'max_epoch', 'test_step', 'save_step',\n",
        "                               'is_train', 'random_seed', 'log_level', 'display'])\n",
        "    preprocess_conf(conf)\n",
        "\n",
        "    DATA_DIR = os.path.join(conf.data_dir, conf.data)\n",
        "    SAMPLE_DIR = os.path.join(conf.sample_dir, conf.data, model_dir)\n",
        "\n",
        "    check_and_create_dir(DATA_DIR)\n",
        "    check_and_create_dir(SAMPLE_DIR)\n",
        "\n",
        "    device = torch.device(\"cuda\" if (conf.use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
        "    logger.info(\"Using device: %s\", device)\n",
        "\n",
        "    # 0. prepare datasets\n",
        "\n",
        "    if conf.data == \"cifar\":\n",
        "        #TODO\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset: %s\" % conf.data)\n",
        "\n",
        "    try:\n",
        "        stat = Statistic(network, network.optimizer, conf.data, model_dir, conf.test_step)\n",
        "    except TypeError:\n",
        "        # older Statistic signature variations: try with optimizer attr\n",
        "        stat = Statistic(network, network.optimizer if hasattr(network, 'optimizer') else None, conf.data, model_dir, conf.test_step)\n",
        "\n",
        "    # Try loading existing checkpoint\n",
        "    stat.load_model()\n",
        "\n",
        "    if conf.is_train:\n",
        "        logger.info(\"Training starts!\")\n",
        "        initial_step = stat.get_t() if stat else 0\n",
        "        iterator = trange(conf.max_epoch, ncols=70, initial=initial_step)\n",
        "\n",
        "        for epoch in iterator:\n",
        "            # 1. train\n",
        "\n",
        "            # 2. test\n",
        "\n",
        "            # 3. generate samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
